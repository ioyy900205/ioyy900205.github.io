<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Paper_Review on 亮的笔记</title>
    <link>https://ioyy900205.github.io/tags/paper_review/</link>
    <description>Recent content in Paper_Review on 亮的笔记</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 31 Aug 2023 11:28:02 +0800</lastBuildDate><atom:link href="https://ioyy900205.github.io/tags/paper_review/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>60-调研——Visual Audio Speech Enhancement</title>
      <link>https://ioyy900205.github.io/post/2023-08-31-%E8%B0%83%E7%A0%94vase/</link>
      <pubDate>Thu, 31 Aug 2023 11:28:02 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2023-08-31-%E8%B0%83%E7%A0%94vase/</guid>
      <description>在此处编辑 blog.walterlv.com 的博客摘要
  1 论文调研  1.1 ICASSP 2023  Audio for Multimedia and Multimodal Processing Human Identification and Face Recognition Learning from Multimodal Data ASR: Noise Robustness Keyword Spotting   1.2 INTERSPEECH 2023  Resources for Spoken Language Processing Analysis of Speech and Audio Signals Speech Recognition: Technologies and Systems for New Applications Spoken Dialog Systems and Conversational Analysis Speech Coding and Enhancement Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources, and Evaluation Speech, Voice, and Hearing Disorders Source Separation Speaker and Language Identification Speaker Recognition Speech Perception, Production, and Acquisition Multi-modal Systems Multi-talker Methods in Speech Processing New Computational Strategies for ASR Training and Inference Dialog Management Show and Tell: Health Applications and Emotion Recognition Show and Tell: Speech Tools, Speech Enhancement, Speech Synthesis     2 基于论文调研的分析  2.</description>
    </item>
    
    <item>
      <title>59-时频通道双重注意力</title>
      <link>https://ioyy900205.github.io/post/2022-02-14-59-duality-temporal-channel-frequency-attention-enhanced-speaker-representation-learning/</link>
      <pubDate>Mon, 14 Feb 2022 14:43:42 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2022-02-14-59-duality-temporal-channel-frequency-attention-enhanced-speaker-representation-learning/</guid>
      <description>水一篇论文。。
 @TOC
标题  参考资料 @TOC
谢磊老师团队作品 2021.12
Audio, Speech and Language Processing Group (ASLP@NPU), School of Computer Science, Northwestern Polytechnical University, Xi’an, China
标题：时间-通道-频率的双重性注意增强说话人的表征学习
简单来说就是将图像中的CBAM重新拆分整合了一下。
AI Labs - Trident &amp;gt; 7. DUALITY TEMPORAL-CHANNEL-FREQUENCY ATTENTION ENHANCED SPEAKER REPRESENTATION LEARNING &amp;gt; image2022-2-14 14:25:48.png
我接着查看了参考文献
Qibin Hou, Daquan Zhou, and Jiashi Feng, “Coordinate attention for efficient mobile network design,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp.</description>
    </item>
    
    <item>
      <title>58-facebook denoiser 翻译</title>
      <link>https://ioyy900205.github.io/post/2022-01-10-58facebook-denoiser-%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Mon, 10 Jan 2022 14:44:57 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2022-01-10-58facebook-denoiser-%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/</guid>
      <description>Real Time Speech Enhancement in the Waveform Domain
网上找不到相关的翻译文章。 这里就做贡献了
  1. 摘要 2. 简介 3. 模型  3.1. 符号和问题设置 3.2. DEMUCS architecture 3.3. 目标   4. 实验  4.1. 实施详情 4.2. 结果 4.3. 消融实验 4.4. 实时性评估 4.5. 对ASR模型的影响   5. 相关工作 6. 结论  1. 摘要 We present a causal speech enhancement model working on the raw waveform that runs in real-time on a laptop CPU. The proposed model is based on an encoder-decoder architecture with skip-connections.</description>
    </item>
    
    <item>
      <title>56-S-DCCRN论文阅读</title>
      <link>https://ioyy900205.github.io/post/2021-12-28-56-s-dccrn%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</link>
      <pubDate>Tue, 28 Dec 2021 15:32:05 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-12-28-56-s-dccrn%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</guid>
      <description>在语音增强方面，复杂的神经网络由于其在处理复值频谱方面的有效性而显示出良好的性能。最近的大多数语音增强方法主要集中在采样率为16K Hz的宽频信号上。然而，由于难以对更多的频段，特别是高频成分进行建模，因此仍然缺乏对超宽频段（如32K Hz）甚至全频段（48K）去噪的研究。在本文中，我们将之前的深度复杂卷积递归神经网络（DCCRN）大幅扩展为超宽频带版本&amp;ndash;S-DCCRN，以对32K Hz采样率的语音进行去噪。
具体操作：
 我们首先采用了一个级联的子带和全带处理模块，它由两个小尺寸的DCCRN组成&amp;ndash;一个操作子带信号，一个操作全带信号，旨在从局部和全局频率信息中受益。此外，我们没有简单地采用STFT特征作为输入，而是使用了一个以端到端方式训练的复杂特征编码器来完善不同频段的信息。 我们还使用了一个复杂的特征解码器来将特征还原到时频域。最后，采用一种可学习的频谱压缩方法来调整不同频段的能量，这有利于神经网络的学习。   所提出的模型S-DCCRN已经超过了PercepNet以及几个有竞争力的模型，在语音质量和可懂度方面达到了最先进的性能。消融研究也证明了不同贡献的有效性。   @TOC
1. 引言 随着电话会议和其他实时语音通信场景的快速发展，对高质量高保真语音的需求急剧增加。随着采样率的提高，语音将包含更丰富的信息和更精细的细节，特别是在更高的频段。然而，目前大多数基于深度学习的语音增强方法主要关注采样率为16K Hz的宽频信号。在超宽频[1]甚至全频段信号上进行语音增强的潜力仍有待探索，因为在对更多频段尤其是高频成分进行建模方面存在着挑战。此外，用更大维度的特征进行建模将导致更高的建模复杂性，使实时实施变得更加困难。一些语音增强器采用了压缩的特征，如吠声频谱[2]来模拟高频信号，而特征压缩可能会不可避免地失去频段的重要信息，导致次优性能。
长期以来，基于DNN的语音前端算法只试图增强噪声幅度，而将噪声相位直接用于语音波形重建。其原因可以归结为相位的结构不明确，被认为是具有挑战性的估计。随后，Williamson等人提出了复杂比率掩码（CRM）[3]，它可以通过同时增强噪声语音的实部和虚部来完美重建语音。基于宽带场景的SOTA方法，如SDD-Net[4]和DCCRN[5]，已经显示出出色的性能，特别是对于低信噪比和突发噪声的复杂去噪情况。DCCRN结合了DCUNET[6]和CRN[7]的优点，使用LSTM对时间背景进行建模，可训练的参数和计算成本大大降低。SDD-Net应用功率压缩频谱[8]作为输入特征，并采用了四个特别设计的阶段，极大地提高了同步去杂和去噪的语音质量。对于全波段的情况，RNNoise[2]的研究采用树皮谱，而不是STFT，作为模型的输入。树皮谱在频率轴上完全只有22个维度[9]，这可以大大减少模型的大小，并加快模型推理的速度。树皮谱假设语音和噪声的频谱包络足够平坦[2]。然而，由于真实声学场景的复杂性（如突发噪声和混响），这种方法在真实声学场景中可能导致严重的衰减，从而导致过多的噪声残留。最近，PercepNet[10]提出了一种感知频带表示法，它只对32个三角谱带进行操作，根据等效的entrectangular带宽（ERB）尺度进行间隔[9]。然而，树皮尺度和ERB尺度的分辨率比STFT的线性频谱更粗糙，导致了频带信息的泄漏。最近，超宽频带/全频带信号的语音增强引起了广泛关注&amp;ndash;深度噪声抑制挑战（DNS）[11]特别设立了全频带轨道。
本文提出了超宽频DCCRN（S-DCCRN），用于32K Hz采样率的超宽频场景中的语音增强。这项工作的贡献有三个方面，在Voicebank和Demond数据集上进行客观评估，在DNS-2021盲测集上进行主观评估。
我们提出了两个轻量级的DCCRN子模块，分别用于子频段和全频段（SAF）建模，因为我们认为低频段含有较高的能量，而高频段对主观感受有很大影响[12]。因此，采用子波段处理模块对低频段和高频段分别建模。然而，由于低频和高频成分之间没有明确的信息交互，仅采用子带处理可能会导致频段之间的不平滑连接。因此，我们进一步应用全频段处理模块来平滑不同频段的边界。详细地说，子频带处理模块包括一个子频带DCCRN，它用群复卷积代替原始DCCRN的复卷积，以分别模拟低频段和高频段。全频带和子频带处理模块的编码器和解码器之间的卷积通路[13]被用来进行更好的信息交互，避免全频带的信息丢失。在较小的模型尺寸下，与神谕DCCRN相比，SAF模块导致了0.17的PESQ改进。
受宽带去噪中频谱压缩的启发[8]，我们在模型中引入了可学习频谱压缩（LSC），它可以动态地调整不同频段的能量。LSC的使用使得高频段的模式更加清晰，这种更新带来了0.07的额外PESQ增益。
受DPT-FSNet[14]的编码器/解码器块的启发，我们在STFT之后采用了复杂特征编码器（CFE），在iSTFT之前采用了复杂特征解码器（CFD）。我们保持与大多数宽频语音增强模型相同的STFT点。尽管在高采样率的情况下，频率分辨率相对较低，但CFE块可以细化STFT频谱的不同频段的信息。通过可学习的频谱压缩，这种更新带来了0.07的额外PESQ增益。
所提出的S-DCCRN模型超过了所有测试过的SOTA模型，包括RNNoise和DCCRN，并在Interspeech 2021 DNS挑战赛的盲测集上获得了3.62的MOS分数，表现优异[11]。
2. 超宽带DCCRN 2.1. 复数编解码器 研究人员通常采用Bark频谱作为宽频带场景下全频带语音增强的网络输入，它可以将物理频率转换成基于人类感知的心理声学频率[2]。然而，通过这种转换，原来的物理频段被压缩了，而且相位信息也被丢弃了。此外，基于人类感知的特征可能不适合于网络的输入。另一方面，直接使用STFT特征也会引起一些问题。随着STFT点数的增加，由于高维输入特征难以建模，网络的复杂性会增加。相反，使用点数较少的STFT特征也会造成频率分辨率的下降。本文受DPT-FSNet[14]的编码器/解码器块的启发，在STFT之后采用复杂特征编码器/解码器，基于512维的复杂STFT特征来细化不同频段的信息。
如图2（a）所示，复杂特征编码器（CFE）模块的输入是通过STFT得到的T-F频谱。我们采用核大小为1的复数conv2d来提取高维信息。然后使用扩张的密集块[15]来捕捉时间尺度上的长期背景特征。最后，采用 采用复杂的conv2d来提取复杂的局部特征。在每次卷积之后，都会相继进行LayerNorm和PReLU激活。
如图2（b）所示，复杂特征解码器（CFD）模块的输入是SAF模块输出的实/图像特征。具体来说，我们采用扩展的密集块来处理估计的实/图像特征。然后，扩张密集块的输出被复杂像素卷积处理，用复杂卷积代替像素卷积[16]中的卷积。像素卷积被认为是替代转置卷积的更好方法，以避免棋盘式伪影[17]。最后，进行复杂卷积，将高维特征还原到时频域。如图2（c）所示，每个密集块由五层conv2d组成。各帧之间的卷积是因果关系。与之前所有层的密集连接避免了梯度消失的问题[15]。
2.2. 子带和全带处理模块 随着采样率的增加，频段的数量也在很大程度上增加。由于低频和高频之间的信息有很大的不同，不同的频段很难只通过全频段处理来建模[12]。在一个模块中对它们进行建模并不是最佳选择。另一方面，由于不同频段之间没有信息交互，子频段处理会在不同频段的边界造成一定的不平滑连接。基于上述考虑，我们提出了一个子波段和全波段处理（SAF）模块，以发挥两者的优势。
如图1所示，我们使用来自CFE的编码特征作为SAF模块的输入，该模块主要由子带DCCRN和全带DCCRN组成。在SAF模块中，特征首先被一个子带DCCRN处理。子带DCCRN的输出与来自CFE的编码特征（被认为有助于平滑频带）的连接被视为全带DCCRN的输入。全波段DCCRN的输出是来自CFE的编码特征的复数比率掩码（CRM）。
子波段DCCRN的结构如图3所示。子波段DCCRN的总体设计与神谕DCCRN相似，但神谕DCCRN中的复杂卷积块被复杂群卷积块所取代。如图4所示，复杂组卷积块的目的是对低频段和高频段分别建模。此外，为了从每个编码器层聚集更丰富的信息，并缓解各频段之间的不平滑连接，我们采用了编码器和解码器之间的卷积通路块，这在DCCRN+[13]中被证明是有用的。具体来说，编码器和解码器之间的卷积通路由一个复杂的卷积块和批量归一化组成。
在将来自CFE的编码特征与子带DCCRN的输出相连接后，我们使用另一个全带DCCRN，它采取普通复数卷积而不是复数组卷积，以进一步再生和平滑不同的频段。全波段DCCRN还采用了编码器和解码器之间的卷积途径，以实现更好的信息交互。
2.3. 可学习的频谱压缩 有学者指出，频谱中的局部模式在每个频段中往往是不同的：低频段往往包含高能量、音调以及长时间持续的声音，而高频段则可能有低能量成分、噪声和快速衰减的声音[12]。最近，关于宽频带去噪的频谱压缩显示了有希望的结果，它可以通过0.5的压缩率增加高频带的能量[8]。
我们认为，频段的压缩率应该是不同的，因为高频率的频段可能需要较低的压缩率来保持其高能量。这激发了我们开发一个可学习的频谱压缩模块，使用一组网络层来压缩STFT频谱。在可学习的网络层之后进行sigmoid激活，目的是将输出压缩到0∼1。
详细来说，可学习频谱压缩可以描述为 YLSC = |Y |α ejjY (1) 其中Y和α分别表示噪声频谱和可学习参数。
2.4. 损失函数 对于学习目标，我们首先应用SI-SNR[18]损失，它是一个时域损失函数。此外，我们采用复杂的均方误差（MSE）损失和Kullback-Leibler Divergence[19]来提高估计频谱和复杂域中的清洁频谱之间的相似性。KL Divergence的目的是从概率分布的角度来优化清洁频谱和估计频谱。这三种损失是通过以下方式共同优化的
3. 实验 3.1. 数据集 我们对32K采样率的音频样本进行了语音增强实验。我们首先在Voice Bank和DEMAND数据集[20]上进行消融实验，以证明每个提议的子模块的有效性。具体来说，源语音来自VoiceBank语料库[21]，其中包含28个扬声器用于训练，另外2个扬声器用于测试。10种噪声类型，其中2种是人工生成的，8种是来自DEMAND[22]的真实录音，用于训练。请注意，所有的数据在实验前都从48K降频到32K Hz。总的来说，固定的训练和验证集分别包含11,572个语料（10小时）和872个语料（30分钟）。我们还在这个数据集上将其他SOTA模型（包括PercepNet[10]）与S-DCCRN进行比较。 然后，S-DCCRN被进一步训练，并用Interspeech 2021 DNS挑战数据集进行评估，以显示其在更复杂和真实的声学场景中的性能。源语音数据来自DNS-2021全频段数据集，其中包含672小时的语音数据。180小时的DNS-2021噪声集，包括来自150个噪声类别的65000个噪声片段，被选作源噪声数据。训练集包含605小时源语音数据，而验证集分别包含67小时源语音数据。训练数据是以32K Hz的采样率即时生成的，并在一个批次中被分割成8 s的小块，信噪比范围为-5至20 dB。经过14个历时的训练，该模型 &amp;quot;看到 &amp;quot;的总数据超过9000小时。  3.</description>
    </item>
    
    <item>
      <title>48-论文阅读小目标检测综述</title>
      <link>https://ioyy900205.github.io/post/2021-09-27-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%B0%8F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/</link>
      <pubDate>Mon, 27 Sep 2021 09:10:57 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-09-27-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%B0%8F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/</guid>
      <description>小目标检测方法的问题和方法整理。
 @TOC
0.1. 多尺度方法 YOLO、Faster-RCNN只用最后一层，SSD首次采用了多尺度预测的方式。
0.1.1. 图像金字塔方法 首先将图像缩放的不同的分辨率，通过不同分辨率的图像上分别提取特征来形成多尺度的特征表达。
缺点：由于需要多种分辨率图像分别提取特征，严重增加了推理时间，限制了该方法在实时性要求比较高的条件下应用。
0.1.2. DSDD算法  用101替代VGG16 将高层特征的语义信息融入底层特征（反卷积）  0.1.3. 特征金字塔方法 自底向上的分支用于产生多尺度的特征。自顶向下的方法用于将丰富语义信息传递到底层。
方法：高层特征进行2倍上采样得到和相邻底层一样的分辨率，然后底层特征经过1x1卷积和上采样之后的高层特征进行元素级别的相加，再经过3x3卷积得到最终特征图。
FPN目前已经成为了一个标准配置，也有很多基于FPN的优化工作相继涌现出来。
0.1.4. PANet算法 作者再FPN的基础上又增加了一个自底向上的路径增强分支。
FPN中，高层特征与低层特征之间路径较长，造成在金字塔的顶部含有的底层信息较少。为了解决这个问题，PANet使用较少数量的卷积层构建了路径增强模块，尽可能多的保留底层信息。同时增加了自适应的池化层模块，使得感兴趣的区域中包含多层特征，而不是单层特征，进行了进一步的特征融合。
0.1.5. ASFF算法 注意到一个问题，FPN中，目标在某层被当作正类时，但是在其他层可能会被当成负类。这样在特征金字塔的某一层单独检测时会引入其他层的矛盾信息。
自适应的空间特征融合方法。该方法在FPN的基础上通过学习权重参数的方式将不同特征层融合到一起，得到融合之后的特征图用于最终的预测。
在论文中，作者将 ASFF 应用到 YOLO3 中，为了验证 ASFF 的有效性，首先在 YOLO3 应用了一系列的技巧，对 YOLO3 进行优化，将其在 COCO 2017验证集上的APs 指标由 18. 3 提升到 24. 6，将优化之后的 YOLO3 作为一个强的基线． 然后，在此基础上加入 ASFF，APs 指标由 24. 6 提升到 27. 5，提升了将近 3 个百分点，由此可见 ASFF 对于小目标检测的有效性。
0.1.6. Libra-RCNN算法 改进的方法分别提取了 4 个级别的多尺度特征{ C2，C3，C4，C5} ，然后将 { C2，C3，C5 } 缩放到和 C4 同样大小，进行集成操作，也就是将这 4 个尺度的特征进行 求和取平均得到集成之后的特征，再将得到的特征送入设计的增强模块中进行一个加强操作，最后再将加强后的特征和{ C2，C3，C4，C5} 相加，增强原特征．</description>
    </item>
    
    <item>
      <title>论文阅读BN</title>
      <link>https://ioyy900205.github.io/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-bn/</link>
      <pubDate>Fri, 24 Sep 2021 14:07:57 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-bn/</guid>
      <description>原来在训练深层网络的时候非常困难。这是因为在训练过程中，每一层的输入分布都会随着前几层的参数变化而变化。
原来的问题：
 训练速度慢：降低学习率和谨慎的参数初始化 训练困难：训练具有饱和非线性的模型非常困难  内部协变量转移
 @TOC
标题  参考资料</description>
    </item>
    
    <item>
      <title>41-NCNN模型部署</title>
      <link>https://ioyy900205.github.io/post/2021-08-06-41-ncnn%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E4%B8%80/</link>
      <pubDate>Fri, 06 Aug 2021 14:30:00 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-08-06-41-ncnn%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E4%B8%80/</guid>
      <description>争取比全网做的都要清晰
 @TOC
文件变化 整个工作流程文件有：
1.pth文件
2.onnx文件 （普通onnx、onnxsim）
3.ncnn文件 （普通ncnn、ncnnoptimal、ncnn压缩）
编译问题 多的就解释不了了，看代码
-std=c++11 -I/usr/local/include/opencv4 -I/home/liuliang/ncnn/build/install/include/ncnn -L/home/liuliang/ncnn/build/install/lib -lncnn -fopenmp -lopencv_imgcodecs -lopencv_flann -lopencv_core  参考资料</description>
    </item>
    
    <item>
      <title>35——NMS非极大值抑制</title>
      <link>https://ioyy900205.github.io/post/2021-07-05-35nms/</link>
      <pubDate>Mon, 05 Jul 2021 15:14:54 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-07-05-35nms/</guid>
      <description>代码实现
 @TOC
1. 代码 &amp;#39;&amp;#39;&amp;#39; Date: 2021-07-05 14:21:06 LastEditors: Liuliang LastEditTime: 2021-07-05 15:16:44 Description: &amp;#39;&amp;#39;&amp;#39; import numpy as np # import cv2 import matplotlib.pyplot as plt def py_cpu_nms(dets, thresh): &amp;#34;&amp;#34;&amp;#34;Pure Python NMS baseline.&amp;#34;&amp;#34;&amp;#34; # x1、y1、x2、y2、以及score赋值 x1 = dets[:, 0] y1 = dets[:, 1] x2 = dets[:, 2] y2 = dets[:, 3] scores = dets[:, 4] # 每一个检测框的面积 areas = (x2 - x1 ) * (y2 - y1 ) # 按照score置信度降序排序 order = scores.</description>
    </item>
    
    <item>
      <title>30——强化学习</title>
      <link>https://ioyy900205.github.io/post/2021-06-16-30%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Wed, 16 Jun 2021 11:25:16 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-16-30%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</guid>
      <description>维基百科：
强化学习（RL）是机器学习的一个领域，涉及软件代理如何在环境中采取行动以最大化一些累积奖励的概念。该问题由于其一般性，在许多其他学科中得到研究，如博弈论，控制理论，运筹学，信息论，基于仿真的优化，多智能体系统，群智能，统计和遗传算法。在运筹学和控制文献中，强化学习被称为近似动态规划或神经动态规划。
百度：
强化学习(reinforcement learning)，又称再励学习、评价学习，是一种重要的机器学习方法，在智能控制机器人及分析预测等领域有许多应用。
但在传统的机器学习分类中没有提到过强化学习，而在连接主义学习中，把学习算法分为三种类型，即非监督学习(unsupervised learning)、监督学习(supervised leaning)和强化学习。
 @TOC
强化学习的主流算法 免模型学习（Model-Free） vs 有模型学习（Model-Based）
在介绍详细算法之前，我们先来了解一下强化学习算法的2大分类。这2个分类的重要差异是：智能体是否能完整了解或学习到所在环境的模型
有模型学习（Model-Based）对环境有提前的认知，可以提前考虑规划，但是缺点是如果模型跟真实世界不一致，那么在实际使用场景下会表现的不好。
免模型学习（Model-Free）放弃了模型学习，在效率上不如前者，但是这种方式更加容易实现，也容易在真实场景下调整到很好的状态。所以免模型学习方法更受欢迎，得到更加广泛的开发和测试。
 参考资料
https://easyai.tech/ai-definition/reinforcement-learning/</description>
    </item>
    
    <item>
      <title>29——GRU 3步介绍</title>
      <link>https://ioyy900205.github.io/post/2021-06-09-29gru%E4%B8%89%E6%AD%A5/</link>
      <pubDate>Wed, 09 Jun 2021 16:05:03 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-09-29gru%E4%B8%89%E6%AD%A5/</guid>
      <description>29——GRU 3步介绍
GRU它引⼊了**重置⻔（reset gate）和更新⻔（update gate）**的概念，从而修改了循环神经⽹络中隐藏状态的计算⽅式。
 @TOC
1. ⻔控循环单元 1.1. 重置门和更新门 1.2. 候选隐藏状态 1.3. 隐藏状态 我们对⻔控循环单元的设计稍作总结：
重置⻔有助于捕捉时间序列⾥短期的依赖关系； 更新⻔有助于捕捉时间序列⾥⻓期的依赖关系。
 参考资料</description>
    </item>
    
    <item>
      <title>28——训练trick之优化器</title>
      <link>https://ioyy900205.github.io/post/2021-06-09-28%E8%AE%AD%E7%BB%83trick%E4%B9%8B%E4%BC%98%E5%8C%96%E5%99%A8/</link>
      <pubDate>Wed, 09 Jun 2021 15:24:43 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-09-28%E8%AE%AD%E7%BB%83trick%E4%B9%8B%E4%BC%98%E5%8C%96%E5%99%A8/</guid>
      <description>在机器学习的场景下，梯度下降学习的目标通常是最小化机器学习问题的损失函数。 一个好的算法能够快速可靠地找到最小值.(也就是说，它不会陷入局部极小值、鞍点或高原区域，而是寻找全局最小值)。
 @TOC
标题  参考资料
https://zhuanlan.zhihu.com/p/147275344</description>
    </item>
    
    <item>
      <title>25——GFNet论文阅读</title>
      <link>https://ioyy900205.github.io/post/2021-06-05-25gfnet/</link>
      <pubDate>Sat, 05 Jun 2021 10:54:57 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-05-25gfnet/</guid>
      <description>NeurIPS 2020录用的一篇论文：《Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classiﬁcation
论文链接：https://arxiv.org/pdf/2010.05300.pdf
代码和预训练模型链接：https://github.com/blackfeather-wang/GFNet-Pytorch
论文第一作者：
王语霖，清华大学自动化系直博二年级，导师为吴澄院士和黄高助理教授，研究兴趣为深度学习与计算机视觉，在NeurIPS 2019/2020以第一作者发表两篇学术论文。
 @TOC
1. 研究动机及简介 推理CNN所需的计算量（FLOPs）基本与像素数目成正比，即与图形的长、宽成二次关系。
在实际应用（例如手机APP、自动驾驶系统、图片搜索引擎）中，计算量往往正比于能耗或者时间开销，显然，无论出于成本因素还是从安全性和用户体验的角度考虑，网络的计算开销都应当尽可能小。
这便是本文所提出方法的出发点，我们的目标是，对于输入图片，自适应地找到其与任务最相关的区域，进而通过使神经网络只处理这些区域，以尽可能小的计算量得到可信的结果。具体而言，我们采用的方法是，将一张分辨率较高的图片表征为若干个包含其关键部分的“小块”（Patch），而后仅将这些小块输入神经网络。以下面的示意图为例，将一张224x224的图片分解为3个96x96的Patch进行处理所需的计算量仅为原图的55.2%。
2. Method 为了实现上述目的，事实上，有两个显然的困难：
(a) 任意给定一张输入图片，如何判断其与任务最相关的区域在哪里呢？
(b) 考虑到我们的最终目的是使神经网络得到正确的预测结果，不同输入所需的计算量是不一样的，例如对于下面所示的两个输入图片，神经网络可能仅需要处理一个patch就能识别出特征非常突出的月亮，但是需要处理更多的patch才能分辨出猫咪的具体品种。
为了解决这两个问题，我们设计了一个Glance and Focus的框架，将这一思路建模为了一个序列决策过程，如下图所示。
其具体执行流程为：
 首先，对于一张任意给定的输入图片，由于我们没有任何关于它的先验知识，我们直接将其放缩为一个patch的大小，输入网络，这一方面产生了一个初步的判断结果，另一方面也提供了原始输入图片的空间分布信息；这一阶段称为扫视（Glance）。 而后，我们再以这些基本的空间分布信息为基础，逐步从原图上取得高分辨率的patch，将其不断输入网络，以此逐步更新预测结果和空间分布信息，得到更为准确的判断，并逐步寻找神经网络尚未见到过的关键区域；这一阶段称为关注（Focus）。  值得注意的是，在上述序列过程的每一步结束之后，我们会将神经网络的预测自信度（confidence）与一个预先定义的阈值进行比较，一旦confidence超过阈值，我们便视为网络已经得到了可信的结果，这一过程立即终止。此机制称为自适应推理（Adaptive Inference）。通过这种机制，我们一方面可以使不同难易度的样本具有不同的序列长度，从而动态分配计算量、提高整体效率；另一方面可以简单地通过改变阈值调整网络的整体计算开销，而不需要重新训练网络，这使得我们的模型可以动态地以最小的计算开销达到所需的性能，或者实时最大化地利用所有可用的计算资源以提升模型表现。
3. 显而易见的困难 how to identify class-ciscriminative regions? 如何识别阶级歧视性区域？
how to determine the number of class-discriminative regions? 如何确定类区分区域的数量？
4. 网络结构 GFNet共有四个组件，分别为：
全局编码器${f_g}$和局部编码器${f_l}$ （Global Encoder and Local Encoder）为两个CNN，分别用于从放缩后的原图和局部patch中提取信息，之所以用两个CNN，是因为我们发现一个CNN很难同时适应缩略图和局部patch两种尺度（scale）的输入。几乎所有现有的网络结构均可以作为这两个编码器以提升其推理效率（如MobileNet-V3、EfficientNet、RegNet等）。 分类器 [公式] （Classifier）为一个循环神经网络（RNN），输入为全局池化后的特征向量，用于整合过去所有输入的信息，以得到目前最优的分类结果。 图像块选择网络 [公式] （Patch Proposal Network）是另一个循环神经网络（RNN），输入为全局池化前的特征图（不做池化是为了避免损失空间信息），用于整合目前为止所有的空间分布信息，并决定下一个patch的位置。值得注意的是由于取得patch的crop操作不可求导，[公式]是使用强化学习中的策略梯度方法（policy gradient）训练的。</description>
    </item>
    
    <item>
      <title>25——Noisy Student训练</title>
      <link>https://ioyy900205.github.io/post/2021-06-04-24noisy-student%E6%8F%90%E9%AB%98%E7%B2%BE%E5%BA%A6%E7%9A%84%E7%A5%9E%E5%99%A8/</link>
      <pubDate>Fri, 04 Jun 2021 14:47:02 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-04-24noisy-student%E6%8F%90%E9%AB%98%E7%B2%BE%E5%BA%A6%E7%9A%84%E7%A5%9E%E5%99%A8/</guid>
      <description>想要提高模型的精度和鲁棒性，尝试考虑使用无标注数据！
实际在普通的 ImageNet 数据集上，近年来的模型不断在刷新 Top-1 识别率，幅度不高，趋近于饱和，但是该模型在一些鲁棒性测试数据集上的提高确实惊人的。
论文地址：https://arxiv.org/pdf/1911.04252.pdf
  1. 数据集简介 2. 2.Self-training 方法 3. Noisy Student  1. 数据集简介 mage-A/C/P 是三种不同的鲁棒性测试数据集。
Image-A: A 是 Adversarial，对抗的意思。从真实世界搜集了 7500 张未经修改、完全自然生成的图片作为对抗样本测试常规 ImageNet 下训练的模型鲁棒性，以 DenseNet-121 为例，其测试准确率仅为 2%，准确率下降了约 90%，由此可知该数据集的难度。
其中红色的是标签是 ResNet-50 模型给出的，黑色的标签是实际真实标签，而且以很高的信任度识别错误，要注意虽然这些样本是对抗样本，但都是来自真实世界未加对抗调整的自然图片。
Image-C/P：C 是 Corruption 腐蚀、污染的意思，即在图片中引入 75 种不同的噪音形式，测试模型的抗击能力。
这里给出 15 种不同类型的算法干扰，如引入噪声，模糊处理，模仿天气因素干扰以及基于色彩空间数字化干扰等；P 是 Perturbations 扰动的意思，它与 C 差不多，但是会在一个连续的时间步内连续对一个干净的原始图像做处理，且每一次处理都是微小的扰动，比如平移像素点，旋转，缩放以及 C 中使用的干扰模糊等扰动。
2. 2.Self-training 方法 Self-training是最简单的半监督方法之一，其主要思想是找到一种方法，用未标记的数据集来扩充已标记的数据集。算法流程如下：
（1）首先，利用已标记的数据来训练一个好的模型，然后使用这个模型对未标记的数据进行标记。
（2）然后，进行伪标签的生成，因为我们知道，已训练好的模型对未标记数据的所有预测都不可能都是好的，因此对于经典的Self-training，通常是使用分数阈值过滤部分预测，以选择出未标记数据的预测标签的一个子集。
（3）其次，将生成的伪标签与原始的标记数据相结合，并在合并后数据上进行联合训练。
（4）整个过程可以重复n次，直到达到收敛。
3. Noisy Student Self-training是利用未标记数据的好方法。但这篇来自Google的文章却强调了Noisy Student。怎么回事？它与经典方法有什么不同吗？
Noisy Student的作者发现，要使这种方法发挥作用，student model在训练过程中应加噪声，如dropout, stochastic depth andaugmentation等。而teacher model在产生伪标签时不应加噪声。因为Noisy 是整个算法的一个重要部分，所以他们称之为“Noisy Student”。</description>
    </item>
    
    <item>
      <title>【现场分享】智源大会类脑视觉</title>
      <link>https://ioyy900205.github.io/post/2021-05-31-%E7%8E%B0%E5%9C%BA%E5%88%86%E4%BA%AB%E6%99%BA%E6%BA%90%E5%A4%A7%E4%BC%9A%E7%B1%BB%E8%84%91%E8%A7%86%E8%A7%89/</link>
      <pubDate>Mon, 31 May 2021 10:41:41 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-31-%E7%8E%B0%E5%9C%BA%E5%88%86%E4%BA%AB%E6%99%BA%E6%BA%90%E5%A4%A7%E4%BC%9A%E7%B1%BB%E8%84%91%E8%A7%86%E8%A7%89/</guid>
      <description>【现场分享】智源大会类脑视觉
 @TOC
标题 黄铁军
唐华锦
张兆翔
王威
类脑 拓展马尔视觉计算原理
研究内容 计算 成像 应用
 参考资料</description>
    </item>
    
    <item>
      <title>21——MSDNet论文的推理模块</title>
      <link>https://ioyy900205.github.io/post/2021-05-22-21msdnet%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Sat, 22 May 2021 10:21:24 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-22-21msdnet%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</guid>
      <description>本文的核心主旨在于：在计算资源限制下对不同的图像进行不同的处理，可以理解成对于简单样本采用简单的方式处理，对于复杂样本则尽可能给其分配资源，以避免不必要的资源浪费节省计算量，并且在这种推理的思想上要实现网络对数据的自动适应。所以本文设计了一个新颖的二维多尺度网络结构，根据不同的资源需求训练了多个分类器，为了最大程度地重用分类器之间的计算，我们将它们作为早期出口并入单个深度卷积神经网络，并通过密集连接将它们互连，该构架在整个网络中同时保持粗略和精细的scale，获得了良好的效果。
  1. model 2. 推理部分  2.1. 实时推理方法 2.2. 出口分配   3. 讨论  1. model 这里有两个基本模块，一个是第一层的横向传播模块，一个是下采样加横向传播模块。
2. 推理部分 2.1. 实时推理方法 其实就是将每个出口的结果打印出来。
只不过在推理过程中，考虑了载入数据的时间（大约0.45s），整个batch的推理时间大约是0.6-0.7s&amp;rsquo;s之间。
程序在最后的地方对每个出口进行了输出
 prec@1 56.632 prec@5 79.942 prec@1 65.136 prec@5 86.252 prec@1 68.420 prec@5 88.632 prec@1 69.770 prec@5 89.418 prec@1 71.336 prec@5 90.364   2.2. 出口分配 这种方式对5个出口进行了样本数量设定（这里设置了40组）。进而可以学习到每个出口的threshold，从而实现了不同出口的退出机制。
3. 讨论 每个出口样本数量设定 是一个超参数。MSDNet所得到的结果目前来看不一定是最好的。
以下有收获：
 多尺度架构的编程方法。 网络模型参数和计算量的计算。 退出机制的学习方法。   参考资料
手动debug</description>
    </item>
    
    <item>
      <title>12————阅读论文记录</title>
      <link>https://ioyy900205.github.io/post/2021-05-14-12%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87%E9%93%BE%E6%8E%A5/</link>
      <pubDate>Fri, 14 May 2021 15:20:41 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-14-12%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87%E9%93%BE%E6%8E%A5/</guid>
      <description>阅读论文记录
  1. 图像分类(Classification) 2. 目标检测(Object Detection) 3. 目标分割(Segmentation) 4. Others 5. 动态神经网络   参考资料
 1. 图像分类(Classification)  AlexNet http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf ZFNet(Visualizing and Understanding Convolutional Networks) https://arxiv.org/abs/1311.2901 VGG https://arxiv.org/abs/1409.1556 GoogLeNet, Inceptionv1(Going deeper with convolutions) https://arxiv.org/abs/1409.4842 Batch Normalization https://arxiv.org/abs/1502.03167 Inceptionv3(Rethinking the Inception Architecture for Computer Vision) https://arxiv.org/abs/1512.00567 Inceptionv4, Inception-ResNet https://arxiv.org/abs/1602.07261 Xception(Deep Learning with Depthwise Separable Convolutions) https://arxiv.org/abs/1610.02357 ResNet https://arxiv.org/abs/1512.03385 ResNeXt https://arxiv.org/abs/1611.05431 DenseNet https://arxiv.org/abs/1608.06993 NASNet-A(Learning Transferable Architectures for Scalable Image Recognition) https://arxiv.</description>
    </item>
    
    <item>
      <title>审稿学习系列01——图像质量评估</title>
      <link>https://ioyy900205.github.io/post/2021-05-10-%E5%AE%A1%E7%A8%BF%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%9701/</link>
      <pubDate>Mon, 10 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-10-%E5%AE%A1%E7%A8%BF%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%9701/</guid>
      <description>审稿学习系列01——图像质量评估 LIU Liang
  1. 背景 2. 图像质量评估（Image Quality Assessment, IQA）——方法分类  2.1. 主观方法 2.2. 客观方法   3. 图像质量评估（Image Quality Assessment, IQA）——图像提供信息分类  3.1. 全参考(Full Reference-IQA, FR-IQA) 3.2. 半参考(Reduced Reference-IQA, RR-IQA) 3.3. 无参考(No Reference-IQA, NR-IQA)   4. 数据集 5. 评估方法  5.1. 评估指标   6. 结果  1. 背景 质量评估(Quality Assessment，QA)在许多领域有其广泛的实用性。（比如图像压缩、视频编解码、视频监控等。）
并且对高效、可靠质量评估的需求日益增加，所以QA成为一个感兴趣的研究领域。
每年都涌现出大量的新的QA算法，有些是扩展已有的算法，也有一些是QA算法的应用。
质量评估可分为：
  图像质量评估（Image Quality Assessment, IQA）
  视频质量评估（Video Quality Assessment, VQA）</description>
    </item>
    
  </channel>
</rss>
