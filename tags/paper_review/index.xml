<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Paper_Review on 亮的笔记</title>
    <link>https://ioyy900205.github.io/tags/paper_review/</link>
    <description>Recent content in Paper_Review on 亮的笔记</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Fri, 06 Aug 2021 14:30:00 +0800</lastBuildDate><atom:link href="https://ioyy900205.github.io/tags/paper_review/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>41-NCNN模型部署</title>
      <link>https://ioyy900205.github.io/post/2021-08-06-41-ncnn%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E4%B8%80/</link>
      <pubDate>Fri, 06 Aug 2021 14:30:00 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-08-06-41-ncnn%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E4%B8%80/</guid>
      <description>争取比全网做的都要清晰
 @TOC
文件变化 整个工作流程文件有：
1.pth文件
2.onnx文件 （普通onnx、onnxsim）
3.ncnn文件 （普通ncnn、ncnnoptimal、ncnn压缩）
编译问题 多的就解释不了了，看代码
-std=c++11 -I/usr/local/include/opencv4 -I/home/liuliang/ncnn/build/install/include/ncnn -L/home/liuliang/ncnn/build/install/lib -lncnn -fopenmp -lopencv_imgcodecs -lopencv_flann -lopencv_core  参考资料</description>
    </item>
    
    <item>
      <title>35——NMS非极大值抑制</title>
      <link>https://ioyy900205.github.io/post/2021-07-05-35nms/</link>
      <pubDate>Mon, 05 Jul 2021 15:14:54 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-07-05-35nms/</guid>
      <description>代码实现
 @TOC
1. 代码 &amp;#39;&amp;#39;&amp;#39; Date: 2021-07-05 14:21:06 LastEditors: Liuliang LastEditTime: 2021-07-05 15:16:44 Description: &amp;#39;&amp;#39;&amp;#39; import numpy as np # import cv2 import matplotlib.pyplot as plt def py_cpu_nms(dets, thresh): &amp;#34;&amp;#34;&amp;#34;Pure Python NMS baseline.&amp;#34;&amp;#34;&amp;#34; # x1、y1、x2、y2、以及score赋值 x1 = dets[:, 0] y1 = dets[:, 1] x2 = dets[:, 2] y2 = dets[:, 3] scores = dets[:, 4] # 每一个检测框的面积 areas = (x2 - x1 ) * (y2 - y1 ) # 按照score置信度降序排序 order = scores.</description>
    </item>
    
    <item>
      <title>30——强化学习</title>
      <link>https://ioyy900205.github.io/post/2021-06-16-30%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Wed, 16 Jun 2021 11:25:16 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-16-30%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</guid>
      <description>维基百科：
强化学习（RL）是机器学习的一个领域，涉及软件代理如何在环境中采取行动以最大化一些累积奖励的概念。该问题由于其一般性，在许多其他学科中得到研究，如博弈论，控制理论，运筹学，信息论，基于仿真的优化，多智能体系统，群智能，统计和遗传算法。在运筹学和控制文献中，强化学习被称为近似动态规划或神经动态规划。
百度：
强化学习(reinforcement learning)，又称再励学习、评价学习，是一种重要的机器学习方法，在智能控制机器人及分析预测等领域有许多应用。
但在传统的机器学习分类中没有提到过强化学习，而在连接主义学习中，把学习算法分为三种类型，即非监督学习(unsupervised learning)、监督学习(supervised leaning)和强化学习。
 @TOC
强化学习的主流算法 免模型学习（Model-Free） vs 有模型学习（Model-Based）
在介绍详细算法之前，我们先来了解一下强化学习算法的2大分类。这2个分类的重要差异是：智能体是否能完整了解或学习到所在环境的模型
有模型学习（Model-Based）对环境有提前的认知，可以提前考虑规划，但是缺点是如果模型跟真实世界不一致，那么在实际使用场景下会表现的不好。
免模型学习（Model-Free）放弃了模型学习，在效率上不如前者，但是这种方式更加容易实现，也容易在真实场景下调整到很好的状态。所以免模型学习方法更受欢迎，得到更加广泛的开发和测试。
 参考资料
https://easyai.tech/ai-definition/reinforcement-learning/</description>
    </item>
    
    <item>
      <title>29——GRU 3步介绍</title>
      <link>https://ioyy900205.github.io/post/2021-06-09-29gru%E4%B8%89%E6%AD%A5/</link>
      <pubDate>Wed, 09 Jun 2021 16:05:03 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-09-29gru%E4%B8%89%E6%AD%A5/</guid>
      <description>29——GRU 3步介绍
GRU它引⼊了**重置⻔（reset gate）和更新⻔（update gate）**的概念，从而修改了循环神经⽹络中隐藏状态的计算⽅式。
 @TOC
1. ⻔控循环单元 1.1. 重置门和更新门 1.2. 候选隐藏状态 1.3. 隐藏状态 我们对⻔控循环单元的设计稍作总结：
重置⻔有助于捕捉时间序列⾥短期的依赖关系； 更新⻔有助于捕捉时间序列⾥⻓期的依赖关系。
 参考资料</description>
    </item>
    
    <item>
      <title>28——训练trick之优化器</title>
      <link>https://ioyy900205.github.io/post/2021-06-09-28%E8%AE%AD%E7%BB%83trick%E4%B9%8B%E4%BC%98%E5%8C%96%E5%99%A8/</link>
      <pubDate>Wed, 09 Jun 2021 15:24:43 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-09-28%E8%AE%AD%E7%BB%83trick%E4%B9%8B%E4%BC%98%E5%8C%96%E5%99%A8/</guid>
      <description>在机器学习的场景下，梯度下降学习的目标通常是最小化机器学习问题的损失函数。 一个好的算法能够快速可靠地找到最小值.(也就是说，它不会陷入局部极小值、鞍点或高原区域，而是寻找全局最小值)。
 @TOC
标题  参考资料
https://zhuanlan.zhihu.com/p/147275344</description>
    </item>
    
    <item>
      <title>25——GFNet论文阅读</title>
      <link>https://ioyy900205.github.io/post/2021-06-05-25gfnet/</link>
      <pubDate>Sat, 05 Jun 2021 10:54:57 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-05-25gfnet/</guid>
      <description>NeurIPS 2020录用的一篇论文：《Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classiﬁcation
论文链接：https://arxiv.org/pdf/2010.05300.pdf
代码和预训练模型链接：https://github.com/blackfeather-wang/GFNet-Pytorch
论文第一作者：
王语霖，清华大学自动化系直博二年级，导师为吴澄院士和黄高助理教授，研究兴趣为深度学习与计算机视觉，在NeurIPS 2019/2020以第一作者发表两篇学术论文。
 @TOC
1. 研究动机及简介 推理CNN所需的计算量（FLOPs）基本与像素数目成正比，即与图形的长、宽成二次关系。
在实际应用（例如手机APP、自动驾驶系统、图片搜索引擎）中，计算量往往正比于能耗或者时间开销，显然，无论出于成本因素还是从安全性和用户体验的角度考虑，网络的计算开销都应当尽可能小。
这便是本文所提出方法的出发点，我们的目标是，对于输入图片，自适应地找到其与任务最相关的区域，进而通过使神经网络只处理这些区域，以尽可能小的计算量得到可信的结果。具体而言，我们采用的方法是，将一张分辨率较高的图片表征为若干个包含其关键部分的“小块”（Patch），而后仅将这些小块输入神经网络。以下面的示意图为例，将一张224x224的图片分解为3个96x96的Patch进行处理所需的计算量仅为原图的55.2%。
2. Method 为了实现上述目的，事实上，有两个显然的困难：
(a) 任意给定一张输入图片，如何判断其与任务最相关的区域在哪里呢？
(b) 考虑到我们的最终目的是使神经网络得到正确的预测结果，不同输入所需的计算量是不一样的，例如对于下面所示的两个输入图片，神经网络可能仅需要处理一个patch就能识别出特征非常突出的月亮，但是需要处理更多的patch才能分辨出猫咪的具体品种。
为了解决这两个问题，我们设计了一个Glance and Focus的框架，将这一思路建模为了一个序列决策过程，如下图所示。
其具体执行流程为：
 首先，对于一张任意给定的输入图片，由于我们没有任何关于它的先验知识，我们直接将其放缩为一个patch的大小，输入网络，这一方面产生了一个初步的判断结果，另一方面也提供了原始输入图片的空间分布信息；这一阶段称为扫视（Glance）。 而后，我们再以这些基本的空间分布信息为基础，逐步从原图上取得高分辨率的patch，将其不断输入网络，以此逐步更新预测结果和空间分布信息，得到更为准确的判断，并逐步寻找神经网络尚未见到过的关键区域；这一阶段称为关注（Focus）。  值得注意的是，在上述序列过程的每一步结束之后，我们会将神经网络的预测自信度（confidence）与一个预先定义的阈值进行比较，一旦confidence超过阈值，我们便视为网络已经得到了可信的结果，这一过程立即终止。此机制称为自适应推理（Adaptive Inference）。通过这种机制，我们一方面可以使不同难易度的样本具有不同的序列长度，从而动态分配计算量、提高整体效率；另一方面可以简单地通过改变阈值调整网络的整体计算开销，而不需要重新训练网络，这使得我们的模型可以动态地以最小的计算开销达到所需的性能，或者实时最大化地利用所有可用的计算资源以提升模型表现。
3. 显而易见的困难 how to identify class-ciscriminative regions? 如何识别阶级歧视性区域？
how to determine the number of class-discriminative regions? 如何确定类区分区域的数量？
4. 网络结构 GFNet共有四个组件，分别为：
全局编码器${f_g}$和局部编码器${f_l}$ （Global Encoder and Local Encoder）为两个CNN，分别用于从放缩后的原图和局部patch中提取信息，之所以用两个CNN，是因为我们发现一个CNN很难同时适应缩略图和局部patch两种尺度（scale）的输入。几乎所有现有的网络结构均可以作为这两个编码器以提升其推理效率（如MobileNet-V3、EfficientNet、RegNet等）。 分类器 [公式] （Classifier）为一个循环神经网络（RNN），输入为全局池化后的特征向量，用于整合过去所有输入的信息，以得到目前最优的分类结果。 图像块选择网络 [公式] （Patch Proposal Network）是另一个循环神经网络（RNN），输入为全局池化前的特征图（不做池化是为了避免损失空间信息），用于整合目前为止所有的空间分布信息，并决定下一个patch的位置。值得注意的是由于取得patch的crop操作不可求导，[公式]是使用强化学习中的策略梯度方法（policy gradient）训练的。</description>
    </item>
    
    <item>
      <title>25——Noisy Student训练</title>
      <link>https://ioyy900205.github.io/post/2021-06-04-24noisy-student%E6%8F%90%E9%AB%98%E7%B2%BE%E5%BA%A6%E7%9A%84%E7%A5%9E%E5%99%A8/</link>
      <pubDate>Fri, 04 Jun 2021 14:47:02 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-04-24noisy-student%E6%8F%90%E9%AB%98%E7%B2%BE%E5%BA%A6%E7%9A%84%E7%A5%9E%E5%99%A8/</guid>
      <description>想要提高模型的精度和鲁棒性，尝试考虑使用无标注数据！
实际在普通的 ImageNet 数据集上，近年来的模型不断在刷新 Top-1 识别率，幅度不高，趋近于饱和，但是该模型在一些鲁棒性测试数据集上的提高确实惊人的。
论文地址：https://arxiv.org/pdf/1911.04252.pdf
  1. 数据集简介 2. 2.Self-training 方法 3. Noisy Student  1. 数据集简介 mage-A/C/P 是三种不同的鲁棒性测试数据集。
Image-A: A 是 Adversarial，对抗的意思。从真实世界搜集了 7500 张未经修改、完全自然生成的图片作为对抗样本测试常规 ImageNet 下训练的模型鲁棒性，以 DenseNet-121 为例，其测试准确率仅为 2%，准确率下降了约 90%，由此可知该数据集的难度。
其中红色的是标签是 ResNet-50 模型给出的，黑色的标签是实际真实标签，而且以很高的信任度识别错误，要注意虽然这些样本是对抗样本，但都是来自真实世界未加对抗调整的自然图片。
Image-C/P：C 是 Corruption 腐蚀、污染的意思，即在图片中引入 75 种不同的噪音形式，测试模型的抗击能力。
这里给出 15 种不同类型的算法干扰，如引入噪声，模糊处理，模仿天气因素干扰以及基于色彩空间数字化干扰等；P 是 Perturbations 扰动的意思，它与 C 差不多，但是会在一个连续的时间步内连续对一个干净的原始图像做处理，且每一次处理都是微小的扰动，比如平移像素点，旋转，缩放以及 C 中使用的干扰模糊等扰动。
2. 2.Self-training 方法 Self-training是最简单的半监督方法之一，其主要思想是找到一种方法，用未标记的数据集来扩充已标记的数据集。算法流程如下：
（1）首先，利用已标记的数据来训练一个好的模型，然后使用这个模型对未标记的数据进行标记。
（2）然后，进行伪标签的生成，因为我们知道，已训练好的模型对未标记数据的所有预测都不可能都是好的，因此对于经典的Self-training，通常是使用分数阈值过滤部分预测，以选择出未标记数据的预测标签的一个子集。
（3）其次，将生成的伪标签与原始的标记数据相结合，并在合并后数据上进行联合训练。
（4）整个过程可以重复n次，直到达到收敛。
3. Noisy Student Self-training是利用未标记数据的好方法。但这篇来自Google的文章却强调了Noisy Student。怎么回事？它与经典方法有什么不同吗？
Noisy Student的作者发现，要使这种方法发挥作用，student model在训练过程中应加噪声，如dropout, stochastic depth andaugmentation等。而teacher model在产生伪标签时不应加噪声。因为Noisy 是整个算法的一个重要部分，所以他们称之为“Noisy Student”。</description>
    </item>
    
    <item>
      <title>【现场分享】智源大会类脑视觉</title>
      <link>https://ioyy900205.github.io/post/2021-05-31-%E7%8E%B0%E5%9C%BA%E5%88%86%E4%BA%AB%E6%99%BA%E6%BA%90%E5%A4%A7%E4%BC%9A%E7%B1%BB%E8%84%91%E8%A7%86%E8%A7%89/</link>
      <pubDate>Mon, 31 May 2021 10:41:41 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-31-%E7%8E%B0%E5%9C%BA%E5%88%86%E4%BA%AB%E6%99%BA%E6%BA%90%E5%A4%A7%E4%BC%9A%E7%B1%BB%E8%84%91%E8%A7%86%E8%A7%89/</guid>
      <description>【现场分享】智源大会类脑视觉
 @TOC
标题 黄铁军
唐华锦
张兆翔
王威
类脑 拓展马尔视觉计算原理
研究内容 计算 成像 应用
 参考资料</description>
    </item>
    
    <item>
      <title>21——MSDNet论文的推理模块</title>
      <link>https://ioyy900205.github.io/post/2021-05-22-21msdnet%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Sat, 22 May 2021 10:21:24 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-22-21msdnet%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</guid>
      <description>本文的核心主旨在于：在计算资源限制下对不同的图像进行不同的处理，可以理解成对于简单样本采用简单的方式处理，对于复杂样本则尽可能给其分配资源，以避免不必要的资源浪费节省计算量，并且在这种推理的思想上要实现网络对数据的自动适应。所以本文设计了一个新颖的二维多尺度网络结构，根据不同的资源需求训练了多个分类器，为了最大程度地重用分类器之间的计算，我们将它们作为早期出口并入单个深度卷积神经网络，并通过密集连接将它们互连，该构架在整个网络中同时保持粗略和精细的scale，获得了良好的效果。
  1. model 2. 推理部分  2.1. 实时推理方法 2.2. 出口分配   3. 讨论  1. model 这里有两个基本模块，一个是第一层的横向传播模块，一个是下采样加横向传播模块。
2. 推理部分 2.1. 实时推理方法 其实就是将每个出口的结果打印出来。
只不过在推理过程中，考虑了载入数据的时间（大约0.45s），整个batch的推理时间大约是0.6-0.7s&amp;rsquo;s之间。
程序在最后的地方对每个出口进行了输出
 prec@1 56.632 prec@5 79.942 prec@1 65.136 prec@5 86.252 prec@1 68.420 prec@5 88.632 prec@1 69.770 prec@5 89.418 prec@1 71.336 prec@5 90.364   之前有统计过计算量的地方，所以这里结合计算量就可以得到论文的结果。
2.2. 出口分配 这种方式对5个出口进行了样本数量设定（这里设置了40组）。进而可以学习到每个出口的threshold，从而实现了不同出口的退出机制。
3. 讨论 每个出口样本数量设定 是一个超参数。MSDNet所得到的结果目前来看不一定是最好的。
以下有收获：
 多尺度架构的编程方法。 网络模型参数和计算量的计算。 退出机制的学习方法。   参考资料
手动debug</description>
    </item>
    
    <item>
      <title>12————阅读论文记录</title>
      <link>https://ioyy900205.github.io/post/2021-05-14-12%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87%E9%93%BE%E6%8E%A5/</link>
      <pubDate>Fri, 14 May 2021 15:20:41 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-14-12%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87%E9%93%BE%E6%8E%A5/</guid>
      <description>阅读论文记录
  1. 图像分类(Classification) 2. 目标检测(Object Detection) 3. 目标分割(Segmentation) 4. Others 5. 动态神经网络   参考资料
 1. 图像分类(Classification)  AlexNet http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf ZFNet(Visualizing and Understanding Convolutional Networks) https://arxiv.org/abs/1311.2901 VGG https://arxiv.org/abs/1409.1556 GoogLeNet, Inceptionv1(Going deeper with convolutions) https://arxiv.org/abs/1409.4842 Batch Normalization https://arxiv.org/abs/1502.03167 Inceptionv3(Rethinking the Inception Architecture for Computer Vision) https://arxiv.org/abs/1512.00567 Inceptionv4, Inception-ResNet https://arxiv.org/abs/1602.07261 Xception(Deep Learning with Depthwise Separable Convolutions) https://arxiv.org/abs/1610.02357 ResNet https://arxiv.org/abs/1512.03385 ResNeXt https://arxiv.org/abs/1611.05431 DenseNet https://arxiv.org/abs/1608.06993 NASNet-A(Learning Transferable Architectures for Scalable Image Recognition) https://arxiv.</description>
    </item>
    
    <item>
      <title>审稿学习系列01——图像质量评估</title>
      <link>https://ioyy900205.github.io/post/2021-05-10-%E5%AE%A1%E7%A8%BF%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%9701/</link>
      <pubDate>Mon, 10 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-10-%E5%AE%A1%E7%A8%BF%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%9701/</guid>
      <description>审稿学习系列01——图像质量评估 LIU Liang
  1. 背景 2. 图像质量评估（Image Quality Assessment, IQA）——方法分类  2.1. 主观方法 2.2. 客观方法   3. 图像质量评估（Image Quality Assessment, IQA）——图像提供信息分类  3.1. 全参考(Full Reference-IQA, FR-IQA) 3.2. 半参考(Reduced Reference-IQA, RR-IQA) 3.3. 无参考(No Reference-IQA, NR-IQA)   4. 数据集 5. 评估方法  5.1. 评估指标   6. 结果  1. 背景 质量评估(Quality Assessment，QA)在许多领域有其广泛的实用性。（比如图像压缩、视频编解码、视频监控等。）
并且对高效、可靠质量评估的需求日益增加，所以QA成为一个感兴趣的研究领域。
每年都涌现出大量的新的QA算法，有些是扩展已有的算法，也有一些是QA算法的应用。
质量评估可分为：
  图像质量评估（Image Quality Assessment, IQA）
  视频质量评估（Video Quality Assessment, VQA）</description>
    </item>
    
  </channel>
</rss>
