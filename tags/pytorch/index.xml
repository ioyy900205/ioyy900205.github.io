<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>PyTorch on 亮的笔记</title>
    <link>https://ioyy900205.github.io/tags/pytorch/</link>
    <description>Recent content in PyTorch on 亮的笔记</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Fri, 21 May 2021 15:06:07 +0800</lastBuildDate><atom:link href="https://ioyy900205.github.io/tags/pytorch/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>19——avg_pool2d</title>
      <link>https://ioyy900205.github.io/post/2021-05-21-19avg_pool2d/</link>
      <pubDate>Fri, 21 May 2021 15:06:07 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-21-19avg_pool2d/</guid>
      <description>avg——pool2d理解
 @TOC
1. 直接上代码 import torch from torch.nn import functional as F # 1.初始化 input = torch.tensor( [ [1,1,1,1,1], [1,1,1,1,1], [0,0,0,1,1], [1,1,1,1,1] ]).unsqueeze(0).float() print(input.size())#torch.Size([1, 4, 5]) print(input) #2. avg_pool1d # m1 = F.avg_pool1d(input,kernel_size=2) # print(m1)#tensor([[[1.0000, 1.0000], # # [1.0000, 1.0000], # # [0.0000, 0.5000], # # [1.0000, 1.0000]]]) #3. avg_pool2d # m = F.avg_pool2d(input,kernel_size=2)#这里是在原矩阵中找出2*2的区域求平均，不够的舍弃，stride=(2,2) # print(m)#tensor([[[1.0000, 1.0000], # # [0.5000, 0.7500]]]) m3= F.avg_pool2d(input,kernel_size=2,stride=1)#这里是在原矩阵中找出2*2的区域求平均，不够的舍弃，stride=(1,1),[1,1,4,5]--&amp;gt;[1,1,3,4] print(m3)#tensor([[[1.0000, 1.0000, 1.0000, 1.0000], # [0.5000, 0.</description>
    </item>
    
    <item>
      <title>17——tqdm载入数据集测试</title>
      <link>https://ioyy900205.github.io/post/2021-05-20-17tqdm%E8%BD%BD%E5%85%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E6%B5%8B%E8%AF%95/</link>
      <pubDate>Thu, 20 May 2021 16:50:49 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-20-17tqdm%E8%BD%BD%E5%85%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E6%B5%8B%E8%AF%95/</guid>
      <description>代码已放在mess_around文件夹下
  1. 基本使用方式  1.1. list方式 1.2. 放一个迭代器在里面   2. 实例  2.1. 使用tqdm带来的效率损失  2.1.1. MNIST 2.1.2. ImageNet   2.2. num_workers的使用    1. 基本使用方式 1.1. list方式 （1）
for i in tqdm(range(10000)): #do something sleep(0.01) pass （2）
for char in tqdm([&amp;#34;a&amp;#34;, &amp;#34;b&amp;#34;, &amp;#34;c&amp;#34;, &amp;#34;d&amp;#34;]): #do something pass （3）
pbar = tqdm([&amp;#34;a&amp;#34;, &amp;#34;b&amp;#34;, &amp;#34;c&amp;#34;, &amp;#34;d&amp;#34;]) for char in pbar: sleep(1) pbar.set_description(&amp;#34;Processing %s&amp;#34; % char) 1.2. 放一个迭代器在里面 count = 0 for batch_idx, item in tqdm(enumerate(train_loader)): count+=batch_idx print(count) 这种方式是我使用的方式。简单来说就是把迭代器train_loader放在tqdm里面，这里增加了一个enumerate去增加一个序列。</description>
    </item>
    
    <item>
      <title>15——torch.tensor叶子节点</title>
      <link>https://ioyy900205.github.io/post/2021-05-19-15torch.tensor-%E5%8F%B6%E5%AD%90%E8%8A%82%E7%82%B9/</link>
      <pubDate>Wed, 19 May 2021 09:53:52 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-19-15torch.tensor-%E5%8F%B6%E5%AD%90%E8%8A%82%E7%82%B9/</guid>
      <description>讨论叶子节点
  1. 理顺逻辑  1.1. 元素 1.2. 计算    1. 理顺逻辑 1.1. 元素 在pytorch的计算图中，只有两种元素：
数据（tensor）
  叶子节点(leaf node)
  叶子节点可以理解成不依赖其他tensor的tensor。
  在pytorch中，神经网络层中的权值w的tensor均为叶子节点。
  自己定义的tensor例如a=torch.tensor([1.0])定义的节点是叶子节点。
  All Tensors that have requires_grad which is False will be leaf Tensors by convention.
  For Tensors that have requires_grad which is True, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so grad_fn is None.</description>
    </item>
    
    <item>
      <title>13————tensor内存占用计算实例</title>
      <link>https://ioyy900205.github.io/post/2021-05-17-13tensor%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8%E8%AE%A1%E7%AE%97%E5%AE%9E%E4%BE%8B/</link>
      <pubDate>Mon, 17 May 2021 19:01:17 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-17-13tensor%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8%E8%AE%A1%E7%AE%97%E5%AE%9E%E4%BE%8B/</guid>
      <description>网上查看了CSDN的代码，写的有误。所以这里写一个正确版本的
  ## 1. 各类型所占用的字节数 2. tensor内存计算 3. 清空内存  1. 各类型所占用的字节数  测试代码
import numpy as np import sys # 32位整型 ai32 = np.array([], dtype=np.int32) bi32 = np.arange(1, dtype=np.int32) ci32 = np.arange(5, dtype=np.int32) # 64位整型 ai64 = np.array([], dtype=np.int64) bi64 = np.arange(1, dtype=np.int64) ci64 = np.arange(5, dtype=np.int64) # 32位浮点数 af32 = np.array([], dtype=np.float32) bf32 = np.arange(1, dtype=np.float32) cf32 = np.arange(5, dtype=np.float32) # 64位浮点数 af64 = np.array([], dtype=np.float64) bf64 = np.</description>
    </item>
    
    <item>
      <title>08——数据集操作和图像处理——PyTorch</title>
      <link>https://ioyy900205.github.io/post/2021-05-11-voc2012%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%9B%BE%E7%89%87%E5%92%8Cbbox%E4%BF%A1%E6%81%AF%E6%98%BE%E7%A4%BA/</link>
      <pubDate>Tue, 11 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-11-voc2012%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%9B%BE%E7%89%87%E5%92%8Cbbox%E4%BF%A1%E6%81%AF%E6%98%BE%E7%A4%BA/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. 数据集读取 2. 转换函数 3. bbox画图  1. 数据集读取 from torch.utils.data import Dataset import os import torch import json from PIL import Image from lxml import etree class VOC2012DataSet(Dataset): &amp;#34;&amp;#34;&amp;#34;读取解析PASCAL VOC2012数据集&amp;#34;&amp;#34;&amp;#34; def __init__(self, voc_root, transforms, txt_name: str = &amp;#34;train.txt&amp;#34;): self.root = os.path.join(voc_root, &amp;#34;VOCdevkit&amp;#34;, &amp;#34;VOC2012&amp;#34;) self.img_root = os.path.join(self.root, &amp;#34;JPEGImages&amp;#34;) self.annotations_root = os.path.join(self.root, &amp;#34;Annotations&amp;#34;) # read train.txt or val.txt file txt_path = os.path.join(self.root, &amp;#34;ImageSets&amp;#34;, &amp;#34;Main&amp;#34;, txt_name) assert os.</description>
    </item>
    
    <item>
      <title>06——torch.cat维度操作——PyTorch</title>
      <link>https://ioyy900205.github.io/post/2021-05-10-torch.cat%E7%9A%84%E7%BB%B4%E5%BA%A6%E6%93%8D%E4%BD%9C/</link>
      <pubDate>Mon, 10 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-10-torch.cat%E7%9A%84%E7%BB%B4%E5%BA%A6%E6%93%8D%E4%BD%9C/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. torch.cat  1.1. 二维数组  1.1.1. dim=0 1.1.2. dim=1   1.2. 三维数组  1.2.1. dim=0 1.2.2. dim=1 1.2.3. dim=2     2. 小结  1. torch.cat 1.1. 二维数组 1.1.1. dim=0 运行
import torch A = torch.ones(2,3) #2x3的张量（矩阵）  print(&amp;#34;A:&amp;#34;,A) B=2*torch.ones(4,3) #4x3的张量（矩阵）  print(&amp;#34;B:&amp;#34;,B) C=torch.cat((A,B),0)#按维数0（行）拼接 print(&amp;#34;C:&amp;#34;,C) print(C.size()) 结果
A: tensor([[1., 1., 1.], [1., 1., 1.]]) B: tensor([[2., 2., 2.], [2., 2., 2.], [2.</description>
    </item>
    
    <item>
      <title>07——torch.stack维度操作——PyTorch</title>
      <link>https://ioyy900205.github.io/post/2021-05-10-torch.stack%E7%9A%84%E7%BB%B4%E5%BA%A6%E6%93%8D%E4%BD%9C/</link>
      <pubDate>Mon, 10 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-10-torch.stack%E7%9A%84%E7%BB%B4%E5%BA%A6%E6%93%8D%E4%BD%9C/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. 初始化 2. torch.stak  2.1. dim = 0 2.2. dim = 1 2.3. dim = 2    1. 初始化 input
import torch import numpy as np # 创建3*3的矩阵，a、b a=np.array([[1,2,3],[4,5,6],[7,8,9]]) b=np.array([[10,20,30],[40,50,60],[70,80,90]]) # 将矩阵转化为Tensor a = torch.from_numpy(a) b = torch.from_numpy(b) # 打印a、b、c print(a,a.size()) print(b,b.size()) output
tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) torch.Size([3, 3]) tensor([[10, 20, 30], [40, 50, 60], [70, 80, 90]]) torch.</description>
    </item>
    
    <item>
      <title>03——nn.module学习——PyTorch</title>
      <link>https://ioyy900205.github.io/post/2021-04-27-nn.module/</link>
      <pubDate>Tue, 27 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-04-27-nn.module/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. nn.moduel初步  1.1. Linear类 1.2. Conv2d类 1.3. 自定义层的步骤   2. 自定义层  2.1. 定义一个自定义层MyLayer 2.2. 自定义模型并且训练    1. nn.moduel初步 keras更加注重的是层Layer、pytorch更加注重的是模型Module。
1.1. Linear类 import math import torch from torch.nn.parameter import Parameter from .. import functional as F from .. import init from .module import Module from ..._jit_internal import weak_module, weak_script_method class Linear(Module): __constants__ = [&amp;#39;bias&amp;#39;] def __init__(self, in_features, out_features, bias=True): super(Linear, self).__init__() self.</description>
    </item>
    
  </channel>
</rss>
