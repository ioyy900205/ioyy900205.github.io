<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>PyTorch on 亮的笔记</title>
    <link>https://ioyy900205.github.io/tags/pytorch/</link>
    <description>Recent content in PyTorch on 亮的笔记</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 05 Jun 2021 10:54:57 +0800</lastBuildDate><atom:link href="https://ioyy900205.github.io/tags/pytorch/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>25——GFNet</title>
      <link>https://ioyy900205.github.io/post/2021-06-05-25gfnet/</link>
      <pubDate>Sat, 05 Jun 2021 10:54:57 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-05-25gfnet/</guid>
      <description>NeurIPS 2020录用的一篇论文：《Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classiﬁcation
论文链接：https://arxiv.org/pdf/2010.05300.pdf
代码和预训练模型链接：https://github.com/blackfeather-wang/GFNet-Pytorch
论文第一作者：
王语霖，清华大学自动化系直博二年级，导师为吴澄院士和黄高助理教授，研究兴趣为深度学习与计算机视觉，在NeurIPS 2019/2020以第一作者发表两篇学术论文。
 @TOC
研究动机及简介 推理CNN所需的计算量（FLOPs）基本与像素数目成正比，即与图形的长、宽成二次关系。
在实际应用（例如手机APP、自动驾驶系统、图片搜索引擎）中，计算量往往正比于能耗或者时间开销，显然，无论出于成本因素还是从安全性和用户体验的角度考虑，网络的计算开销都应当尽可能小。
这便是本文所提出方法的出发点，我们的目标是，对于输入图片，自适应地找到其与任务最相关的区域，进而通过使神经网络只处理这些区域，以尽可能小的计算量得到可信的结果。具体而言，我们采用的方法是，将一张分辨率较高的图片表征为若干个包含其关键部分的“小块”（Patch），而后仅将这些小块输入神经网络。以下面的示意图为例，将一张224x224的图片分解为3个96x96的Patch进行处理所需的计算量仅为原图的55.2%。
Method 为了实现上述目的，事实上，有两个显然的困难：
(a) 任意给定一张输入图片，如何判断其与任务最相关的区域在哪里呢？
(b) 考虑到我们的最终目的是使神经网络得到正确的预测结果，不同输入所需的计算量是不一样的，例如对于下面所示的两个输入图片，神经网络可能仅需要处理一个patch就能识别出特征非常突出的月亮，但是需要处理更多的patch才能分辨出猫咪的具体品种。
为了解决这两个问题，我们设计了一个Glance and Focus的框架，将这一思路建模为了一个序列决策过程，如下图所示。
其具体执行流程为：
 首先，对于一张任意给定的输入图片，由于我们没有任何关于它的先验知识，我们直接将其放缩为一个patch的大小，输入网络，这一方面产生了一个初步的判断结果，另一方面也提供了原始输入图片的空间分布信息；这一阶段称为扫视（Glance）。 而后，我们再以这些基本的空间分布信息为基础，逐步从原图上取得高分辨率的patch，将其不断输入网络，以此逐步更新预测结果和空间分布信息，得到更为准确的判断，并逐步寻找神经网络尚未见到过的关键区域；这一阶段称为关注（Focus）。  值得注意的是，在上述序列过程的每一步结束之后，我们会将神经网络的预测自信度（confidence）与一个预先定义的阈值进行比较，一旦confidence超过阈值，我们便视为网络已经得到了可信的结果，这一过程立即终止。此机制称为自适应推理（Adaptive Inference）。通过这种机制，我们一方面可以使不同难易度的样本具有不同的序列长度，从而动态分配计算量、提高整体效率；另一方面可以简单地通过改变阈值调整网络的整体计算开销，而不需要重新训练网络，这使得我们的模型可以动态地以最小的计算开销达到所需的性能，或者实时最大化地利用所有可用的计算资源以提升模型表现。
网络结构 GFNet共有四个组件，分别为：
全局编码器${f_g}$和局部编码器${f_l}$ （Global Encoder and Local Encoder）为两个CNN，分别用于从放缩后的原图和局部patch中提取信息，之所以用两个CNN，是因为我们发现一个CNN很难同时适应缩略图和局部patch两种尺度（scale）的输入。几乎所有现有的网络结构均可以作为这两个编码器以提升其推理效率（如MobileNet-V3、EfficientNet、RegNet等）。 分类器 [公式] （Classifier）为一个循环神经网络（RNN），输入为全局池化后的特征向量，用于整合过去所有输入的信息，以得到目前最优的分类结果。 图像块选择网络 [公式] （Patch Proposal Network）是另一个循环神经网络（RNN），输入为全局池化前的特征图（不做池化是为了避免损失空间信息），用于整合目前为止所有的空间分布信息，并决定下一个patch的位置。值得注意的是由于取得patch的crop操作不可求导，[公式]是使用强化学习中的策略梯度方法（policy gradient）训练的。
 参考资料
https://zhuanlan.zhihu.com/p/266306870</description>
    </item>
    
    <item>
      <title>23-【pytorch Lightning】</title>
      <link>https://ioyy900205.github.io/post/2021-06-03-23pytorch-lightning/</link>
      <pubDate>Thu, 03 Jun 2021 15:13:20 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-03-23pytorch-lightning/</guid>
      <description>大部分的DL/ML代码都可以分为以下这三部分：
 研究代码 Research code (LightningModule ) 工程代码 Engineering code ( Trainer ) 非必要代码 Non-essential code ( Callbacks )   @TOC
研究代码  参考资料</description>
    </item>
    
    <item>
      <title>22——VSCode中plt.show()无法显示图片问题</title>
      <link>https://ioyy900205.github.io/post/2021-06-03-22vscode%E4%B8%ADplt.show%E6%97%A0%E6%B3%95%E6%98%BE%E7%A4%BA%E5%9B%BE%E7%89%87%E9%97%AE%E9%A2%98/</link>
      <pubDate>Thu, 03 Jun 2021 14:24:36 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-03-22vscode%E4%B8%ADplt.show%E6%97%A0%E6%B3%95%E6%98%BE%E7%A4%BA%E5%9B%BE%E7%89%87%E9%97%AE%E9%A2%98/</guid>
      <description>远程SSh时候，VSCode无法显示图片
  1. 解决方法  1. 解决方法 终端中输入：
export DISPLAY=:10.0  参考资料 https://www.pythonheidong.com/blog/article/714470/11e6dbde8921d93ae464/</description>
    </item>
    
    <item>
      <title>20——torch_view操作指南</title>
      <link>https://ioyy900205.github.io/post/2021-05-21-20torch_view%E6%93%8D%E4%BD%9C%E6%8C%87%E5%8D%97/</link>
      <pubDate>Fri, 21 May 2021 18:13:22 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-21-20torch_view%E6%93%8D%E4%BD%9C%E6%8C%87%E5%8D%97/</guid>
      <description>还是发现对view的操作不够深入，这里做了一个小demo。自行体会一下
  1. 代码 2. 结果 3. 理解  1. 代码 &amp;#39;&amp;#39;&amp;#39; Date: 2021-05-21 15:43:43 LastEditors: Liuliang LastEditTime: 2021-05-21 15:50:36 Description: view &amp;#39;&amp;#39;&amp;#39; import torch a = torch.arange(0,12,1) print(a) b = a.view(2,-1) print(b) c = b.view(6,2) print(c) 2. 结果 tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) tensor([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) tensor([[ 0, 1], [ 2, 3], [ 4, 5], [ 6, 7], [ 8, 9], [10, 11]]) 3.</description>
    </item>
    
    <item>
      <title>19——avg_pool2d</title>
      <link>https://ioyy900205.github.io/post/2021-05-21-19avg_pool2d/</link>
      <pubDate>Fri, 21 May 2021 15:06:07 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-21-19avg_pool2d/</guid>
      <description>avg——pool2d理解
 @TOC
1. 直接上代码 import torch from torch.nn import functional as F # 1.初始化 input = torch.tensor( [ [1,1,1,1,1], [1,1,1,1,1], [0,0,0,1,1], [1,1,1,1,1] ]).unsqueeze(0).float() print(input.size())#torch.Size([1, 4, 5]) print(input) #2. avg_pool1d # m1 = F.avg_pool1d(input,kernel_size=2) # print(m1)#tensor([[[1.0000, 1.0000], # # [1.0000, 1.0000], # # [0.0000, 0.5000], # # [1.0000, 1.0000]]]) #3. avg_pool2d # m = F.avg_pool2d(input,kernel_size=2)#这里是在原矩阵中找出2*2的区域求平均，不够的舍弃，stride=(2,2) # print(m)#tensor([[[1.0000, 1.0000], # # [0.5000, 0.7500]]]) m3= F.avg_pool2d(input,kernel_size=2,stride=1)#这里是在原矩阵中找出2*2的区域求平均，不够的舍弃，stride=(1,1),[1,1,4,5]--&amp;gt;[1,1,3,4] print(m3)#tensor([[[1.0000, 1.0000, 1.0000, 1.0000], # [0.5000, 0.</description>
    </item>
    
    <item>
      <title>17——tqdm载入数据集测试</title>
      <link>https://ioyy900205.github.io/post/2021-05-20-17tqdm%E8%BD%BD%E5%85%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E6%B5%8B%E8%AF%95/</link>
      <pubDate>Thu, 20 May 2021 16:50:49 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-20-17tqdm%E8%BD%BD%E5%85%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E6%B5%8B%E8%AF%95/</guid>
      <description>代码已放在mess_around文件夹下
  1. 基本使用方式  1.1. list方式 1.2. 放一个迭代器在里面   2. 实例  2.1. 使用tqdm带来的效率损失  2.1.1. MNIST 2.1.2. ImageNet   2.2. num_workers的使用    1. 基本使用方式 1.1. list方式 （1）
for i in tqdm(range(10000)): #do something sleep(0.01) pass （2）
for char in tqdm([&amp;#34;a&amp;#34;, &amp;#34;b&amp;#34;, &amp;#34;c&amp;#34;, &amp;#34;d&amp;#34;]): #do something pass （3）
pbar = tqdm([&amp;#34;a&amp;#34;, &amp;#34;b&amp;#34;, &amp;#34;c&amp;#34;, &amp;#34;d&amp;#34;]) for char in pbar: sleep(1) pbar.set_description(&amp;#34;Processing %s&amp;#34; % char) 1.2. 放一个迭代器在里面 count = 0 for batch_idx, item in tqdm(enumerate(train_loader)): count+=batch_idx print(count) 这种方式是我使用的方式。简单来说就是把迭代器train_loader放在tqdm里面，这里增加了一个enumerate去增加一个序列。</description>
    </item>
    
    <item>
      <title>15——torch.tensor叶子节点</title>
      <link>https://ioyy900205.github.io/post/2021-05-19-15torch.tensor-%E5%8F%B6%E5%AD%90%E8%8A%82%E7%82%B9/</link>
      <pubDate>Wed, 19 May 2021 09:53:52 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-19-15torch.tensor-%E5%8F%B6%E5%AD%90%E8%8A%82%E7%82%B9/</guid>
      <description>讨论叶子节点
  1. 理顺逻辑  1.1. 元素 1.2. 计算    1. 理顺逻辑 1.1. 元素 在pytorch的计算图中，只有两种元素：
数据（tensor）
  叶子节点(leaf node)
  叶子节点可以理解成不依赖其他tensor的tensor。
  在pytorch中，神经网络层中的权值w的tensor均为叶子节点。
  自己定义的tensor例如a=torch.tensor([1.0])定义的节点是叶子节点。
  All Tensors that have requires_grad which is False will be leaf Tensors by convention.
  For Tensors that have requires_grad which is True, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so grad_fn is None.</description>
    </item>
    
    <item>
      <title>13————tensor内存占用计算实例</title>
      <link>https://ioyy900205.github.io/post/2021-05-17-13tensor%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8%E8%AE%A1%E7%AE%97%E5%AE%9E%E4%BE%8B/</link>
      <pubDate>Mon, 17 May 2021 19:01:17 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-17-13tensor%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8%E8%AE%A1%E7%AE%97%E5%AE%9E%E4%BE%8B/</guid>
      <description>网上查看了CSDN的代码，写的有误。所以这里写一个正确版本的
  ## 1. 各类型所占用的字节数 2. tensor内存计算 3. 清空内存  1. 各类型所占用的字节数  测试代码
import numpy as np import sys # 32位整型 ai32 = np.array([], dtype=np.int32) bi32 = np.arange(1, dtype=np.int32) ci32 = np.arange(5, dtype=np.int32) # 64位整型 ai64 = np.array([], dtype=np.int64) bi64 = np.arange(1, dtype=np.int64) ci64 = np.arange(5, dtype=np.int64) # 32位浮点数 af32 = np.array([], dtype=np.float32) bf32 = np.arange(1, dtype=np.float32) cf32 = np.arange(5, dtype=np.float32) # 64位浮点数 af64 = np.array([], dtype=np.float64) bf64 = np.</description>
    </item>
    
    <item>
      <title>08——数据集操作和图像处理——PyTorch</title>
      <link>https://ioyy900205.github.io/post/2021-05-11-voc2012%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%9B%BE%E7%89%87%E5%92%8Cbbox%E4%BF%A1%E6%81%AF%E6%98%BE%E7%A4%BA/</link>
      <pubDate>Tue, 11 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-11-voc2012%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%9B%BE%E7%89%87%E5%92%8Cbbox%E4%BF%A1%E6%81%AF%E6%98%BE%E7%A4%BA/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. 数据集读取 2. 转换函数 3. bbox画图  1. 数据集读取 from torch.utils.data import Dataset import os import torch import json from PIL import Image from lxml import etree class VOC2012DataSet(Dataset): &amp;#34;&amp;#34;&amp;#34;读取解析PASCAL VOC2012数据集&amp;#34;&amp;#34;&amp;#34; def __init__(self, voc_root, transforms, txt_name: str = &amp;#34;train.txt&amp;#34;): self.root = os.path.join(voc_root, &amp;#34;VOCdevkit&amp;#34;, &amp;#34;VOC2012&amp;#34;) self.img_root = os.path.join(self.root, &amp;#34;JPEGImages&amp;#34;) self.annotations_root = os.path.join(self.root, &amp;#34;Annotations&amp;#34;) # read train.txt or val.txt file txt_path = os.path.join(self.root, &amp;#34;ImageSets&amp;#34;, &amp;#34;Main&amp;#34;, txt_name) assert os.</description>
    </item>
    
    <item>
      <title>06——torch.cat维度操作——PyTorch</title>
      <link>https://ioyy900205.github.io/post/2021-05-10-torch.cat%E7%9A%84%E7%BB%B4%E5%BA%A6%E6%93%8D%E4%BD%9C/</link>
      <pubDate>Mon, 10 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-10-torch.cat%E7%9A%84%E7%BB%B4%E5%BA%A6%E6%93%8D%E4%BD%9C/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. torch.cat  1.1. 二维数组  1.1.1. dim=0 1.1.2. dim=1   1.2. 三维数组  1.2.1. dim=0 1.2.2. dim=1 1.2.3. dim=2     2. 小结  1. torch.cat 1.1. 二维数组 1.1.1. dim=0 运行
import torch A = torch.ones(2,3) #2x3的张量（矩阵）  print(&amp;#34;A:&amp;#34;,A) B=2*torch.ones(4,3) #4x3的张量（矩阵）  print(&amp;#34;B:&amp;#34;,B) C=torch.cat((A,B),0)#按维数0（行）拼接 print(&amp;#34;C:&amp;#34;,C) print(C.size()) 结果
A: tensor([[1., 1., 1.], [1., 1., 1.]]) B: tensor([[2., 2., 2.], [2., 2., 2.], [2.</description>
    </item>
    
    <item>
      <title>07——torch.stack维度操作——PyTorch</title>
      <link>https://ioyy900205.github.io/post/2021-05-10-torch.stack%E7%9A%84%E7%BB%B4%E5%BA%A6%E6%93%8D%E4%BD%9C/</link>
      <pubDate>Mon, 10 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-10-torch.stack%E7%9A%84%E7%BB%B4%E5%BA%A6%E6%93%8D%E4%BD%9C/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. 初始化 2. torch.stak  2.1. dim = 0 2.2. dim = 1 2.3. dim = 2    1. 初始化 input
import torch import numpy as np # 创建3*3的矩阵，a、b a=np.array([[1,2,3],[4,5,6],[7,8,9]]) b=np.array([[10,20,30],[40,50,60],[70,80,90]]) # 将矩阵转化为Tensor a = torch.from_numpy(a) b = torch.from_numpy(b) # 打印a、b、c print(a,a.size()) print(b,b.size()) output
tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) torch.Size([3, 3]) tensor([[10, 20, 30], [40, 50, 60], [70, 80, 90]]) torch.</description>
    </item>
    
    <item>
      <title>03——nn.module学习——PyTorch</title>
      <link>https://ioyy900205.github.io/post/2021-04-27-nn.module/</link>
      <pubDate>Tue, 27 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-04-27-nn.module/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. nn.moduel初步  1.1. Linear类 1.2. Conv2d类 1.3. 自定义层的步骤   2. 自定义层  2.1. 定义一个自定义层MyLayer 2.2. 自定义模型并且训练    1. nn.moduel初步 keras更加注重的是层Layer、pytorch更加注重的是模型Module。
1.1. Linear类 import math import torch from torch.nn.parameter import Parameter from .. import functional as F from .. import init from .module import Module from ..._jit_internal import weak_module, weak_script_method class Linear(Module): __constants__ = [&amp;#39;bias&amp;#39;] def __init__(self, in_features, out_features, bias=True): super(Linear, self).__init__() self.</description>
    </item>
    
  </channel>
</rss>
