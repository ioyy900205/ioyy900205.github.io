<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>亮的笔记</title>
    <link>https://ioyy900205.github.io/</link>
    <description>Recent content on 亮的笔记</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 27 Apr 2021 09:43:30 +0800</lastBuildDate><atom:link href="https://ioyy900205.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>03——nn.module学习</title>
      <link>https://ioyy900205.github.io/post/nn.module/</link>
      <pubDate>Tue, 27 Apr 2021 09:43:30 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/nn.module/</guid>
      <description>1. nn.moduel初步  1.1. Linear类 1.2. Conv2d类 1.3. 自定义层的步骤   2. 自定义层  2.1. 定义一个自定义层MyLayer 2.2. 自定义模型并且训练    1. nn.moduel初步 keras更加注重的是层Layer、pytorch更加注重的是模型Module。
1.1. Linear类 import math import torch from torch.nn.parameter import Parameter from .. import functional as F from .. import init from .module import Module from ..._jit_internal import weak_module, weak_script_method class Linear(Module): __constants__ = [&amp;#39;bias&amp;#39;] def __init__(self, in_features, out_features, bias=True): super(Linear, self).__init__() self.in_features = in_features self.out_features = out_features self.</description>
    </item>
    
    <item>
      <title>01——ResNeXt学习</title>
      <link>https://ioyy900205.github.io/post/resnext/</link>
      <pubDate>Mon, 26 Apr 2021 17:05:30 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/resnext/</guid>
      <description>1. ResNeXt  1.1. 概述 1.2. 思路 1.3. Block 1.4. 组卷积 1.5. 模型复杂度 1.6. Shortcut 1.7. 结果    1. ResNeXt 1.1. 概述 论文：Aggregated Residual Transformations for Deep Neural Networks
论文链接：https://arxiv.org/abs/1611.05431
PyTorch代码：https://github.com/miraclewkf/ResNeXt-PyTorch 2016年,ISCLVCR 2016 no.2
1.2. 思路 Split-Transform-Merge （来源于inception）
堆叠（来源于VGG）
1.3. Block cardinality ，原文的解释是the size of the set of transformations，如上图右边是 cardinality=32 的样子，这里注意每个被聚合的拓扑结构都是一样的(这也是和 Inception 的差别，减轻设计负担)
1.4. 组卷积 最早可以追溯到AlexNet。
32x4d 中 32为组卷积数目，4d为每组卷积4个卷积核。
组卷积可以有不同的配置，但是不同的配置需要通过实验判断效果。
可以发现通过组卷积能够有效降低parameter的大小。
1.5. 模型复杂度 如果想增加模型复杂度，几个选择：
1）宽度；
2）深度；
3）cardinality。</description>
    </item>
    
    <item>
      <title>02——SeNet学习</title>
      <link>https://ioyy900205.github.io/post/senet/</link>
      <pubDate>Mon, 26 Apr 2021 17:05:30 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/senet/</guid>
      <description>1. SENET  1.1. 链接 1.2. 贡献  1.2.1. 提供了子结构 1.2.2. SOTA   1.3. 核心思想 1.4. Squeeze 1.5. Excitation   2. 思考  2.1. 浅层作用较大    1. SENET 1.1. 链接 论文：Squeeze-and-Excitation Networks论文链接：https://arxiv.org/abs/1709.01507代码地址：https://github.com/hujie-frank/SENetPyTorch代码地址：https://github.com/miraclewkf/SENet-PyTorch
A central theme of computer vision research is the search for more powerful representations that capture only those properties of an image that are most salient for a given task
1.2. 贡献 1.2.1. 提供了子结构 Sequeeze-and-Excitation(SE) block并不是一个完整的网络结构，而是一个子结构，可以嵌到其他分类或检测模型中。</description>
    </item>
    
    <item>
      <title>04——轻量级神经网络总结</title>
      <link>https://ioyy900205.github.io/post/%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/</guid>
      <description>需解决的问题 存储问题
数百层网络有着大量的权值参数，保存大量权值参数对设备的内存要求很高； 速度问题
在实际应用中，往往是毫秒级别，为了达到实际应用标准，要么提高处理器性能，要么就减少计算量。 而提高处理器性能在短时间内是无法完成的，因此减少计算量成为了主要的技术手段。 方法 模型结构设计
主要思想在于设计更高效的「网络计算方式」（主要针对卷积方式），从而使网络参数减少的同时，不损失网络性能。 模型压缩
即在已经训练好的模型上进行压缩，使得网络携带更少的网络参数，从而解决内存问题，同时可以解决速度问题。 </description>
    </item>
    
  </channel>
</rss>
