<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>亮的笔记</title>
    <link>https://ioyy900205.github.io/</link>
    <description>Recent content on 亮的笔记</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 03 Jun 2021 14:24:36 +0800</lastBuildDate><atom:link href="https://ioyy900205.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>22——VSCode中plt.show()无法显示图片问题</title>
      <link>https://ioyy900205.github.io/post/2021-06-03-22vscode%E4%B8%ADplt.show%E6%97%A0%E6%B3%95%E6%98%BE%E7%A4%BA%E5%9B%BE%E7%89%87%E9%97%AE%E9%A2%98/</link>
      <pubDate>Thu, 03 Jun 2021 14:24:36 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-03-22vscode%E4%B8%ADplt.show%E6%97%A0%E6%B3%95%E6%98%BE%E7%A4%BA%E5%9B%BE%E7%89%87%E9%97%AE%E9%A2%98/</guid>
      <description>远程SSh时候，VSCode无法显示图片
  1. 解决方法  1. 解决方法 终端中输入：
export DISPLAY=:10.0  参考资料 https://www.pythonheidong.com/blog/article/714470/11e6dbde8921d93ae464/</description>
    </item>
    
    <item>
      <title>timm库的使用</title>
      <link>https://ioyy900205.github.io/post/2021-06-02-timm%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Wed, 02 Jun 2021 11:02:33 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-02-timm%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/</guid>
      <description>在研究transformer时，发现timm库可以直接使用预训练的模型。
  1. timm简介 2. 已有的预训练模型 3. 验证模式查看预训练结果 4. 使用ViT模型 5. 使用tf_efficientnet_l2_ns模型  1. timm简介 PyTorchImageModels，简称timm，是一个巨大的PyTorch代码集合
旨在将各种SOTA模型整合在一起，并具有复现ImageNet训练结果的能力。
2. 已有的预训练模型 &#39;adv_inception_v3&#39;,&#39;cait_m36_384&#39;,&#39;cait_m48_448&#39;,&#39;cait_s24_224&#39;,&#39;cait_s24_384&#39;,&#39;cait_s36_384&#39;,&#39;cait_xs24_384&#39;,&#39;cait_xxs24_224&#39;,&#39;cait_xxs24_384&#39;,&#39;cait_xxs36_224&#39;,&#39;cait_xxs36_384&#39;,&#39;coat_lite_mini&#39;,&#39;coat_lite_tiny&#39;,&#39;cspdarknet53&#39;,&#39;cspresnet50&#39;,&#39;cspresnext50&#39;,&#39;densenet121&#39;,&#39;densenet161&#39;,&#39;densenet169&#39;,&#39;densenet201&#39;,&#39;densenetblur121d&#39;,&#39;dla34&#39;,&#39;dla46_c&#39;,&#39;dla46x_c&#39;,&#39;dla60&#39;,&#39;dla60_res2net&#39;,&#39;dla60_res2next&#39;,&#39;dla60x&#39;,&#39;dla60x_c&#39;,&#39;dla102&#39;,&#39;dla102x&#39;,&#39;dla102x2&#39;,&#39;dla169&#39;,&#39;dm_nfnet_f0&#39;,&#39;dm_nfnet_f1&#39;,&#39;dm_nfnet_f2&#39;,&#39;dm_nfnet_f3&#39;,&#39;dm_nfnet_f4&#39;,&#39;dm_nfnet_f5&#39;,&#39;dm_nfnet_f6&#39;,&#39;dpn68&#39;,&#39;dpn68b&#39;,&#39;dpn92&#39;,&#39;dpn98&#39;,&#39;dpn107&#39;,&#39;dpn131&#39;,&#39;eca_nfnet_l0&#39;,&#39;eca_nfnet_l1&#39;,&#39;ecaresnet26t&#39;,&#39;ecaresnet50d&#39;,&#39;ecaresnet50d_pruned&#39;,&#39;ecaresnet50t&#39;,&#39;ecaresnet101d&#39;,&#39;ecaresnet101d_pruned&#39;,&#39;ecaresnet269d&#39;,&#39;ecaresnetlight&#39;,&#39;efficientnet_b0&#39;,&#39;efficientnet_b1&#39;,&#39;efficientnet_b1_pruned&#39;,&#39;efficientnet_b2&#39;,&#39;efficientnet_b2_pruned&#39;,&#39;efficientnet_b3&#39;,&#39;efficientnet_b3_pruned&#39;,&#39;efficientnet_b4&#39;,&#39;efficientnet_el&#39;,&#39;efficientnet_el_pruned&#39;,&#39;efficientnet_em&#39;,&#39;efficientnet_es&#39;,&#39;efficientnet_es_pruned&#39;,&#39;efficientnet_lite0&#39;,&#39;efficientnetv2_rw_s&#39;,&#39;ens_adv_inception_resnet_v2&#39;,&#39;ese_vovnet19b_dw&#39;,&#39;ese_vovnet39b&#39;,&#39;fbnetc_100&#39;,&#39;gernet_l&#39;,&#39;gernet_m&#39;,&#39;gernet_s&#39;,&#39;ghostnet_100&#39;,&#39;gluon_inception_v3&#39;,&#39;gluon_resnet18_v1b&#39;,&#39;gluon_resnet34_v1b&#39;,&#39;gluon_resnet50_v1b&#39;,&#39;gluon_resnet50_v1c&#39;,&#39;gluon_resnet50_v1d&#39;,&#39;gluon_resnet50_v1s&#39;,&#39;gluon_resnet101_v1b&#39;,&#39;gluon_resnet101_v1c&#39;,&#39;gluon_resnet101_v1d&#39;,&#39;gluon_resnet101_v1s&#39;,&#39;gluon_resnet152_v1b&#39;,&#39;gluon_resnet152_v1c&#39;,&#39;gluon_resnet152_v1d&#39;,&#39;gluon_resnet152_v1s&#39;,&#39;gluon_resnext50_32x4d&#39;,&#39;gluon_resnext101_32x4d&#39;,&#39;gluon_resnext101_64x4d&#39;,&#39;gluon_senet154&#39;,&#39;gluon_seresnext50_32x4d&#39;,&#39;gluon_seresnext101_32x4d&#39;,&#39;gluon_seresnext101_64x4d&#39;,&#39;gluon_xception65&#39;,&#39;hardcorenas_a&#39;,&#39;hardcorenas_b&#39;,&#39;hardcorenas_c&#39;,&#39;hardcorenas_d&#39;,&#39;hardcorenas_e&#39;,&#39;hardcorenas_f&#39;,&#39;hrnet_w18&#39;,&#39;hrnet_w18_small&#39;,&#39;hrnet_w18_small_v2&#39;,&#39;hrnet_w30&#39;,&#39;hrnet_w32&#39;,&#39;hrnet_w40&#39;,&#39;hrnet_w44&#39;,&#39;hrnet_w48&#39;,&#39;hrnet_w64&#39;,&#39;ig_resnext101_32x8d&#39;,&#39;ig_resnext101_32x16d&#39;,&#39;ig_resnext101_32x32d&#39;,&#39;ig_resnext101_32x48d&#39;,&#39;inception_resnet_v2&#39;,&#39;inception_v3&#39;,&#39;inception_v4&#39;,&#39;legacy_senet154&#39;,&#39;legacy_seresnet18&#39;,&#39;legacy_seresnet34&#39;,&#39;legacy_seresnet50&#39;,&#39;legacy_seresnet101&#39;,&#39;legacy_seresnet152&#39;,&#39;legacy_seresnext26_32x4d&#39;,&#39;legacy_seresnext50_32x4d&#39;,&#39;legacy_seresnext101_32x4d&#39;,&#39;mixer_b16_224&#39;,&#39;mixer_b16_224_in21k&#39;,&#39;mixer_l16_224&#39;,&#39;mixer_l16_224_in21k&#39;,&#39;mixnet_l&#39;,&#39;mixnet_m&#39;,&#39;mixnet_s&#39;,&#39;mixnet_xl&#39;,&#39;mnasnet_100&#39;,&#39;mobilenetv2_100&#39;,&#39;mobilenetv2_110d&#39;,&#39;mobilenetv2_120d&#39;,&#39;mobilenetv2_140&#39;,&#39;mobilenetv3_large_100&#39;,&#39;mobilenetv3_large_100_miil&#39;,&#39;mobilenetv3_large_100_miil_in21k&#39;,&#39;mobilenetv3_rw&#39;,&#39;nasnetalarge&#39;,&#39;nf_regnet_b1&#39;,&#39;nf_resnet50&#39;,&#39;nfnet_l0&#39;,&#39;pit_b_224&#39;,&#39;pit_b_distilled_224&#39;,&#39;pit_s_224&#39;,&#39;pit_s_distilled_224&#39;,&#39;pit_ti_224&#39;,&#39;pit_ti_distilled_224&#39;,&#39;pit_xs_224&#39;,&#39;pit_xs_distilled_224&#39;,&#39;pnasnet5large&#39;,&#39;regnetx_002&#39;,&#39;regnetx_004&#39;,&#39;regnetx_006&#39;,&#39;regnetx_008&#39;,&#39;regnetx_016&#39;,&#39;regnetx_032&#39;,&#39;regnetx_040&#39;,&#39;regnetx_064&#39;,&#39;regnetx_080&#39;,&#39;regnetx_120&#39;,&#39;regnetx_160&#39;,&#39;regnetx_320&#39;,&#39;regnety_002&#39;,&#39;regnety_004&#39;,&#39;regnety_006&#39;,&#39;regnety_008&#39;,&#39;regnety_016&#39;,&#39;regnety_032&#39;,&#39;regnety_040&#39;,&#39;regnety_064&#39;,&#39;regnety_080&#39;,&#39;regnety_120&#39;,&#39;regnety_160&#39;,&#39;regnety_320&#39;,&#39;repvgg_a2&#39;,&#39;repvgg_b0&#39;,&#39;repvgg_b1&#39;,&#39;repvgg_b1g4&#39;,&#39;repvgg_b2&#39;,&#39;repvgg_b2g4&#39;,&#39;repvgg_b3&#39;,&#39;repvgg_b3g4&#39;,&#39;res2net50_14w_8s&#39;,&#39;res2net50_26w_4s&#39;,&#39;res2net50_26w_6s&#39;,&#39;res2net50_26w_8s&#39;,&#39;res2net50_48w_2s&#39;,&#39;res2net101_26w_4s&#39;,&#39;res2next50&#39;,&#39;resnest14d&#39;,&#39;resnest26d&#39;,&#39;resnest50d&#39;,&#39;resnest50d_1s4x24d&#39;,&#39;resnest50d_4s2x40d&#39;,&#39;resnest101e&#39;,&#39;resnest200e&#39;,&#39;resnest269e&#39;,&#39;resnet18&#39;,&#39;resnet18d&#39;,&#39;resnet26&#39;,&#39;resnet26d&#39;,&#39;resnet34&#39;,&#39;resnet34d&#39;,&#39;resnet50&#39;,&#39;resnet50d&#39;,&#39;resnet101d&#39;,&#39;resnet152d&#39;,&#39;resnet200d&#39;,&#39;resnetblur50&#39;,&#39;resnetrs50&#39;,&#39;resnetrs101&#39;,&#39;resnetrs152&#39;,&#39;resnetrs200&#39;,&#39;resnetrs270&#39;,&#39;resnetrs350&#39;,&#39;resnetrs420&#39;,&#39;resnetv2_50x1_bitm&#39;,&#39;resnetv2_50x1_bitm_in21k&#39;,&#39;resnetv2_50x3_bitm&#39;,&#39;resnetv2_50x3_bitm_in21k&#39;,&#39;resnetv2_101x1_bitm&#39;,&#39;resnetv2_101x1_bitm_in21k&#39;,&#39;resnetv2_101x3_bitm&#39;,&#39;resnetv2_101x3_bitm_in21k&#39;,&#39;resnetv2_152x2_bitm&#39;,&#39;resnetv2_152x2_bitm_in21k&#39;,&#39;resnetv2_152x4_bitm&#39;,&#39;resnetv2_152x4_bitm_in21k&#39;,&#39;resnext50_32x4d&#39;,&#39;resnext50d_32x4d&#39;,&#39;resnext101_32x8d&#39;,&#39;rexnet_100&#39;,&#39;rexnet_130&#39;,&#39;rexnet_150&#39;,&#39;rexnet_200&#39;,&#39;selecsls42b&#39;,&#39;selecsls60&#39;,&#39;selecsls60b&#39;,&#39;semnasnet_100&#39;,&#39;seresnet50&#39;,&#39;seresnet152d&#39;,&#39;seresnext26d_32x4d&#39;,&#39;seresnext26t_32x4d&#39;,&#39;seresnext50_32x4d&#39;,&#39;skresnet18&#39;,&#39;skresnet34&#39;,&#39;skresnext50_32x4d&#39;,&#39;spnasnet_100&#39;,&#39;ssl_resnet18&#39;,&#39;ssl_resnet50&#39;,&#39;ssl_resnext50_32x4d&#39;,&#39;ssl_resnext101_32x4d&#39;,&#39;ssl_resnext101_32x8d&#39;,&#39;ssl_resnext101_32x16d&#39;,&#39;swin_base_patch4_window7_224&#39;,&#39;swin_base_patch4_window7_224_in22k&#39;,&#39;swin_base_patch4_window12_384&#39;,&#39;swin_base_patch4_window12_384_in22k&#39;,&#39;swin_large_patch4_window7_224&#39;,&#39;swin_large_patch4_window7_224_in22k&#39;,&#39;swin_large_patch4_window12_384&#39;,&#39;swin_large_patch4_window12_384_in22k&#39;,&#39;swin_small_patch4_window7_224&#39;,&#39;swin_tiny_patch4_window7_224&#39;,&#39;swsl_resnet18&#39;,&#39;swsl_resnet50&#39;,&#39;swsl_resnext50_32x4d&#39;,&#39;swsl_resnext101_32x4d&#39;,&#39;swsl_resnext101_32x8d&#39;,&#39;swsl_resnext101_32x16d&#39;,&#39;tf_efficientnet_b0&#39;,&#39;tf_efficientnet_b0_ap&#39;,&#39;tf_efficientnet_b0_ns&#39;,&#39;tf_efficientnet_b1&#39;,&#39;tf_efficientnet_b1_ap&#39;,&#39;tf_efficientnet_b1_ns&#39;,&#39;tf_efficientnet_b2&#39;,&#39;tf_efficientnet_b2_ap&#39;,&#39;tf_efficientnet_b2_ns&#39;,&#39;tf_efficientnet_b3&#39;,&#39;tf_efficientnet_b3_ap&#39;,&#39;tf_efficientnet_b3_ns&#39;,&#39;tf_efficientnet_b4&#39;,&#39;tf_efficientnet_b4_ap&#39;,&#39;tf_efficientnet_b4_ns&#39;,&#39;tf_efficientnet_b5&#39;,&#39;tf_efficientnet_b5_ap&#39;,&#39;tf_efficientnet_b5_ns&#39;,&#39;tf_efficientnet_b6&#39;,&#39;tf_efficientnet_b6_ap&#39;,&#39;tf_efficientnet_b6_ns&#39;,&#39;tf_efficientnet_b7&#39;,&#39;tf_efficientnet_b7_ap&#39;,&#39;tf_efficientnet_b7_ns&#39;,&#39;tf_efficientnet_b8&#39;,&#39;tf_efficientnet_b8_ap&#39;,&#39;tf_efficientnet_cc_b0_4e&#39;,&#39;tf_efficientnet_cc_b0_8e&#39;,&#39;tf_efficientnet_cc_b1_8e&#39;,&#39;tf_efficientnet_el&#39;,&#39;tf_efficientnet_em&#39;,&#39;tf_efficientnet_es&#39;,&#39;tf_efficientnet_l2_ns&#39;,&#39;tf_efficientnet_l2_ns_475&#39;,&#39;tf_efficientnet_lite0&#39;,&#39;tf_efficientnet_lite1&#39;,&#39;tf_efficientnet_lite2&#39;,&#39;tf_efficientnet_lite3&#39;,&#39;tf_efficientnet_lite4&#39;,&#39;tf_efficientnetv2_b0&#39;,&#39;tf_efficientnetv2_b1&#39;,&#39;tf_efficientnetv2_b2&#39;,&#39;tf_efficientnetv2_b3&#39;,&#39;tf_efficientnetv2_l&#39;,&#39;tf_efficientnetv2_l_in21ft1k&#39;,&#39;tf_efficientnetv2_l_in21k&#39;,&#39;tf_efficientnetv2_m&#39;,&#39;tf_efficientnetv2_m_in21ft1k&#39;,&#39;tf_efficientnetv2_m_in21k&#39;,&#39;tf_efficientnetv2_s&#39;,&#39;tf_efficientnetv2_s_in21ft1k&#39;,&#39;tf_efficientnetv2_s_in21k&#39;,&#39;tf_inception_v3&#39;,&#39;tf_mixnet_l&#39;,&#39;tf_mixnet_m&#39;,&#39;tf_mixnet_s&#39;,&#39;tf_mobilenetv3_large_075&#39;,&#39;tf_mobilenetv3_large_100&#39;,&#39;tf_mobilenetv3_large_minimal_100&#39;,&#39;tf_mobilenetv3_small_075&#39;,&#39;tf_mobilenetv3_small_100&#39;,&#39;tf_mobilenetv3_small_minimal_100&#39;,&#39;tnt_s_patch16_224&#39;,&#39;tresnet_l&#39;,&#39;tresnet_l_448&#39;,&#39;tresnet_m&#39;,&#39;tresnet_m_448&#39;,&#39;tresnet_m_miil_in21k&#39;,&#39;tresnet_xl&#39;,&#39;tresnet_xl_448&#39;,&#39;tv_densenet121&#39;,&#39;tv_resnet34&#39;,&#39;tv_resnet50&#39;,&#39;tv_resnet101&#39;,&#39;tv_resnet152&#39;,&#39;tv_resnext50_32x4d&#39;,&#39;vgg11&#39;,&#39;vgg11_bn&#39;,&#39;vgg13&#39;,&#39;vgg13_bn&#39;,&#39;vgg16&#39;,&#39;vgg16_bn&#39;,&#39;vgg19&#39;,&#39;vgg19_bn&#39;,&#39;vit_base_patch16_224&#39;,&#39;vit_base_patch16_224_in21k&#39;,&#39;vit_base_patch16_224_miil&#39;,&#39;vit_base_patch16_224_miil_in21k&#39;,&#39;vit_base_patch16_384&#39;,&#39;vit_base_patch32_224_in21k&#39;,&#39;vit_base_patch32_384&#39;,&#39;vit_base_r50_s16_224_in21k&#39;,&#39;vit_base_r50_s16_384&#39;,&#39;vit_deit_base_distilled_patch16_224&#39;,&#39;vit_deit_base_distilled_patch16_384&#39;,&#39;vit_deit_base_patch16_224&#39;,&#39;vit_deit_base_patch16_384&#39;,&#39;vit_deit_small_distilled_patch16_224&#39;,&#39;vit_deit_small_patch16_224&#39;,&#39;vit_deit_tiny_distilled_patch16_224&#39;,&#39;vit_deit_tiny_patch16_224&#39;,&#39;vit_large_patch16_224&#39;,&#39;vit_large_patch16_224_in21k&#39;,&#39;vit_large_patch16_384&#39;,&#39;vit_large_patch32_224_in21k&#39;,&#39;vit_large_patch32_384&#39;,&#39;vit_small_patch16_224&#39;,&#39;wide_resnet50_2&#39;,&#39;wide_resnet101_2&#39;,&#39;xception&#39;,&#39;xception41&#39;,&#39;xception65&#39;,&#39;xception71&#39; 3.</description>
    </item>
    
    <item>
      <title>【现场分享】智源大会类脑视觉</title>
      <link>https://ioyy900205.github.io/post/2021-05-31-%E7%8E%B0%E5%9C%BA%E5%88%86%E4%BA%AB%E6%99%BA%E6%BA%90%E5%A4%A7%E4%BC%9A%E7%B1%BB%E8%84%91%E8%A7%86%E8%A7%89/</link>
      <pubDate>Mon, 31 May 2021 10:41:41 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-31-%E7%8E%B0%E5%9C%BA%E5%88%86%E4%BA%AB%E6%99%BA%E6%BA%90%E5%A4%A7%E4%BC%9A%E7%B1%BB%E8%84%91%E8%A7%86%E8%A7%89/</guid>
      <description>【现场分享】智源大会类脑视觉
 @TOC
标题 黄铁军
唐华锦
张兆翔
王威
类脑 拓展马尔视觉计算原理
研究内容 计算 成像 应用
 参考资料</description>
    </item>
    
    <item>
      <title>21——MSDNet论文的推理模块</title>
      <link>https://ioyy900205.github.io/post/2021-05-22-21msdnet%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Sat, 22 May 2021 10:21:24 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-22-21msdnet%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</guid>
      <description>本文的核心主旨在于：在计算资源限制下对不同的图像进行不同的处理，可以理解成对于简单样本采用简单的方式处理，对于复杂样本则尽可能给其分配资源，以避免不必要的资源浪费节省计算量，并且在这种推理的思想上要实现网络对数据的自动适应。所以本文设计了一个新颖的二维多尺度网络结构，根据不同的资源需求训练了多个分类器，为了最大程度地重用分类器之间的计算，我们将它们作为早期出口并入单个深度卷积神经网络，并通过密集连接将它们互连，该构架在整个网络中同时保持粗略和精细的scale，获得了良好的效果。
  1. model 2. 推理部分  2.1. 实时推理方法 2.2. 出口分配   3. 讨论  1. model 这里有两个基本模块，一个是第一层的横向传播模块，一个是下采样加横向传播模块。
2. 推理部分 2.1. 实时推理方法 其实就是将每个出口的结果打印出来。
只不过在推理过程中，考虑了载入数据的时间（大约0.45s），整个batch的推理时间大约是0.6-0.7s&amp;rsquo;s之间。
程序在最后的地方对每个出口进行了输出
 prec@1 56.632 prec@5 79.942 prec@1 65.136 prec@5 86.252 prec@1 68.420 prec@5 88.632 prec@1 69.770 prec@5 89.418 prec@1 71.336 prec@5 90.364   之前有统计过计算量的地方，所以这里结合计算量就可以得到论文的结果。
2.2. 出口分配 这种方式对5个出口进行了样本数量设定（这里设置了40组）。进而可以学习到每个出口的threshold，从而实现了不同出口的退出机制。
3. 讨论 每个出口样本数量设定 是一个超参数。MSDNet所得到的结果目前来看不一定是最好的。
以下有收获：
 多尺度架构的编程方法。 网络模型参数和计算量的计算。 退出机制的学习方法。   参考资料
手动debug</description>
    </item>
    
    <item>
      <title>20——torch_view操作指南</title>
      <link>https://ioyy900205.github.io/post/2021-05-21-20torch_view%E6%93%8D%E4%BD%9C%E6%8C%87%E5%8D%97/</link>
      <pubDate>Fri, 21 May 2021 18:13:22 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-21-20torch_view%E6%93%8D%E4%BD%9C%E6%8C%87%E5%8D%97/</guid>
      <description>还是发现对view的操作不够深入，这里做了一个小demo。自行体会一下
  1. 代码 2. 结果 3. 理解  1. 代码 &amp;#39;&amp;#39;&amp;#39; Date: 2021-05-21 15:43:43 LastEditors: Liuliang LastEditTime: 2021-05-21 15:50:36 Description: view &amp;#39;&amp;#39;&amp;#39; import torch a = torch.arange(0,12,1) print(a) b = a.view(2,-1) print(b) c = b.view(6,2) print(c) 2. 结果 tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) tensor([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) tensor([[ 0, 1], [ 2, 3], [ 4, 5], [ 6, 7], [ 8, 9], [10, 11]]) 3.</description>
    </item>
    
    <item>
      <title>19——avg_pool2d</title>
      <link>https://ioyy900205.github.io/post/2021-05-21-19avg_pool2d/</link>
      <pubDate>Fri, 21 May 2021 15:06:07 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-21-19avg_pool2d/</guid>
      <description>avg——pool2d理解
 @TOC
1. 直接上代码 import torch from torch.nn import functional as F # 1.初始化 input = torch.tensor( [ [1,1,1,1,1], [1,1,1,1,1], [0,0,0,1,1], [1,1,1,1,1] ]).unsqueeze(0).float() print(input.size())#torch.Size([1, 4, 5]) print(input) #2. avg_pool1d # m1 = F.avg_pool1d(input,kernel_size=2) # print(m1)#tensor([[[1.0000, 1.0000], # # [1.0000, 1.0000], # # [0.0000, 0.5000], # # [1.0000, 1.0000]]]) #3. avg_pool2d # m = F.avg_pool2d(input,kernel_size=2)#这里是在原矩阵中找出2*2的区域求平均，不够的舍弃，stride=(2,2) # print(m)#tensor([[[1.0000, 1.0000], # # [0.5000, 0.7500]]]) m3= F.avg_pool2d(input,kernel_size=2,stride=1)#这里是在原矩阵中找出2*2的区域求平均，不够的舍弃，stride=(1,1),[1,1,4,5]--&amp;gt;[1,1,3,4] print(m3)#tensor([[[1.0000, 1.0000, 1.0000, 1.0000], # [0.5000, 0.</description>
    </item>
    
    <item>
      <title>18——bottleneck结构解析和make_layers解析</title>
      <link>https://ioyy900205.github.io/post/2021-05-21-18bottleneck%E7%BB%93%E6%9E%84%E8%A7%A3%E6%9E%90%E5%92%8Cmake_layers%E8%A7%A3%E6%9E%90/</link>
      <pubDate>Fri, 21 May 2021 11:37:47 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-21-18bottleneck%E7%BB%93%E6%9E%84%E8%A7%A3%E6%9E%90%E5%92%8Cmake_layers%E8%A7%A3%E6%9E%90/</guid>
      <description>bottleneck
make_layers
  1. bottleneck 2. make_layers 3. DenseNet模型  1. bottleneck class Bottleneck(nn.Module): def __init__(self, in_planes, growth_rate): super(Bottleneck, self).__init__() self.bn1 = nn.BatchNorm2d(in_planes) self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False) self.bn2 = nn.BatchNorm2d(4*growth_rate) self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False) def forward(self, x): out = self.conv1(F.relu(self.bn1(x))) out = self.conv2(F.relu(self.bn2(out))) out = torch.cat([out,x], 1) return out 这个结构叫做瓶颈结构,如果in_planes=12，growth_rate=12，输出如下：
(0): Bottleneck( (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv1): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.</description>
    </item>
    
    <item>
      <title>17——tqdm载入数据集测试</title>
      <link>https://ioyy900205.github.io/post/2021-05-20-17tqdm%E8%BD%BD%E5%85%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E6%B5%8B%E8%AF%95/</link>
      <pubDate>Thu, 20 May 2021 16:50:49 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-20-17tqdm%E8%BD%BD%E5%85%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E6%B5%8B%E8%AF%95/</guid>
      <description>代码已放在mess_around文件夹下
  1. 基本使用方式  1.1. list方式 1.2. 放一个迭代器在里面   2. 实例  2.1. 使用tqdm带来的效率损失  2.1.1. MNIST 2.1.2. ImageNet   2.2. num_workers的使用    1. 基本使用方式 1.1. list方式 （1）
for i in tqdm(range(10000)): #do something sleep(0.01) pass （2）
for char in tqdm([&amp;#34;a&amp;#34;, &amp;#34;b&amp;#34;, &amp;#34;c&amp;#34;, &amp;#34;d&amp;#34;]): #do something pass （3）
pbar = tqdm([&amp;#34;a&amp;#34;, &amp;#34;b&amp;#34;, &amp;#34;c&amp;#34;, &amp;#34;d&amp;#34;]) for char in pbar: sleep(1) pbar.set_description(&amp;#34;Processing %s&amp;#34; % char) 1.2. 放一个迭代器在里面 count = 0 for batch_idx, item in tqdm(enumerate(train_loader)): count+=batch_idx print(count) 这种方式是我使用的方式。简单来说就是把迭代器train_loader放在tqdm里面，这里增加了一个enumerate去增加一个序列。</description>
    </item>
    
    <item>
      <title>16——计算机视觉大类分类</title>
      <link>https://ioyy900205.github.io/post/2021-05-19-16%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%A4%A7%E7%B1%BB%E5%88%86%E7%B1%BB/</link>
      <pubDate>Wed, 19 May 2021 16:22:00 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-19-16%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%A4%A7%E7%B1%BB%E5%88%86%E7%B1%BB/</guid>
      <description>待完善
框架重新梳理一下
在语义感知层面下
  1. 任务分类  1. 任务分类 分类、检测、识别、分割、检索、描述、生成
 参考资料</description>
    </item>
    
    <item>
      <title>15——torch.tensor叶子节点</title>
      <link>https://ioyy900205.github.io/post/2021-05-19-15torch.tensor-%E5%8F%B6%E5%AD%90%E8%8A%82%E7%82%B9/</link>
      <pubDate>Wed, 19 May 2021 09:53:52 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-19-15torch.tensor-%E5%8F%B6%E5%AD%90%E8%8A%82%E7%82%B9/</guid>
      <description>讨论叶子节点
  1. 理顺逻辑  1.1. 元素 1.2. 计算    1. 理顺逻辑 1.1. 元素 在pytorch的计算图中，只有两种元素：
数据（tensor）
  叶子节点(leaf node)
  叶子节点可以理解成不依赖其他tensor的tensor。
  在pytorch中，神经网络层中的权值w的tensor均为叶子节点。
  自己定义的tensor例如a=torch.tensor([1.0])定义的节点是叶子节点。
  All Tensors that have requires_grad which is False will be leaf Tensors by convention.
  For Tensors that have requires_grad which is True, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so grad_fn is None.</description>
    </item>
    
    <item>
      <title>14——NFNnet论文解读</title>
      <link>https://ioyy900205.github.io/post/2021-05-18-14nfnnet%E7%BF%BB%E8%AF%91/</link>
      <pubDate>Tue, 18 May 2021 15:33:30 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-18-14nfnnet%E7%BF%BB%E8%AF%91/</guid>
      <description>Title：High-Performance Large-Scale Image Recognition Without Normalization
Time: [v1] Thu, 11 Feb 2021 18:23:20 UTC (241 KB)
link：https://arxiv.org/abs/2102.06171
github: https://github.com/deepmind/deepmind-research/tree/master/nfnets
 NFNet（Normalizer-Free ResNets）是DeepMind提出了一种不需要Batch Normalization的基于ResNet的网络结构，其核心为一种AGC（adaptive gradient clipping technique，自适应梯度裁剪）技术。如下图所示，最小的NFNet版本达到了EfficientNet-B7的准确率，并且训练速度快了8.7倍，最大版本的模型实现了新的SOTA效果。
 1. 摘要 批量归一化是大多数图像分类模型的一个关键组成部分，但它有许多不理想的特性，源于它对批量大小和例子之间的相互作用的依赖。尽管最近的工作已经成功地训练了没有归一化层的深度ResNets，但这些模型并不符合最好的批量归一化网络的测试精度，而且对于大的学习率或强大的数据增量来说往往是不稳定的。在这项工作中，我们开发了一种自适应梯度剪裁技术，克服了这些不稳定性，并设计了一类明显改进的无归一化网络。我们较小的模型与EfficientNet-B7在ImageNet上的测试精度相匹配，同时训练速度快了8.7倍，而我们最大的模型达到了86.5%的最新顶级精度。此外，在对3亿张标记图像的数据集进行大规模预训练后，在ImageNet上进行微调时，无规范化模型的性能明显优于它们的批量规范化模型，我们最好的模型获得了89.2%的准确性。
2. 引言 最近计算机视觉领域的绝大多数模型都是深度残差网络的变种（He等人，2016b;a），通过批量规范化训练（Ioffe &amp;amp; Szegedy，2015）。这两个架构创新的结合使从业者能够训练出更深的网络，在训练集和测试集上都能达到更高的精度。批量归一化还可以平滑损失景观（Santurkar等人，2018），这使得在更大的学习率和更大的批次规模下进行稳定的训练（Bjorck等人，2018；De &amp;amp; Smith，2020），而且它可以产生正则化效应（Hoffer等人，2017；Luo等人，2018）。
然而，批量规范化有三个重要的实际缺点。首先，它是一个令人惊讶的昂贵的计算基元，会产生内存开销（Rota Bulo`等人，2018），并大大增加了在一些网络中评估梯度所需的时间（Gitman &amp;amp; Ginsburg, 2017）。第二，它引入了模型在训练期间和推理时间的行为之间的差异（Summers &amp;amp; Dinneen，2019；Singh &amp;amp; Shrivastava，2019），引入了必须调整的隐藏超参数。第三，也是最重要的一点，批量规范化打破了minibatch中训练实例之间的独立性。
这第三个属性有一系列的负面后果。例如，从业者发现，采用BN的网络往往难以在不同的硬件上精确复制，BN往往是导致微妙的实施错误的原因，特别是在分布式训练期间（Pham等人，2019）。此外，批处理归一化不能用于某些任务，因为批处理中的训练实例之间的互动使网络能够 &amp;ldquo;欺骗 &amp;ldquo;某些损失函数。例如，批量归一化需要特别注意防止一些对比学习算法中的信息泄露（Chen等人，2020；He等人，2020）。这也是序列建模任务的一个主要问题，这促使语言模型采用替代的规范化器（Ba等人，2016；Vaswani等人，2017）。如果在训练过程中，批量归一化的网络有很大的方差，那么批量归一化网络的性能也会下降（Shen等人，2020）。最后，批量归一化的性能对批量大小很敏感，当批量大小太小时，批量归一化网络的性能很差（Hoffer等人，2017；Ioffe，2017；Wu &amp;amp; He，2018），这限制了我们在有限硬件上可以训练的最大模型大小。我们在附录B中阐述了与批量归一化相关的挑战。
因此，尽管BN使深度学习社区近年来取得了实质性的进展，但我们预计从长远来看，它可能会阻碍进展。我们认为社区应该寻求确定一个简单的替代方案，以实现有竞争力的测试精度，并可用于广泛的任务。虽然已经提出了一些替代的规范化器（Ba等人，2016；Wu &amp;amp; He，2018；Huang等人，2020），但这些替代物往往实现了较差的测试精度，并引入了自己的缺点，如推理时的额外计算成本。幸运的是，近年来出现了两个有前途的研究主题。第一个是研究训练期间BN的好处的来源（Balduzzi等人，2017；Santurkar等人，2018；Bjorck等人，2018；Luo等人，2018；Yang等人，2019；Jacot等人。2019年；De &amp;amp; Smith，2020年），而第二种方法是在没有归一化层的情况下将深度ResNets训练到有竞争力的精度（Hanin &amp;amp; Rolnick，2018；Zhang等人，2019a；De &amp;amp; Smith，2020；Shao等人，2020；Brock等人，2021）。
图1
许多这些工作的一个关键主题是，通过抑制剩余分支上的隐性激活的规模，可以在没有规范化的情况下训练非常深的ResNets。实现这一目标的最简单方法是在每个残差分支的末端引入一个可学习的标量，初始化为零（Goyal等人，2017；Zhang等人，2019a；De &amp;amp; Smith，2020；Bachlechner等人，2020）。然而仅靠这一招还不足以在具有挑战性的基准上获得有竞争力的测试精度。另一项工作表明，ReLU激活引入了一个 &amp;ldquo;平均转移&amp;rdquo;，这导致不同训练实例的隐藏激活随着网络深度的增加而变得越来越相关（Huang等人，2017；Jacot等人，2019）。在最近的一项工作中，Brock等人（2021）引入了 &amp;ldquo;无正则化 &amp;ldquo;的ResNets，它在初始化时抑制了残余分支，并应用Scaled Weight Standardization（Qiao等人，2019）来消除平均移动。通过额外的正则化，这些未正则化的网络在ImageNet（Russakovsky等人，2015）上与批量正则化的ResNets（He等人，2016a）的性能相匹配，但它们在大批量时并不稳定，也不符合EfficientNets（Tan &amp;amp; Le，2019）的性能，即目前的技术状态（Gong等人，2020）。本文在这一工作思路的基础上，试图解决这些核心限制。我们的主要贡献如下:</description>
    </item>
    
    <item>
      <title>13————tensor内存占用计算实例</title>
      <link>https://ioyy900205.github.io/post/2021-05-17-13tensor%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8%E8%AE%A1%E7%AE%97%E5%AE%9E%E4%BE%8B/</link>
      <pubDate>Mon, 17 May 2021 19:01:17 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-17-13tensor%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8%E8%AE%A1%E7%AE%97%E5%AE%9E%E4%BE%8B/</guid>
      <description>网上查看了CSDN的代码，写的有误。所以这里写一个正确版本的
  ## 1. 各类型所占用的字节数 2. tensor内存计算 3. 清空内存  1. 各类型所占用的字节数  测试代码
import numpy as np import sys # 32位整型 ai32 = np.array([], dtype=np.int32) bi32 = np.arange(1, dtype=np.int32) ci32 = np.arange(5, dtype=np.int32) # 64位整型 ai64 = np.array([], dtype=np.int64) bi64 = np.arange(1, dtype=np.int64) ci64 = np.arange(5, dtype=np.int64) # 32位浮点数 af32 = np.array([], dtype=np.float32) bf32 = np.arange(1, dtype=np.float32) cf32 = np.arange(5, dtype=np.float32) # 64位浮点数 af64 = np.array([], dtype=np.float64) bf64 = np.</description>
    </item>
    
    <item>
      <title>12————阅读论文记录</title>
      <link>https://ioyy900205.github.io/post/2021-05-14-12%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87%E9%93%BE%E6%8E%A5/</link>
      <pubDate>Fri, 14 May 2021 15:20:41 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-14-12%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87%E9%93%BE%E6%8E%A5/</guid>
      <description>阅读论文记录
  1. 图像分类(Classification) 2. 目标检测(Object Detection) 3. 目标分割(Segmentation) 4. Others 5. 动态神经网络   参考资料
 1. 图像分类(Classification)  AlexNet http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf ZFNet(Visualizing and Understanding Convolutional Networks) https://arxiv.org/abs/1311.2901 VGG https://arxiv.org/abs/1409.1556 GoogLeNet, Inceptionv1(Going deeper with convolutions) https://arxiv.org/abs/1409.4842 Batch Normalization https://arxiv.org/abs/1502.03167 Inceptionv3(Rethinking the Inception Architecture for Computer Vision) https://arxiv.org/abs/1512.00567 Inceptionv4, Inception-ResNet https://arxiv.org/abs/1602.07261 Xception(Deep Learning with Depthwise Separable Convolutions) https://arxiv.org/abs/1610.02357 ResNet https://arxiv.org/abs/1512.03385 ResNeXt https://arxiv.org/abs/1611.05431 DenseNet https://arxiv.org/abs/1608.06993 NASNet-A(Learning Transferable Architectures for Scalable Image Recognition) https://arxiv.</description>
    </item>
    
    <item>
      <title>11——SSD:Single Shot MultiBox Detector</title>
      <link>https://ioyy900205.github.io/post/2021-05-13-11ssd_single-shot-multibox-detector-copy/</link>
      <pubDate>Thu, 13 May 2021 10:40:00 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-13-11ssd_single-shot-multibox-detector-copy/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. background 2. 目标损失函数 3. 多尺度多比例默认框 4. 负样本均衡 5. 数据增强 6. 性能比较  6.1. 在VOC2007测试集上的检测性能 6.2. 在VOC2012测试集上的检测性能 6.3. 在COCO测试集上的检测性能   7. 总结  1. background ECCV2016 author:wei liu 实现真正的实时
2016年，Wei Liu等人提出了SSD算法，它是继YOLOv1算法提出后的又一单阶段目标检测算法。在当时，单阶段算法相比于以R-CNN系列为主的双阶段算法更快，所以被学术界和工业界所青睐。为了提高检测的精度，SSD创造性地提出了多尺度预测的目标检测算法，使得网络更容易适应对不同大小物体的检测。
放上对比结构如下： 我的理解是 网络抽取不同的特征层进行预测，较低的特征层可以预测教细节物体，较高特征层可以预测教大物体.
整体结构上，SSD采用了VGG-16作为骨干网络提取特征。在骨干网络末尾添加了若干个特征层，特征层与特征层之间使用1x1和3x3的卷积核计算特征和降采样，并从中选择了6个不同大小的特征层来预测目标。
需要注意的是，用来预测目标的每一个特征层上预设的Anchors数量不一定是相同的。比如对于38x38的特征层，每个格子预设4个不同比例的Anchors；对于19x19的特征层，每个格子预设了6个不同比例的Anchors。
2. 目标损失函数 每一个目标检测网络都具有其独特的目标损失函数，它决定着网络训练迭代的方向。
根据目标检测网络的一般定义，我们知道总损失函数一般由两部分组成：定位损失和置信度损失。
其中，定位损失计算公式为：
定位损失函数借鉴了Faster R-CNN中的Smooth L1函数，用来计算预测边界框 l 和真实边界框 g 之间的损失误差。其中，m 是代表边界框位置信息的集合{cx，cy，w，h}。
容易看出，SSD没有直接将目标的中心位置、长宽作为真实标签。而是采取真实框与预选框Anchors的偏移作为真实标签。那么在学习的过程中，也会去主动预测偏移值，这样更有利于网络的训练，避免在训练初期产生较大的振荡。
置信度损失函数是在多个类别置信度c上的softmax损失：
3. 多尺度多比例默认框 前面已经提到，SSD算法中采用了类似于Faster R-CNN中的Anchors机制用于目标框的回归。值得一提的是，SSD采用的Anchors方法更为灵活，不仅在每一个特征层设置了不同大小和比例的预选框，而且针对于不同的特征层，也设计了相应的大小的预选框。
经过计算可知，对于越深的特征层（尺寸越小），设置的预选框尺寸越大。这是因为，尺寸越小的特征层，感受野越大。SSD的目的就是：要让感受野小的特征层检测小目标，使用感受野大的特征层检测更大的目标。
4. 负样本均衡 在训练过程中，大部分的预选框Anchors并不能和真实框匹配上，因此负样本很多，而正样本却很少，正负样本数量严重失衡，不利于训练。
考虑到这个问题，SSD在训练过程中，没有选取所有的负样本。而是先将负样本的置信度损失进行排序，仅选取置信度高的一些负样本，使得负样本数与正样本数之间的比率最大不超过3：1。作者认为，这样使得训练更稳定更快。
5. 数据增强 数据增强又叫数据增广，是扩大数据集的一种方式，经常用于深度学习中来防止过拟合。在SSD算法中，也使用了一些数据增强的技巧。采用数据增强后，目标检测性能也得到了明显的提升，具体实验结果如下:
6. 性能比较 6.</description>
    </item>
    
    <item>
      <title>09——Faster R-CNN</title>
      <link>https://ioyy900205.github.io/post/2021-05-12-09-faster-r-cnn/</link>
      <pubDate>Wed, 12 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-12-09-faster-r-cnn/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. 背景  1.1. 时间线 1.2. 结构  1.2.1. CNN 1.2.2. RPN 1.2.3. ROI Pooling     2. 一些问题 3. 损失函数 4. 后处理  4.1. NMS 4.2. Proposal selection 4.3. Standalone application   5. 训练 6. 推理 7. 后记  1. 背景 Faster R-CNN最初发表于NIPS 2015。发表后，它经历了几次修改。
Faster R-CNN是R-CNN论文的第三次迭代&amp;ndash;Ross Girshick是作者和合作者。
1.1. 时间线   R-CNN
Published in 2014
“Rich feature hierarchies for accurate object detection and semantic segmentation”,</description>
    </item>
    
    <item>
      <title>10——目标检测参数</title>
      <link>https://ioyy900205.github.io/post/2021-05-12-10%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%8F%82%E6%95%B0/</link>
      <pubDate>Wed, 12 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-12-10%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%8F%82%E6%95%B0/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. background Pascal VOC 和 COCO 2. precision 和 recall  2.1. 基本定义 2.2. precision-recall curve   3. COCO Evaluation Result 4. 参考  1. background Pascal VOC 和 COCO 最近看目标检测，少不了了解其中参数的意义。一顿搜所哎。
最精简的回答：
目标检测中衡量识别精度的指标是mAP（mean average precision）。多个类别物体检测中，每一个类别都可以根据recall和precision绘制一条曲线，AP就是该曲线下的面积，mAP是多个类别AP的平均值
番外篇 在coco数据集出来之前，基本都用pascal voc 现在都用coco了。
2. precision 和 recall 2.1. 基本定义  precision查准率 preicision是在你认为的正样本中， 有多大比例真的是正样本 recall查全率 recall则是在真正的正样本中， 有多少被你找到了  2.2. precision-recall curve 如果threshold太高， prediction非常严格， 所以我们认为是鸭子的基本都是鸭子，precision就高了；但也因为筛选太严格， 我们也放过了一些score比较低的鸭子， 所以recall就低了
如果threshold太低， 什么都会被当成鸭子， precision就会很低， recall就会很高。</description>
    </item>
    
    <item>
      <title>08——数据集操作和图像处理——PyTorch</title>
      <link>https://ioyy900205.github.io/post/2021-05-11-voc2012%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%9B%BE%E7%89%87%E5%92%8Cbbox%E4%BF%A1%E6%81%AF%E6%98%BE%E7%A4%BA/</link>
      <pubDate>Tue, 11 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-11-voc2012%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%9B%BE%E7%89%87%E5%92%8Cbbox%E4%BF%A1%E6%81%AF%E6%98%BE%E7%A4%BA/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. 数据集读取 2. 转换函数 3. bbox画图  1. 数据集读取 from torch.utils.data import Dataset import os import torch import json from PIL import Image from lxml import etree class VOC2012DataSet(Dataset): &amp;#34;&amp;#34;&amp;#34;读取解析PASCAL VOC2012数据集&amp;#34;&amp;#34;&amp;#34; def __init__(self, voc_root, transforms, txt_name: str = &amp;#34;train.txt&amp;#34;): self.root = os.path.join(voc_root, &amp;#34;VOCdevkit&amp;#34;, &amp;#34;VOC2012&amp;#34;) self.img_root = os.path.join(self.root, &amp;#34;JPEGImages&amp;#34;) self.annotations_root = os.path.join(self.root, &amp;#34;Annotations&amp;#34;) # read train.txt or val.txt file txt_path = os.path.join(self.root, &amp;#34;ImageSets&amp;#34;, &amp;#34;Main&amp;#34;, txt_name) assert os.</description>
    </item>
    
    <item>
      <title>06——torch.cat维度操作——PyTorch</title>
      <link>https://ioyy900205.github.io/post/2021-05-10-torch.cat%E7%9A%84%E7%BB%B4%E5%BA%A6%E6%93%8D%E4%BD%9C/</link>
      <pubDate>Mon, 10 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-10-torch.cat%E7%9A%84%E7%BB%B4%E5%BA%A6%E6%93%8D%E4%BD%9C/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. torch.cat  1.1. 二维数组  1.1.1. dim=0 1.1.2. dim=1   1.2. 三维数组  1.2.1. dim=0 1.2.2. dim=1 1.2.3. dim=2     2. 小结  1. torch.cat 1.1. 二维数组 1.1.1. dim=0 运行
import torch A = torch.ones(2,3) #2x3的张量（矩阵）  print(&amp;#34;A:&amp;#34;,A) B=2*torch.ones(4,3) #4x3的张量（矩阵）  print(&amp;#34;B:&amp;#34;,B) C=torch.cat((A,B),0)#按维数0（行）拼接 print(&amp;#34;C:&amp;#34;,C) print(C.size()) 结果
A: tensor([[1., 1., 1.], [1., 1., 1.]]) B: tensor([[2., 2., 2.], [2., 2., 2.], [2.</description>
    </item>
    
    <item>
      <title>07——torch.stack维度操作——PyTorch</title>
      <link>https://ioyy900205.github.io/post/2021-05-10-torch.stack%E7%9A%84%E7%BB%B4%E5%BA%A6%E6%93%8D%E4%BD%9C/</link>
      <pubDate>Mon, 10 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-10-torch.stack%E7%9A%84%E7%BB%B4%E5%BA%A6%E6%93%8D%E4%BD%9C/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. 初始化 2. torch.stak  2.1. dim = 0 2.2. dim = 1 2.3. dim = 2    1. 初始化 input
import torch import numpy as np # 创建3*3的矩阵，a、b a=np.array([[1,2,3],[4,5,6],[7,8,9]]) b=np.array([[10,20,30],[40,50,60],[70,80,90]]) # 将矩阵转化为Tensor a = torch.from_numpy(a) b = torch.from_numpy(b) # 打印a、b、c print(a,a.size()) print(b,b.size()) output
tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) torch.Size([3, 3]) tensor([[10, 20, 30], [40, 50, 60], [70, 80, 90]]) torch.</description>
    </item>
    
    <item>
      <title>审稿学习系列01——图像质量评估</title>
      <link>https://ioyy900205.github.io/post/2021-05-10-%E5%AE%A1%E7%A8%BF%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%9701/</link>
      <pubDate>Mon, 10 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-10-%E5%AE%A1%E7%A8%BF%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%9701/</guid>
      <description>审稿学习系列01——图像质量评估 LIU Liang
  1. 背景 2. 图像质量评估（Image Quality Assessment, IQA）——方法分类  2.1. 主观方法 2.2. 客观方法   3. 图像质量评估（Image Quality Assessment, IQA）——图像提供信息分类  3.1. 全参考(Full Reference-IQA, FR-IQA) 3.2. 半参考(Reduced Reference-IQA, RR-IQA) 3.3. 无参考(No Reference-IQA, NR-IQA)   4. 数据集 5. 评估方法  5.1. 评估指标   6. 结果  1. 背景 质量评估(Quality Assessment，QA)在许多领域有其广泛的实用性。（比如图像压缩、视频编解码、视频监控等。）
并且对高效、可靠质量评估的需求日益增加，所以QA成为一个感兴趣的研究领域。
每年都涌现出大量的新的QA算法，有些是扩展已有的算法，也有一些是QA算法的应用。
质量评估可分为：
  图像质量评估（Image Quality Assessment, IQA）
  视频质量评估（Video Quality Assessment, VQA）</description>
    </item>
    
    <item>
      <title>05——ShuffleNet学习</title>
      <link>https://ioyy900205.github.io/post/2021-05-07-shufflenet/</link>
      <pubDate>Fri, 07 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-07-shufflenet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>03——nn.module学习——PyTorch</title>
      <link>https://ioyy900205.github.io/post/2021-04-27-nn.module/</link>
      <pubDate>Tue, 27 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-04-27-nn.module/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. nn.moduel初步  1.1. Linear类 1.2. Conv2d类 1.3. 自定义层的步骤   2. 自定义层  2.1. 定义一个自定义层MyLayer 2.2. 自定义模型并且训练    1. nn.moduel初步 keras更加注重的是层Layer、pytorch更加注重的是模型Module。
1.1. Linear类 import math import torch from torch.nn.parameter import Parameter from .. import functional as F from .. import init from .module import Module from ..._jit_internal import weak_module, weak_script_method class Linear(Module): __constants__ = [&amp;#39;bias&amp;#39;] def __init__(self, in_features, out_features, bias=True): super(Linear, self).__init__() self.</description>
    </item>
    
    <item>
      <title>04——轻量级神经网络总结</title>
      <link>https://ioyy900205.github.io/post/2021-04-27-%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/</link>
      <pubDate>Tue, 27 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-04-27-%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. 需解决的问题 2. 衡量指标  2.1. FLOPs 2.2. Params  2.2.1. 卷积层的参数量 2.2.2. 全连接层的参数量   2.3. MAC 2.4. MACC(也叫MADD)   3. 方法  3.1. 模型结构设计  3.1.1. 分组卷积 3.1.2. 分解卷积   3.2. 模型压缩  3.2.1. 权值量化 3.2.2. 网络剪枝 3.2.3. 低秩近似 3.2.4. 知识蒸馏      1. 需解决的问题 存储问题
数百层网络有着大量的权值参数，保存大量权值参数对设备的内存要求很高； 速度问题
在实际应用中，往往是毫秒级别，为了达到实际应用标准，要么提高处理器性能，要么就减少计算量。 而提高处理器性能在短时间内是无法完成的，因此减少计算量成为了主要的技术手段。 2. 衡量指标 目前，网络架构设计主要由计算复杂度的间接度量（如FLOPs）测量。然而，直接度量（例如，速度）还取决于诸如存储器访问成本和平台特性的其他因素。
2.1. FLOPs FLOPS(floating point operations per second)</description>
    </item>
    
    <item>
      <title>01——ResNeXt学习</title>
      <link>https://ioyy900205.github.io/post/2021-04-26-resnext/</link>
      <pubDate>Mon, 26 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-04-26-resnext/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. ResNeXt  1.1. 概述 1.2. 思路 1.3. Block 1.4. 组卷积 1.5. 模型复杂度 1.6. Shortcut 1.7. 结果    1. ResNeXt 1.1. 概述 论文：Aggregated Residual Transformations for Deep Neural Networks
论文链接：https://arxiv.org/abs/1611.05431
PyTorch代码：https://github.com/miraclewkf/ResNeXt-PyTorch 2016年,ISCLVCR 2016 no.2
1.2. 思路 Split-Transform-Merge （来源于inception）
堆叠（来源于VGG）
1.3. Block cardinality ，原文的解释是the size of the set of transformations，如上图右边是 cardinality=32 的样子，这里注意每个被聚合的拓扑结构都是一样的(这也是和 Inception 的差别，减轻设计负担)
1.4. 组卷积 最早可以追溯到AlexNet。
32x4d 中 32为组卷积数目，4d为每组卷积4个卷积核。
组卷积可以有不同的配置，但是不同的配置需要通过实验判断效果。
可以发现通过组卷积能够有效降低parameter的大小。
1.5. 模型复杂度 如果想增加模型复杂度，几个选择：</description>
    </item>
    
    <item>
      <title>02——SeNet学习</title>
      <link>https://ioyy900205.github.io/post/2021-04-26-senet/</link>
      <pubDate>Mon, 26 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-04-26-senet/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. SENET  1.1. 链接 1.2. 贡献  1.2.1. 提供了子结构 1.2.2. SOTA   1.3. 核心思想 1.4. Squeeze 1.5. Excitation   2. 思考  2.1. 浅层作用较大    1. SENET 1.1. 链接 论文：Squeeze-and-Excitation Networks论文链接：https://arxiv.org/abs/1709.01507代码地址：https://github.com/hujie-frank/SENetPyTorch代码地址：https://github.com/miraclewkf/SENet-PyTorch
A central theme of computer vision research is the search for more powerful representations that capture only those properties of an image that are most salient for a given task</description>
    </item>
    
    <item>
      <title>REF01——避坑指南——打印的RESNET18</title>
      <link>https://ioyy900205.github.io/post/2021-04-25-ref01%E9%81%BF%E5%9D%91%E6%8C%87%E5%8D%97%E6%89%93%E5%8D%B0%E7%9A%84resnet18/</link>
      <pubDate>Sun, 25 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-04-25-ref01%E9%81%BF%E5%9D%91%E6%8C%87%E5%8D%97%E6%89%93%E5%8D%B0%E7%9A%84resnet18/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. BasicBlock 2. BottleNeck 3. ResNet18  1. BasicBlock 其中有个坑是下采样。如下图所示：
具体来说，如代码所展示的：
if stride != 1 or in_planes != self.expansion*planes: self.shortcut = nn.Sequential( nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(self.expansion*planes) ) 当 stride不等于1 or 输入通道数和输出通道数不相同 时： 进行下采样操作。
下采样：Conv2D+BN
目的是让跳层能够保持维数
2. BottleNeck 同样的BottleNeck也存在这样的问题。
3. ResNet18 ResNet( (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layer1): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.</description>
    </item>
    
    <item>
      <title>About Me</title>
      <link>https://ioyy900205.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/about/</guid>
      <description>I was born in 1990 in Wuhan, Hubei Province.
I received dual B.S., Wuhan University and Wuhan University of Technology in 2012, Ph.D., North China University of Electric Power in 2017.
Interests: Deep learning, computer vision
Award: National Scholarship, Distinguished Graduate Scholarship, ICONE-22 Best Paper</description>
    </item>
    
  </channel>
</rss>
