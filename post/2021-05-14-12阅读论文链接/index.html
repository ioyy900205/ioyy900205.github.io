<!doctype html>
<html lang="en-us">
  <head>
    <title>12————阅读论文记录 // 亮的笔记</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.90.1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Liu Liang" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://ioyy900205.github.io/css/main.min.88e7083eff65effb7485b6e6f38d10afbec25093a6fac42d734ce9024d3defbd.css" />

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="12————阅读论文记录"/>
<meta name="twitter:description" content="阅读论文记录
  1. 图像分类(Classification) 2. 目标检测(Object Detection) 3. 目标分割(Segmentation) 4. Others 5. 动态神经网络   参考资料
 1. 图像分类(Classification)  AlexNet http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf ZFNet(Visualizing and Understanding Convolutional Networks) https://arxiv.org/abs/1311.2901 VGG https://arxiv.org/abs/1409.1556 GoogLeNet, Inceptionv1(Going deeper with convolutions) https://arxiv.org/abs/1409.4842 Batch Normalization https://arxiv.org/abs/1502.03167 Inceptionv3(Rethinking the Inception Architecture for Computer Vision) https://arxiv.org/abs/1512.00567 Inceptionv4, Inception-ResNet https://arxiv.org/abs/1602.07261 Xception(Deep Learning with Depthwise Separable Convolutions) https://arxiv.org/abs/1610.02357 ResNet https://arxiv.org/abs/1512.03385 ResNeXt https://arxiv.org/abs/1611.05431 DenseNet https://arxiv.org/abs/1608.06993 NASNet-A(Learning Transferable Architectures for Scalable Image Recognition) https://arxiv."/>

    <meta property="og:title" content="12————阅读论文记录" />
<meta property="og:description" content="阅读论文记录
  1. 图像分类(Classification) 2. 目标检测(Object Detection) 3. 目标分割(Segmentation) 4. Others 5. 动态神经网络   参考资料
 1. 图像分类(Classification)  AlexNet http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf ZFNet(Visualizing and Understanding Convolutional Networks) https://arxiv.org/abs/1311.2901 VGG https://arxiv.org/abs/1409.1556 GoogLeNet, Inceptionv1(Going deeper with convolutions) https://arxiv.org/abs/1409.4842 Batch Normalization https://arxiv.org/abs/1502.03167 Inceptionv3(Rethinking the Inception Architecture for Computer Vision) https://arxiv.org/abs/1512.00567 Inceptionv4, Inception-ResNet https://arxiv.org/abs/1602.07261 Xception(Deep Learning with Depthwise Separable Convolutions) https://arxiv.org/abs/1610.02357 ResNet https://arxiv.org/abs/1512.03385 ResNeXt https://arxiv.org/abs/1611.05431 DenseNet https://arxiv.org/abs/1608.06993 NASNet-A(Learning Transferable Architectures for Scalable Image Recognition) https://arxiv." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ioyy900205.github.io/post/2021-05-14-12%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87%E9%93%BE%E6%8E%A5/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-05-14T15:20:41+08:00" />
<meta property="article:modified_time" content="2021-05-14T15:20:41+08:00" />



  </head>
  <body>
    <header class="app-header">
      <a href="https://ioyy900205.github.io"><img class="app-header-avatar" src="https://ss1.bdstatic.com/70cFvXSh_Q1YnxGkpoWK1HF6hhy/it/u=3520060145,2875461924&amp;fm=26&amp;gp=0.jpg" alt="Liu Liang" /></a>
      <h1>亮的笔记</h1>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/">Home</a>
             - 
          
          <a class="app-header-menu-item" href="/tags/">Tags</a>
             - 
          
          <a class="app-header-menu-item" href="/about/">About</a>
      </nav>
      <p>深度学习笔记本</p>
      <div class="app-header-social">
        
          <a href="https://github.com/ioyy900205" target="_blank" rel="noreferrer noopener">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>Github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg>
          </a>
        
          <a href="https://twitter.com/ioyy900205" target="_blank" rel="noreferrer noopener">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-twitter">
  <title>Twitter</title>
  <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path>
</svg>
          </a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">12————阅读论文记录</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          May 14, 2021
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          9 min read
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7.01" y2="7"></line>
</svg>
              <a class="tag" href="https://ioyy900205.github.io/tags/paper_review/">Paper_Review</a>
        </div>
      </div>
    </header>
    <div class="post-content">
      <p>阅读论文记录</p>
<hr>
<ul>
<li><a href="#1-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BBclassification">1. 图像分类(Classification)</a></li>
<li><a href="#2-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Bobject-detection">2. 目标检测(Object Detection)</a></li>
<li><a href="#3-%E7%9B%AE%E6%A0%87%E5%88%86%E5%89%B2segmentation">3. 目标分割(Segmentation)</a></li>
<li><a href="#4-others">4. Others</a></li>
<li><a href="#5-%E5%8A%A8%E6%80%81%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">5. 动态神经网络</a></li>
</ul>
<hr>
<p><strong>参考资料</strong></p>
<hr>
<h2 id="1-图像分类classification">1. 图像分类(Classification)</h2>
<ul>
<li>AlexNet <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></li>
<li>ZFNet(Visualizing and Understanding Convolutional Networks) <a href="https://arxiv.org/abs/1311.2901">https://arxiv.org/abs/1311.2901</a></li>
<li>VGG <a href="https://arxiv.org/abs/1409.1556">https://arxiv.org/abs/1409.1556</a></li>
<li>GoogLeNet, Inceptionv1(Going deeper with convolutions) <a href="https://arxiv.org/abs/1409.4842">https://arxiv.org/abs/1409.4842</a></li>
<li>Batch Normalization <a href="https://arxiv.org/abs/1502.03167">https://arxiv.org/abs/1502.03167</a></li>
<li>Inceptionv3(Rethinking the Inception Architecture for Computer Vision) <a href="https://arxiv.org/abs/1512.00567">https://arxiv.org/abs/1512.00567</a></li>
<li>Inceptionv4, Inception-ResNet <a href="https://arxiv.org/abs/1602.07261">https://arxiv.org/abs/1602.07261</a></li>
<li>Xception(Deep Learning with Depthwise Separable Convolutions) <a href="https://arxiv.org/abs/1610.02357">https://arxiv.org/abs/1610.02357</a></li>
<li>ResNet <a href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</a></li>
<li>ResNeXt <a href="https://arxiv.org/abs/1611.05431">https://arxiv.org/abs/1611.05431</a></li>
<li>DenseNet <a href="https://arxiv.org/abs/1608.06993">https://arxiv.org/abs/1608.06993</a></li>
<li>NASNet-A(Learning Transferable Architectures for Scalable Image Recognition) <a href="https://arxiv.org/abs/1707.07012">https://arxiv.org/abs/1707.07012</a></li>
<li>SENet(Squeeze-and-Excitation Networks) <a href="https://arxiv.org/abs/1709.01507">https://arxiv.org/abs/1709.01507</a></li>
<li>MobileNet(v1) <a href="https://arxiv.org/abs/1704.04861">https://arxiv.org/abs/1704.04861</a></li>
<li>MobileNet(v2) <a href="https://arxiv.org/abs/1801.04381">https://arxiv.org/abs/1801.04381</a></li>
<li>MobileNet(v3) <a href="https://arxiv.org/abs/1905.02244">https://arxiv.org/abs/1905.02244</a></li>
<li>ShuffleNet(v1) <a href="https://arxiv.org/abs/1707.01083">https://arxiv.org/abs/1707.01083</a></li>
<li>ShuffleNet(v2) <a href="https://arxiv.org/abs/1807.11164">https://arxiv.org/abs/1807.11164</a></li>
<li>Bag of Tricks for Image Classification with Convolutional Neural Networks <a href="https://arxiv.org/abs/1812.01187">https://arxiv.org/abs/1812.01187</a></li>
<li>EfficientNet(v1) <a href="https://arxiv.org/abs/1905.11946">https://arxiv.org/abs/1905.11946</a></li>
<li>EfficientNet(v2) <a href="https://arxiv.org/abs/2104.00298">https://arxiv.org/abs/2104.00298</a></li>
<li>CSPNet <a href="https://arxiv.org/abs/1911.11929">https://arxiv.org/abs/1911.11929</a></li>
<li>RegNet <a href="https://arxiv.org/abs/2003.13678">https://arxiv.org/abs/2003.13678</a></li>
<li>NFNets(High-Performance Large-Scale Image Recognition Without Normalization) <a href="https://arxiv.org/abs/2102.06171">https://arxiv.org/abs/2102.06171</a></li>
<li></li>
</ul>
<h2 id="2-目标检测object-detection">2. 目标检测(Object Detection)</h2>
<ul>
<li>R-CNN <a href="https://arxiv.org/abs/1311.2524">https://arxiv.org/abs/1311.2524</a></li>
<li>Fast R-CNN <a href="https://arxiv.org/abs/1504.08083">https://arxiv.org/abs/1504.08083</a></li>
<li>Faster R-CNN <a href="https://arxiv.org/abs/1506.01497">https://arxiv.org/abs/1506.01497</a></li>
<li>Mask R-CNN <a href="https://arxiv.org/abs/1703.06870">https://arxiv.org/abs/1703.06870</a></li>
<li>SSD <a href="https://arxiv.org/abs/1512.02325">https://arxiv.org/abs/1512.02325</a></li>
<li>FPN(Feature Pyramid Networks for Object Detection) <a href="https://arxiv.org/abs/1612.03144">https://arxiv.org/abs/1612.03144</a></li>
<li>RetinaNet(Focal Loss for Dense Object Detection) <a href="https://arxiv.org/abs/1708.02002">https://arxiv.org/abs/1708.02002</a></li>
<li>Bag of Freebies for Training Object Detection Neural Networks <a href="https://arxiv.org/abs/1902.04103">https://arxiv.org/abs/1902.04103</a></li>
<li>YOLOv1 <a href="https://arxiv.org/abs/1506.02640">https://arxiv.org/abs/1506.02640</a></li>
<li>YOLOv2 <a href="https://arxiv.org/abs/1506.02640">https://arxiv.org/abs/1612.08242</a></li>
<li>YOLOv3 <a href="https://arxiv.org/abs/1804.02767">https://arxiv.org/abs/1804.02767</a></li>
<li>YOLOv4 <a href="https://arxiv.org/abs/2004.10934">https://arxiv.org/abs/2004.10934</a></li>
</ul>
<h2 id="3-目标分割segmentation">3. 目标分割(Segmentation)</h2>
<ul>
<li>FCN(Fully Convolutional Networks for Semantic Segmentation) <a href="https://arxiv.org/abs/1411.4038">https://arxiv.org/abs/1411.4038</a></li>
<li>UNet(U-Net: Convolutional Networks for Biomedical Image Segmentation) <a href="https://arxiv.org/abs/1505.04597">https://arxiv.org/abs/1505.04597</a></li>
<li>DeepLabv1(Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs) <a href="https://arxiv.org/abs/1412.7062">https://arxiv.org/abs/1412.7062</a></li>
<li>DeepLabv2(Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs) <a href="https://arxiv.org/abs/1606.00915">https://arxiv.org/abs/1606.00915</a></li>
<li>DeepLabv3(Rethinking Atrous Convolution for Semantic Image Segmentation) <a href="https://arxiv.org/abs/1706.05587">https://arxiv.org/abs/1706.05587</a></li>
<li>DeepLabv3+(Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation) <a href="https://arxiv.org/abs/1802.02611">https://arxiv.org/abs/1802.02611</a></li>
</ul>
<h2 id="4-others">4. Others</h2>
<ul>
<li>Microsoft COCO: Common Objects in Context <a href="https://arxiv.org/abs/1405.0312">https://arxiv.org/abs/1405.0312</a></li>
<li>The PASCALVisual Object Classes Challenge: A Retrospective <a href="http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham15.pdf">http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham15.pdf</a></li>
</ul>
<h2 id="5-动态神经网络">5. 动态神经网络</h2>
<ul>
<li>
<p>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In NeurIPS, 2012.</p>
</li>
<li>
<p>Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.</p>
</li>
<li>
<p>Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, 2015.</p>
</li>
<li>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.</p>
</li>
<li>
<p>Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR, 2017.</p>
</li>
<li>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.</p>
</li>
<li>
<p>Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. In ICLR, 2017.</p>
</li>
<li>
<p>Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Dif- ferentiable Architecture Search. In ICLR, 2018.</p>
</li>
<li>
<p>Alex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016.</p>
</li>
<li>
<p>Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, and Kilian Weinberger. Multi-scale dense networks for resource efficient image classification. In ICLR, 2018.</p>
</li>
<li>
<p>Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. Condconv: Conditionally parameterized convolutions for effi- cient inference. In NeurIPS, 2019.</p>
</li>
<li>
<p>Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. In NeurIPs, 2017.</p>
</li>
<li>
<p>Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In NeurIPS, 2017.</p>
</li>
<li>
<p>Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In ICLR, 2017.</p>
</li>
<li>
<p>Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Lu Yuan, and Zicheng Liu. Dynamic convolution: Attention over convolution kernels. In CVPR, 2020.</p>
</li>
<li>
<p>Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In CVPR, 2018.</p>
</li>
<li>
<p>Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module. In ECCV, 2018.</p>
</li>
<li>
<p>Jiaolong Yang, Peiran Ren, Dongqing Zhang, Dong Chen, Fang Wen, Hongdong Li, and Gang Hua. Neural aggregation network for video face recognition. In CVPR, 2017.</p>
</li>
<li>
<p>Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neu- ral networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.</p>
</li>
<li>
<p>Gao Huang, Shichen Liu, Laurens Van der Maaten, and Kilian Q Weinberger. Condensenet: An efficient densenet using learned group convolutions. In CVPR, 2018.</p>
</li>
<li>
<p>Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks. In NeurIPS, 2016.</p>
</li>
<li>
<p>Le Yang, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, and Gao Huang. Resolution Adaptive Networks for Efficient Inference. In CVPR, 2020.</p>
</li>
<li>
<p>Michael Figurnov, Maxwell D Collins, Yukun Zhu, Li Zhang, Jonathan Huang, Dmitry Vetrov, and Ruslan Salakhutdinov. Spa- tially adaptive computation time for residual networks. In CVPR, 2017.</p>
</li>
<li>
<p>Xiaoxiao Li, Ziwei Liu, Ping Luo, Chen Change Loy, and Xiaoou Tang. Not all pixels are equal: Difficulty-aware semantic segmen- tation via deep layer cascade. In CVPR, 2017.</p>
</li>
<li>
<p>Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszko- reit, and Lukasz Kaiser. Universal Transformers. In ICLR, 2019.</p>
</li>
<li>
<p>Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli. Depth-Adaptive Transformer. In ICLR, 2020.</p>
</li>
<li>
<p>David H Hubel and Torsten N Wiesel. Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex. The Journal of physiology, 1962.</p>
</li>
<li>
<p>Akira Murata, Vittorio Gallese, Giuseppe Luppino, Masakazu Kaseda, and Hideo Sakata. Selectivity for the shape, size, and orientation of objects for grasping in neurons of monkey parietal area aip. Journal of neurophysiology, 2000.</p>
</li>
<li>
<p>Yulin Wang, Kangchen Lv, Rui Huang, Shiji Song, Le Yang, and Gao Huang. Glance and focus: a dynamic approach to reducing spatial redundancy in image classification. In NeurIPS, 2020.</p>
</li>
<li>
<p>Paul Viola and Michael J. Jones. Robust real-time face detection. IJCV, 2004.</p>
</li>
<li>
<p>Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Ge- offrey E Hinton. Adaptive mixtures of local experts. Neural computation, 1991.</p>
</li>
<li>
<p>Wolfgang Maass. Networks of spiking neurons: the third gener- ation of neural network models. Neural networks, 1997.</p>
</li>
<li>
<p>Eugene M Izhikevich. Simple model of spiking neurons. TNN, 2003.</p>
</li>
<li>
<p>Tolga Bolukbasi, Joseph Wang, Ofer Dekel, and Venkatesh Saligrama. Adaptive neural networks for efficient inference. In ICML, 2017.</p>
</li>
<li>
<p>Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference via early exiting from deep neural networks. In ICPR, 2016.</p>
</li>
<li>
<p>Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez. Skipnet: Learning dynamic routing in convolutional networks. In ECCV, 2018.</p>
</li>
<li>
<p>Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie, Larry S Davis, Kristen Grauman, and Rogerio Feris. Blockdrop: Dynamic inference paths in residual networks. In CVPR, 2018.</p>
</li>
<li>
<p>Andrew Davis and Itamar Arel. Low-rank approximations for conditional feedforward computation in deep neural networks. arXiv preprint arXiv:1312.4461, 2013.</p>
</li>
<li>
<p>Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Ge- offrey E Hinton. Adaptive mixtures of local experts. Neural computation, 1991.</p>
</li>
<li>
<p>David Eigen, Marc’Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep mixture of experts. In ICLR Workshop, 2013.</p>
</li>
<li>
<p>Weizhe Hua, Yuan Zhou, Christopher M De Sa, Zhiru Zhang, and G Edward Suh. Channel gating neural networks. In NeurIPS, 2019.</p>
</li>
<li>
<p>Chuanjian Liu, Yunhe Wang, Kai Han, Chunjing Xu, and Chang Xu. Learning instance-wise sparsity for accelerating deep models. In IJCAI, 2019.</p>
</li>
<li>
<p>Yue Wang, Jianghao Shen, Ting-Kuei Hu, Pengfei Xu, Tan Nguyen, Richard G. Baraniuk, Zhangyang Wang, and Yingyan Lin. Dual dynamic inference: Enabling more efficient, adaptive and controllable deep inference. JSTSP, 2020.</p>
</li>
<li>
<p>Wenhan Xia, Hongxu Yin, Xiaoliang Dai, and Niraj K Jha. Fully dynamic inference with deep neural networks. arXiv preprint arXiv:2007.15151, 2020.</p>
</li>
<li>
<p>Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In ICCV, 2017.</p>
</li>
<li>
<p>Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. Deformable convnets v2: More deformable, better results. In CVPR, 2019.</p>
</li>
<li>
<p>Hang Gao, Xizhou Zhu, Stephen Lin, and Jifeng Dai. Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation. In ICLR, 2019.</p>
</li>
<li>
<p>Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V Gool. Dynamic filter networks. In NeurIPS, 2016.</p>
</li>
<li>
<p>David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. In ICLR, 2016.</p>
</li>
<li>
<p>Ningning Ma, Xiangyu Zhang, Jiawei Huang, and Jian Sun. WeightNet: Revisiting the Design Space of Weight Networks. In ECCV, 2020.</p>
</li>
<li>
<p>Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang. Residual attention network for image classification. In CVPR, 2017.</p>
</li>
<li>
<p>Abhijit Guha Roy, Nassir Navab, and Christian Wachinger. Con- current spatial and channel ‘squeeze &amp; excitation’in fully convo- lutional networks. In MICCAI, 2018.</p>
</li>
<li>
<p>Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Lu Yuan, and Zicheng Liu. Dynamic relu. In ECCV, 2020.</p>
</li>
<li>
<p>Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for discriminative localization. In CVPR, 2016.</p>
</li>
<li>
<p>Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neu- ral networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.</p>
</li>
<li>
<p>Zhenda Xie, Zheng Zhang, Xizhou Zhu, Gao Huang, and Stephen Lin. Spatially Adaptive Inference with Stochastic Feature Sam- pling and Interpolation. In ECCV, 2020.</p>
</li>
<li>
<p>Alexander Kirillov, Yuxin Wu, Kaiming He, and Ross Girshick. Pointrend: Image segmentation as rendering. In CVPR, 2020.</p>
</li>
<li>
<p>Jin Chen, Xijun Wang, Zichao Guo, Xiangyu Zhang, and Jian Sun. Dynamic region-aware convolution. arXiv preprint arXiv:2003.12243, 2020.</p>
</li>
<li>
<p>Max Jaderberg, Karen Simonyan, and Andrew Zisserman. Spatial transformer networks. In NeurIPS, 2015.</p>
</li>
<li>
<p>Adria Recasens, Petr Kellnhofer, Simon Stent, Wojciech Matusik, and Antonio Torralba. Learning to zoom: a saliency-based sam- pling layer for neural networks. In ECCV, 2018.</p>
</li>
<li>
<p>Zerui Yang, Yuhui Xu, Wenrui Dai, and Hongkai Xiong. Dynamic-stride-net: deep convolutional neural network with dynamic stride. In SPIE Optoelectronic Imaging and Multimedia Technology, 2019.</p>
</li>
<li>
<p>V ́ıctor Campos, Brendan Jou, Xavier Giro ́-I-Nieto, Jordi Torres, and Shih Fu Chang. Skip RNN: Learning to skip state updates in recurrent neural networks. In ICLR, 2018.</p>
</li>
<li>
<p>Minjoon Seo, Sewon Min, Ali Farhadi, and Hannaneh Hajishirzi. Neural Speed Reading via Skim-RNN. In ICLR, 2018.</p>
</li>
<li>
<p>Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks. In ICLR, 2017.</p>
</li>
<li>
<p>Zhengjie Huang, Zi Ye, Shuangyin Li, and Rong Pan. Length adaptive recurrent model for text classification. In CIKM, 2017.</p>
</li>
<li>
<p>Keyi Yu, Yang Liu, Alexander G. Schwing, and Jian Peng. Fast and accurate text classification: Skimming, rereading and early stopping. In ICLR Workshop, 2018.</p>
</li>
<li>
<p>Hao Li, Hong Zhang, Xiaojuan Qi, Ruigang Yang, and Gao Huang. Improved techniques for training adaptive deep net- works. In ICCV, 2019.</p>
</li>
<li>
<p>Yoshua Bengio, Nicholas Le ́onard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.</p>
</li>
<li>
<p>Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks. In ICLR, 2017.</p>
</li>
<li>
<p>Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameter- ization with gumbel-softmax. In ICLR, 2017.</p>
</li>
<li>
<p>Andreas Veit and Serge Belongie. Convolutional networks with adaptive inference graphs. In ECCV, 2018.</p>
</li>
<li>
<p>Rahul Duggal, Scott Freitas, Sunny Dhamnani, Duen Horng, Chau, and Jimeng Sun. ELF: An Early-Exiting Framework for Long-Tailed Classification. arXiv:2006.11979 [cs, stat], 2020.</p>
</li>
<li>
<p>ClemensRosenbaum,TimKlinger,andMatthewRiemer.Routing networks: Adaptive selection of non-linear functions for multi- task learning. In ICLR, 2018.</p>
</li>
<li>
<p>Ting-Kuei Hu, Tianlong Chen, Haotao Wang, and Zhangyang Wang. Triple Wins: Boosting Accuracy, Robustness and Efﬁciency Together by Enabling Input-Adaptive Inference. In ICLR, 2020.</p>
</li>
<li>
<p>Sanghyun Hong, Yi˘gitcan Kaya, Ionut¸-Vlad Modoranu, and Tudor Dumitras¸. A panda? no, it’s a sloth: Slowdown attacks on adaptive multi-exit neural network inference. arXiv preprint arXiv:2010.02432, 2020.</p>
</li>
</ul>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
