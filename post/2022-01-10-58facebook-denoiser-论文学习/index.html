<!doctype html>
<html lang="en-us">
  <head>
    <title>58-facebook denoiser 翻译 // 亮的笔记</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.90.1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Liu Liang" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://ioyy900205.github.io/css/main.min.88e7083eff65effb7485b6e6f38d10afbec25093a6fac42d734ce9024d3defbd.css" />

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="58-facebook denoiser 翻译"/>
<meta name="twitter:description" content="Real Time Speech Enhancement in the Waveform Domain
网上找不到相关的翻译文章。 这里就做贡献了
  1. 摘要 2. 简介 3. 模型  3.1. 符号和问题设置 3.2. DEMUCS architecture 3.3. 目标   4. 实验  4.1. 实施详情 4.2. 结果 4.3. 消融实验 4.4. 实时性评估 4.5. 对ASR模型的影响   5. 相关工作 6. 结论  1. 摘要 We present a causal speech enhancement model working on the raw waveform that runs in real-time on a laptop CPU. The proposed model is based on an encoder-decoder architecture with skip-connections."/>

    <meta property="og:title" content="58-facebook denoiser 翻译" />
<meta property="og:description" content="Real Time Speech Enhancement in the Waveform Domain
网上找不到相关的翻译文章。 这里就做贡献了
  1. 摘要 2. 简介 3. 模型  3.1. 符号和问题设置 3.2. DEMUCS architecture 3.3. 目标   4. 实验  4.1. 实施详情 4.2. 结果 4.3. 消融实验 4.4. 实时性评估 4.5. 对ASR模型的影响   5. 相关工作 6. 结论  1. 摘要 We present a causal speech enhancement model working on the raw waveform that runs in real-time on a laptop CPU. The proposed model is based on an encoder-decoder architecture with skip-connections." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ioyy900205.github.io/post/2022-01-10-58facebook-denoiser-%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-01-10T14:44:57+08:00" />
<meta property="article:modified_time" content="2022-01-10T14:44:57+08:00" />



  </head>
  <body>
    <header class="app-header">
      <a href="https://ioyy900205.github.io"><img class="app-header-avatar" src="https://ss1.bdstatic.com/70cFvXSh_Q1YnxGkpoWK1HF6hhy/it/u=3520060145,2875461924&amp;fm=26&amp;gp=0.jpg" alt="Liu Liang" /></a>
      <h1>亮的笔记</h1>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/">Home</a>
             - 
          
          <a class="app-header-menu-item" href="/tags/">Tags</a>
             - 
          
          <a class="app-header-menu-item" href="/about/">About</a>
      </nav>
      <p>深度学习笔记本</p>
      <div class="app-header-social">
        
          <a href="https://github.com/ioyy900205" target="_blank" rel="noreferrer noopener">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>Github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg>
          </a>
        
          <a href="https://twitter.com/ioyy900205" target="_blank" rel="noreferrer noopener">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-twitter">
  <title>Twitter</title>
  <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path>
</svg>
          </a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">58-facebook denoiser 翻译</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Jan 10, 2022
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          15 min read
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7.01" y2="7"></line>
</svg>
              <a class="tag" href="https://ioyy900205.github.io/tags/paper_review/">Paper_Review</a>
        </div>
      </div>
    </header>
    <div class="post-content">
      <p>Real Time Speech Enhancement in the Waveform Domain</p>
<p>网上找不到相关的翻译文章。 这里就做贡献了</p>
<hr>
<ul>
<li><a href="#1-%E6%91%98%E8%A6%81">1. 摘要</a></li>
<li><a href="#2-%E7%AE%80%E4%BB%8B">2. 简介</a></li>
<li><a href="#3-%E6%A8%A1%E5%9E%8B">3. 模型</a>
<ul>
<li><a href="#31-%E7%AC%A6%E5%8F%B7%E5%92%8C%E9%97%AE%E9%A2%98%E8%AE%BE%E7%BD%AE">3.1. 符号和问题设置</a></li>
<li><a href="#32-demucs-architecture">3.2. DEMUCS architecture</a></li>
<li><a href="#33-%E7%9B%AE%E6%A0%87">3.3. 目标</a></li>
</ul>
</li>
<li><a href="#4-%E5%AE%9E%E9%AA%8C">4. 实验</a>
<ul>
<li><a href="#41-%E5%AE%9E%E6%96%BD%E8%AF%A6%E6%83%85">4.1. 实施详情</a></li>
<li><a href="#42-%E7%BB%93%E6%9E%9C">4.2. 结果</a></li>
<li><a href="#43-%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C">4.3. 消融实验</a></li>
<li><a href="#44-%E5%AE%9E%E6%97%B6%E6%80%A7%E8%AF%84%E4%BC%B0">4.4. 实时性评估</a></li>
<li><a href="#45-%E5%AF%B9asr%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BD%B1%E5%93%8D">4.5. 对ASR模型的影响</a></li>
</ul>
</li>
<li><a href="#5-%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C">5. 相关工作</a></li>
<li><a href="#6-%E7%BB%93%E8%AE%BA">6. 结论</a></li>
</ul>
<h2 id="1-摘要">1. 摘要</h2>
<p>We present a causal speech enhancement model working on the raw waveform that runs in real-time on a laptop CPU. The proposed model is based on an encoder-decoder architecture with skip-connections. It is optimized on both time and frequency domains, using multiple loss functions. Empirical evidence shows that it is capable of removing various kinds of background noise including stationary and non-stationary noises, as well as room reverb. Additionally, we suggest a set of data augmentation techniques applied directly on the raw waveform which further improve model performance and its generalization abilities. We perform evaluations on several standard benchmarks, both using objective metrics and human judgements. The proposed model matches state-of-the-art performance of both causal and non causal methods while working directly on the raw waveform.</p>
<pre><code>我们提出了一个在原始波形上工作的因果语音增强模型，它在笔记本电脑的CPU上实时运行。所提出的模型是基于一个带有跳过连接的编码器-解码器结构。它在时域和频域上都进行了优化，使用了多种损失函数。经验证明，它能够去除各种背景噪声，包括静止和非静止噪声，以及房间混响。此外，我们提出了一套直接应用于原始波形的数据增强技术，这进一步提高了模型的性能和它的泛化能力。我们对几个标准的基准进行了评估，包括使用客观指标和人类判断。所提出的模型与最先进的因果和非因果方法的性能相匹配，同时直接在原始波形上工作。
</code></pre>
<h2 id="2-简介">2. 简介</h2>
<p>Speech enhancement is the task of maximizing the perceptual quality of speech signals, in particular by removing background noise. Most recorded conversational speech signal contains some form of noise that hinders intelligibility, such as street noise, dogs barking, keyboard typing, etc. Thus, speech enhancement is a particularly important task in itself for audio and video calls [1], hearing aids [2], and can also help automatic speech recognition (ASR) systems [3]. For many such applications, a key feature of a speech enhancement system is to run in real time and with as little lag as possible (online), on the communication device, preferably on commodity hardware.</p>
<pre><code>语音增强是将语音信号的感知质量最大化的任务，特别是通过去除背景噪音。大多数录制的对话语音信号都含有某种形式的噪音，如街道噪音、狗叫声、键盘敲击声等，这些噪音会阻碍语音的理解。因此，对于音频和视频通话[1]、助听器[2]来说，语音增强本身就是一项特别重要的任务，而且还可以帮助自动语音识别（ASR）系统[3]。对于许多这样的应用，语音增强系统的一个关键特征是在通信设备上实时运行，并尽可能地减少滞后（在线），最好是在商品硬件上。
</code></pre>
<p>Decades of work in speech enhancement showed feasible solutions which estimated the noise model and used it to recover noise-deducted speech [4, 5]. Although those approaches can generalize well across domains they still have trouble dealing with commons noises such as non-stationary noise or a babble noise which is encountered when a crowd of people are simultaneously talking. The presence of such noise types degrades hearing intelligibility of human speech greatly [6]. Recently, deep neural networks (DNN) based models perform significantly better on non-stationary noise and babble noise while generating higher quality speech in objective and subjective evaluations over traditional methods [7, 8]. Additionally, deep learning based methods have also shown to be superior over traditional methods for the related task of a single-channel source separation [9, 10, 11].</p>
<pre><code>几十年来的语音增强工作显示了可行的解决方案，这些解决方案估计了噪声模型，并使用它来恢复被噪声诱导的语音[4, 5]。尽管这些方法可以很好地跨领域推广，但它们仍然难以处理常见的噪声，如非稳态噪声或一群人同时说话时遇到的咿呀噪声。这类噪声的存在大大降低了人类语音的听觉清晰度[6]。最近，基于深度神经网络（DNN）的模型在非稳态噪声和咿咿呀呀的噪声上表现得非常好，同时在客观和主观评价上比传统方法产生了更高的语音质量[7, 8]。此外，基于深度学习的方法在单声道声源分离的相关任务中也显示出优于传统方法[9, 10, 11]。
</code></pre>
<p>Inspired by these recent advances, we proposed a real-time version of the DEMUCS [11] architecture adapted for speech enhancement. It consists of a causal model, based on convolutions and LSTMs, with a frame size of 40ms, a stride of 16ms, and that runs faster than real-time on a single laptop CPU core. For audio quality purposes, our model goes from waveform to waveform, through hierarchical generation (using U-Net [12] like skip-connections). We optimize the model to directly output the “clean” version of the speech signal while minimizing a regression loss function (L1 loss), complemented with a spectrogram domain loss [13, 14]. Moreover, we proposed a set of simple and effective data augmentation techniques: namely frequency band masking and signal reverberation. Although enforcing a vital real-time constraint on model run-time, our model yields comparable performance to state of the art model by objective and subjective measures.</p>
<pre><code>受这些最新进展的启发，我们提出了一个适用于语音增强的实时版DEMUCS[11]架构。它包括一个因果模型，基于卷积和LSTMs，帧大小为40ms，步长为16ms，在单个笔记本CPU核心上的运行速度比实时快。为了保证音频质量，我们的模型从波形到波形，通过分层生成（使用U-Net[12]一样的跳过连接）。我们优化模型以直接输出语音信号的 &quot;干净 &quot;版本，同时最小化回归损失函数（L1损失），并辅以频谱域损失[13, 14]。此外，我们提出了一套简单而有效的数据增强技术：即频带掩蔽和信号混响。尽管对模型的运行时间进行了重要的实时约束，但我们的模型通过客观和主观的措施产生了与最先进的模型相当的性能。
</code></pre>
<p>Although, multiple metrics exist to measure speech enhancement systems these have shown to not correlate well with human judgements [1]. Hence, we report results for both objective metrics as well as human evaluation. Additionally we conduct an ablation study over the loss and augmentation functions to better highlight the contribution of each part. Finally, we analyzed the artifacts of the enhancement process using Word Error Rates (WERs) produced by an Automatic Speech Recognition (ASR) model.</p>
<pre><code>尽管存在多种衡量语音增强系统的指标，但这些指标与人类的判断并不相关[1]。因此，我们报告了客观指标和人类评价的结果。此外，我们对损失和增强函数进行了消减研究，以更好地突出每个部分的贡献。最后，我们使用自动语音识别（ASR）模型产生的单词错误率（WERs）分析了增强过程中的假象。
</code></pre>
<p>Results suggest that the proposed method is comparable to the current state-of-the-art model across all metrics while working directly on the raw waveform. Moreover, the enhanced samples are found to be beneficial for improving an ASR model under noisy conditions.</p>
<pre><code>结果表明，所提出的方法在所有指标上与目前最先进的模型相当，同时直接在原始波形上工作。此外，增强的样本被发现有利于改善噪声条件下的ASR模型。
</code></pre>
<h2 id="3-模型">3. 模型</h2>
<h3 id="31-符号和问题设置">3.1. 符号和问题设置</h3>
<p>We focus on monaural (single-microphone) speech enhancement that can operate in real-time applications. Specifically, given an audio signal x ∈ RT, composed of a clean speech y ∈ RT that is corrupted by an additive background signal n ∈ RT so that x = y + n. The length, T, is not a fixed value across samples, since the input utterances can have different durations. Our goal is to find an enhancement function f such that f(x) ≈ y.</p>
<pre><code>我们专注于单声道（单麦克风）语音增强，可以在实时应用中操作。具体来说，给定一个音频信号x∈RT，由一个干净的语音y∈RT组成，该语音被一个加性背景信号n∈RT所破坏，因此x=y+n。长度T在各样本中不是一个固定值，因为输入语料可以有不同的持续时间。我们的目标是找到一个增强函数f，使f（x）≈y。
</code></pre>
<p>In this study we set f to be the DEMUCS architecture [11], which was initially developed for music source separation, and adapt it to the task of causal speech enhancement, a visual description of the model can be seen in Figure 1a.</p>
<pre><code>在这项研究中，我们将f设定为DEMUCS架构[11]，该架构最初是为音乐源分离而开发的，并将其调整为因果语音增强的任务，图1a中可以看到该模型的视觉描述。
</code></pre>
<h3 id="32-demucs-architecture">3.2. DEMUCS architecture</h3>
<p>DEMUCS consists in a multi-layer convolutional encoder and decoder with U-net [12] skip connections, and a sequence modeling network applied on the encoders’ output. It is characterized by its number of layers L, initial number of hidden channels H, layer kernel size K and stride S and resampling factor U. The encoder and decoder layers are numbered from 1 to L (in reverse order for the decoder, so layers at the same scale have the same index). As we focus on monophonic speech enhancement, the input and output of the model has a single channel only.</p>
<pre><code>DEMUCS由一个带有U-net[12]跳过连接的多层卷积编码器和解码器组成，以及一个应用于编码器输出的序列建模网络。它的特点是层数L、隐藏通道的初始数量H、层核大小K和跨度S以及重采样系数U。编码器和解码器的层数从1到L（对于解码器来说，顺序相反，所以同一规模的层有相同的索引）。由于我们专注于单声道语音增强，该模型的输入和输出只有一个通道。
</code></pre>
<p>Formally, the encoder network E gets as input the raw wave form and outputs a latent representation E(x) = z. Each layer consists in a convolution layer with a kernel size of K and stride
of S with 2 i−1H output channels, followed by a ReLU activation, a “1x1” convolution with 2 iH output channels and finally a GLU [15] activation that converts back the number of channels to 2
i−1H, see Figure 1b for a visual description.</p>
<pre><code>从形式上看，编码器网络E获得原始波形作为输入，并输出潜伏表示E（x）=z。每层包括一个卷积层，核大小为K，步长为S，有2个i-1H的输出通道。S的卷积层，有2个i-1H的输出通道，然后是ReLU激活，2个iH输出通道的 &quot;1x1 &quot;卷积，最后是GLU[15]激活，将通道数转换为2
</code></pre>
<p>i-1H，直观描述见图1b。</p>
<p>Next, a sequence modeling R network takes the latent representation z as input and outputs a non-linear transformation of the same size, R(z) = LSTM(z) + z, denoted as zˆ. The LSTM network consists of 2-layers and 2L−1H hidden units. For causal prediction, we use an unidirectional LSTM, while for non causal models, we use a bidirectional LSTM, followed by a linear layer to merge the both outputs.</p>
<pre><code>接下来，一个序列建模R网络将潜伏表征z作为输入，并输出相同大小的非线性变换，R（z）=LSTM（z）+z，表示为zˆ。LSTM网络由2层和2L-1H个隐藏单元组成。对于因果预测，我们使用单向LSTM，而对于非因果模型，我们使用双向LSTM，然后用一个线性层来合并两个输出。
</code></pre>
<p>Lastly, a decoder network D, takes as input zˆ and outputs an estimation of clean signal D(zˆ) = yˆ. The i-th layer of the decoder takes as input 2i−1H channels, and applies a 1x1 convolution with 2iH channels, followed by a GLU activation function that outputs 2i−1H channels and finally a transposed convolution with a kernel size of 8, stride of 4, and 2i−2H output channels accompanied by a ReLU function. For the last layer the output is a single channel and has no ReLU. A skip connection connects the output of the i-th layer of the encoder and the input of the i-th layer of the decoder, see Figure 1a.</p>
<pre><code>最后，一个解码器网络D，将zˆ作为输入，输出清洁信号的估计值D(zˆ)=yˆ。解码器的第i层将2i-1H通道作为输入，并应用1x1卷积的2iH通道，然后是GLU激活函数，输出2i-1H通道，最后是转置卷积，核大小为8，步长为4，输出2i-2H通道，并伴随ReLU函数。最后一层的输出是一个单通道，没有ReLU。一个跳过连接连接了编码器第i层的输出和解码器第i层的输入，见图1a。
</code></pre>
<p>We initialize all model parameters using the scheme proposed by [16]. Finally, we noticed that upsampling the audio by a factor U before feeding it to the encoder improves accuracy. We downsample the output of the model by the same amount. The resampling is done using a sinc interpolation filter [17], as part of the end-to-end training, rather than a pre-processing step.</p>
<pre><code>我们使用[16]提出的方案来初始化所有模型参数。最后，我们注意到，在将音频送入编码器之前，对其进行U系数的上采样可以提高精确度。我们对模型的输出进行同样数量的下采样。重采样是使用sinc插值滤波器[17]完成的，作为端到端训练的一部分，而不是一个预处理步骤。
</code></pre>
<h3 id="33-目标">3.3. 目标</h3>
<p>We use the L1 loss over the waveform together with a multiresolution STFT loss over the spectrogram magnitudes similarly to the one proposed in [13, 14]. Formally, given y and yˆ be the clean signal and the enhanced signal respectively. We define the STFT loss to be the sum of the spectral convergence
(sc) loss and the magnitude loss as follows,</p>
<pre><code>我们在波形上使用L1损失，同时在频谱大小上使用多分辨率STFT损失，这与[13, 14]中提出的方法类似。形式上，给定y和yˆ分别为清洁信号和增强信号。我们将STFT损失定义为频谱收敛的总和(sc)损失和幅度损失之和，如下所示。
</code></pre>
<h2 id="4-实验">4. 实验</h2>
<p>We performed several experiments to evaluate the proposed method against several highly competitive models. We report objective and subjective measures on the Valentini et al. [18] and Deep Noise Suppression (DNS) [19] benchmarks. Moreover, we run an ablation study over the augmentation and loss functions. Finally, we assessed the usability of the enhanced samples to improve ASR performance under noisy conditions. Code and samples can be found in the following link: <a href="https://github.com/facebookresearch/denoiser">https://github.com/facebookresearch/denoiser</a>.</p>
<pre><code>我们进行了几个实验来评估所提出的方法与几个高度竞争的模型。我们报告了Valentini等人[18]和深度噪声抑制（DNS）[19]基准的客观和主观测量。此外，我们对增量和损失函数进行了消融研究。最后，我们评估了增强样本的可用性，以提高噪声条件下的ASR性能。代码和样本可以在以下链接中找到：https://github.com/facebookresearch/denoiser。
</code></pre>
<h3 id="41-实施详情">4.1. 实施详情</h3>
<p>Evaluation Methods We evaluate the quality of the enhanced speech using both objective and subjective measures. For the objective measures we use: (i) PESQ: Perceptual evaluation of speech quality, using the wide-band version recommended in ITU-T P.862.2 [24] (from 0.5 to 4.5) (ii) Short-Time Objective Intelligibility (STOI) [25] (from 0 to 100) (iii) CSIG: Mean opinion score (MOS) prediction of the signal distortion attending only to the speech signal [26] (from 1 to 5). (iv) CBAK: MOS prediction of the intrusiveness of background noise [26] (from 1 to 5). (v) COVL: MOS prediction of the overall effect [26] (from 1 to 5).</p>
<pre><code>评估方法 我们使用客观和主观的措施来评估增强后的语音质量。对于客观措施，我们使用 (i) PESQ：语音质量的感知评估，使用ITU-T P.862.2[24]中推荐的宽频版本（从0.5到4.5） (ii) 短时间客观可懂度（STOI）[25]（从0到100） (iii) CSIG：对仅涉及语音信号的信号失真进行平均意见得分（MOS）预测[26]（从1到5）。(iv) CBAK：对背景噪音的干扰性的MOS预测[26]（从1到5）。(v) COVL：整体效果的MOS预测[26]（从1到5）。
</code></pre>
<p>For the subjective measure, we conducted a MOS study as recommended in ITU-T P.835 [27]. For that, we launched a crowd source evaluation using the CrowdMOS package [28]. We randomly sample 100 utterances and each one was scored by 15 different raters along three axis: level of distortion, intrusiveness of background noise, and overall quality. Averaging results across all annotators and queries gives the final scores.</p>
<pre><code>对于主观测量，我们按照ITU-T P.835[27]中的建议进行了MOS研究。为此，我们使用CrowdMOS软件包[28]进行了众源评估。我们随机抽取100个语料，每个语料由15个不同的评分者沿着三个轴线进行评分：失真程度、背景噪音的干扰性和整体质量。对所有评分者和查询的结果进行平均，得出最终分数。
</code></pre>
<p>Training We train the DEMUCS model for 400 epochs on the Valentini [18] dataset and 250 on the DNS [19] dataset. We use the L1 loss between the predicted and ground truth clean speech waveforms, and for the Valentini dataset, also add the STFT loss described in Section 2.3 with a weight of 0.5. We use the Adam optimizer with a step size of 3e−4, a momentum of β1 = 0.9 and a denominator momentum β2 = 0.999. For the Valentini dataset, we use the original validation set and keep the best model, for the DNS dataset we train without a validation set and keep the last model. The audio is sampled at 16 kHz.</p>
<pre><code>训练 我们在Valentini[18]数据集上训练DEMUCS模型400次，在DNS[19]数据集上训练250次。我们使用预测的和地面真实的干净语音波形之间的L1损失，对于Valentini数据集，还添加了第2.3节中描述的STFT损失，权重为0.5。我们使用步长为3e-4的Adam优化器，动量为β1 = 0.9，分母动量为β2 = 0.999。对于Valentini数据集，我们使用原始验证集并保留最佳模型，对于DNS数据集，我们在没有验证集的情况下进行训练并保留最后一个模型。音频的采样频率是16kHz。
</code></pre>
<p>Model We use three variants of the DEMUCS architecture described in Section 2. For the non causal DEMUCS , we take U=2, S=2, K=8, L=5 and H=64. For the causal DEMUCS , we take U=4, S=4, K=8 and L=5, and either H=48, or H=64. We normalize the input by its standard deviation before feeding it to the model and scale back the output by the same factor. For the evaluation of causal models, we use an online estimate of the standard deviation. With this setup, the causal DEMUCS processes audio has a frame size of 37 ms and a stride of 16 ms.</p>
<pre><code>模型 我们使用第二节中描述的DEMUCS架构的三种变体。对于非因果DEMUCS，我们采用U=2, S=2, K=8, L=5和H=64。对于因果DEMUCS，我们采用U=4，S=4，K=8和L=5，以及H=48或H=64。在将输入输入到模型之前，我们用其标准差对输入进行归一化处理，并以相同的系数对输出进行缩减。对于因果模型的评估，我们使用标准差的在线估计。在这种设置下，因果DEMUCS处理音频的帧大小为37ms，步长为16ms。
</code></pre>
<p>Data augmentation We always apply a random shift between 0 and S seconds. The Remix augmentation shuffles the noises within one batch to form new noisy mixtures. BandMask is a band-stop filter with a stop band between f0 and f1, sampled to remove 20% of the frequencies uniformly in the mel scale. This is equivalent, in the waveform domain, to the SpecAug augmentation [29] used for ASR training. Revecho: given an initial gain λ, early delay τ and RT60, it adds to the noisy signal a series of N decaying echos of the clean speech and noise. The n-th echo has a delay of nτ + jitter and a gain of ρnλ. N and ρ are chosen so that when the total delay reaches RT60, we have ρN ≤ 1e−3. λ, τ and RT60 are sampled uniformly respectively over [0,0.3], [10,30] ms, [0.3,1.3] sec.</p>
<pre><code>数据增强 我们总是在0到S秒之间应用随机移位。混淆增强对一批中的噪声进行洗牌，形成新的噪声混合物。BandMask是一个带阻滤波器，在f0和f1之间有一个阻带，采样后在mel scale中均匀地去除20%的频率。在波形域中，这相当于用于ASR训练的SpecAug增强法[29]。回声：给定一个初始增益λ、早期延迟τ和RT60，它向噪声信号添加一系列干净语音和噪声的N个衰减的回声。第n个回声的延迟为nτ+抖动，增益为ρnλ。λ、τ和RT60分别在[0,0.3]、[10,30]ms、[0.3,1.3]秒内均匀采样。
</code></pre>
<p>We use the random shift for all datasets, Remix and Banmask for Valentini [18], and Revecho only for DNS [19].</p>
<pre><code>我们对所有的数据集使用随机移位，对Valentini[18]使用Remix和Banmask，而对DNS[19]只使用Revecho。
</code></pre>
<p>Causal streaming evaluation In order to test our causal model in real conditions we use a specific streaming implementation at test time. Instead of normalizing by the standard deviation of the audio, we use the standard deviation up to the current position (i.e. we use the cumulative standard deviation)</p>
<pre><code>因果流评估 为了在真实条件下测试我们的因果模型，我们在测试时使用了一个特定的流媒体实现。我们不使用音频的标准偏差来归一化，而是使用到当前位置的标准偏差（即我们使用累积标准偏差）。
</code></pre>
<p>We keep a small buffer of past input/output to limit the side effect of the sinc resampling filters. For the input upsampling, we also use a 3ms lookahead, which takes the total frame size of the model to 40 ms. When applying the model to a given frame of the signal, the rightmost part of the output is invalid, because future audio is required to compute properly the output of the transposed convolutions. Nonetheless, we noticed that using this invalid part as a padding for the streaming downsampling greatly improves the PESQ. The streaming implementation is pure PyTorch. Due to the overlap between frames, care was taken to cache the output of the different layers as needed.</p>
<pre><code>我们保留了一个过去输入/输出的小缓冲区，以限制sinc重采样滤波器的副作用。对于输入重采样，我们还使用了3ms的超前量，这使得模型的总帧数达到40ms。当把模型应用于信号的某一帧时，输出的最右边部分是无效的，因为需要未来的音频来正确计算转置卷积的输出。尽管如此，我们注意到，使用这个无效的部分作为流式降采样的填充，大大改善了PESQ。流媒体的实现是纯粹的PyTorch。由于帧之间的重叠，我们注意根据需要对不同层的输出进行缓存。
</code></pre>
<h3 id="42-结果">4.2. 结果</h3>
<p>Table 1 summarizes the results for Valentini [18] dataset using both causal and non-causal models. Results suggest that DEMUCS matched current SOTA model (DeepMMSE [23]) using both objective and subjective measures, while working directly on the raw waveform and without using extra training data. Additionally, DEMUCS is superior to the other baselines methods, (may they be causal or non-causal), by a significant margin. We also introduce a dry/wet knob, i.e. we output dry · x + (1 − dry) · yˆ, which allows to control the trade-off between noise removal and conservation of the signal. We notice that a small amount of bleeding (5%) improves the overall perceived quality.</p>
<pre><code>表1总结了Valentini[18]数据集使用因果和非因果模型的结果。结果表明，DEMUCS与目前的SOTA模型（DeepMMSE[23]）在客观和主观方面都相匹配，同时直接在原始波形上工作，没有使用额外的训练数据。此外，DEMUCS比其他基线方法（可能是因果关系或非因果关系）要好得多。我们还引入了一个干/湿旋钮，即我们输出干-x+（1-干）-yˆ，这允许控制去除噪声和保存信号之间的权衡。我们注意到，少量的出血（5%）就能提高整体的感知质量。
</code></pre>
<p>We present on Table 2 the overall MOS evaluations on the 3 categories of the DNS [19] blind test set: no reverb (synthetic mixture without reverb), reverb (synthetic mixture, with artificial reverb) and real recordings. We test different strategies for Table 3: Ablation study for the causal DEMUCS model with H=64,S=4,U=4 using the Valentini benchmark [18].</p>
<pre><code>我们在表2中列出了对DNS[19]盲测集的3个类别的总体MOS评价：无混响（没有混响的合成混合物）、混响（合成混合物，有人工混响）和真实录音。我们测试了不同的策略，表3：使用Valentini基准[18]对H=64,S=4,U=4的因果DEMUCS模型进行消融研究。
</code></pre>
<p>the reverb-like Revecho augmentation described in Section 3.1. We either ask the model to remove it (dereverberation), keep it, or keep only part of it. Finally, we either add the same reverb to the speech and noise or use different jitters to simulate having two distinct sources. We entered the challenge with the “remove reverb” model, with poor performance on the Reverb category due to dereverberation artifacts . Doing partial dereverberation improves the overall rating, but not for real recordings (which have typically less reverb already). On real recordings, simulating reverb with two sources improves the ratings.</p>
<pre><code>3.1节中描述的类似混响的Revecho增强。我们或者要求模型删除它（去混响），保留它，或者只保留它的一部分。最后，我们要么给语音和噪声添加相同的混响，要么使用不同的抖动来模拟有两个不同的来源。我们以 &quot;去除混响 &quot;的模式进入挑战赛，由于去混响的假象，我们在混响类别上表现不佳。做部分去混响可以提高总体评分，但对于真实录音（通常已经有了较少的混响）来说就不行了。在真实录音中，用两个来源模拟混响可以提高评分。
</code></pre>
<h3 id="43-消融实验">4.3. 消融实验</h3>
<p>In order to better understand the influence of different components in the proposed model on the overall performance, we conducted an ablation study over the augmentation functions and loss functions. We use the causal DEMUCS version and report PESQ and STOI for each of the methods. Results are presented in Table 3. Results suggest that each of the components contribute to overall performance, with the STFT loss and time shift augmentation producing the biggest increase in performance. Notice, surprisingly the remix augmentation function has a minor contribution to the overall performance.</p>
<pre><code>为了更好地理解所提出的模型中的不同组成部分对整体性能的影响，我们对增量函数和损失函数进行了消融研究。我们使用因果DEMUCS版本，并报告每种方法的PESQ和STOI。结果列于表3。结果表明，每个组件都对整体性能有所贡献，STFT损失和时移增强对性能的提高最大。注意，令人惊讶的是，混音增强功能对整体性能的贡献很小。
</code></pre>
<h3 id="44-实时性评估">4.4. 实时性评估</h3>
<p>We computed the Real-Time Factor (RTF, e.g. time to enhance a frame divided by the stride) under the streaming setting to better match real-world conditions. We benchmark this implementation on a quad-core Intel i5 CPU (2.0 GHz, up to AVX2 instruction set). The RTF is 1.05 for the H=64 version, while for the H=48 the RTF is 0.6. When restricting execution to a single core, the H=48 model still achieves a RTF of 0.8, making it realistic to use in real conditions, for instance along a video call software. We do not provide RTF results for DeepMMSE [23] since no streaming implementation was provided by the authors, thus making it an unfair comparison.</p>
<pre><code>我们在流媒体设置下计算了实时因素（RTF，例如，增强一帧的时间除以步长），以更好地匹配真实世界的条件。我们在四核英特尔i5 CPU（2.0 GHz，最高AVX2指令集）上对这个实现进行了基准测试。H=64版本的RTF为1.05，而H=48版本的RTF为0.6。当把执行限制在一个单核时，H=48模型仍然达到了0.8的RTF，这使得它在实际条件下的使用是现实的，例如沿着视频通话软件。我们没有提供DeepMMSE[23]的RTF结果，因为作者没有提供流式实现，因此使其成为一个不公平的比较。
</code></pre>
<h3 id="45-对asr模型的影响">4.5. 对ASR模型的影响</h3>
<p>Lastly, we evaluated the usability of the enhanced samples to improve ASR performance under noisy conditions. To that end, we synthetically generated noisy data using the LIBRISPEECH dataset [31] together with noises from the test set of the DNS [19] benchmark. We created noisy samples in a controlled setting where we mixed the clean and noise files with SNR levels ∈ {0,10,20,30}. For the ASR model we used a Convolutions and Transformer based acoustic model which get states-of-the-art results on LIBRISPEECH, as described in [32]. To get Word Error Rates (WERs) we follow a simple Viterbi (argmax) decoding with neither language model decoding nor beam-search. That way, we can better understand the impact of Table 4: ASR results with a state-of-the-art acoustic model, Word Error Rates without decoding, no language model. Results on the LIBRISPEECHvalidation sets with added noise from the test set of DNS, and enhanced by DEMUCS .</p>
<pre><code>最后，我们评估了增强样本的可用性，以提高噪声条件下的ASR性能。为此，我们使用LIBRISPEECH数据集[31]和来自DNS[19]基准测试集的噪声来合成噪声数据。我们在一个受控的环境中创建了噪声样本，我们将干净的文件和噪声文件以SNR水平∈{0,10,20,30}混合。对于ASR模型，我们使用了一个基于Convolutions和Transformer的声学模型，该模型在LIBRISPEECH上获得了最先进的结果，如[32]中所述。为了得到单词错误率（WERs），我们采用了简单的Viterbi（argmax）解码，既没有语言模型解码，也没有波束搜索。这样，我们就能更好地理解表4的影响：使用最先进的声学模型的ASR结果，没有解码的词错误率，没有语言模型。在LIBRISPEECH验证集上的结果，加入了来自DNS测试集的噪声，并由DEMUCS增强。
</code></pre>
<h2 id="5-相关工作">5. 相关工作</h2>
<p>Traditionally speech enhancement methods generate either an enhanced version of the magnitude spectrum or produce an estimate of the ideal binary mask (IBM) that is then used to enhance the magnitude spectrum [5, 33].</p>
<pre><code>传统的语音增强方法要么产生一个增强版的幅度谱，要么产生一个理想的二进制掩码（IBM）的估计，然后用来增强幅度谱[5, 33]。
</code></pre>
<p>Over the last years, there has been a growing interest towards DNN based methods for speech enhancement [34, 35, 36, 37, 20, 38, 39, 22, 40, 7, 41, 8, 42, 21, 37]. In [34] a deep feed-forward neural network was used to generate a frequencydomain binary mask using a cost function in the waveform domain. Authors in [43] suggested to use a multi-objective loss function to further improve speech quality. Alternatively authors in [44, 35] use a recursive neural network (RNN) for speech enhancement. In [7] the authors proposed an end-to-end method , namely Speech Enhancement Generative Adversarial Networks (SEGAN) to perform enhancement directly from the raw waveform. The authors in [41, 8, 42, 21] further improve such optimization. In [37] the authors suggest to use a WaveNet [45] model to perform speech denoising by learning a function to map noisy to clean signals.
While considering causal methods, the authors in [46] propose a convolutional recurrent network at the spectral level for real-time speech enhancement, while Xia, Yangyang, et al. [30] suggest to remove the convolutional layers and apply a weighted loss function to further improve results in the real-time setup.</p>
<pre><code>在过去的几年里，人们对基于DNN的语音增强方法越来越感兴趣[34, 35, 36, 37, 20, 38, 39, 22, 40, 7, 41, 8, 42, 21, 37] 。在[34]中，一个深度前馈神经网络被用来使用波形域的成本函数来生成频域二进制掩码。作者在[43]中建议使用多目标损失函数来进一步提高语音质量。另外，[44，35]中的作者使用递归神经网络（RNN）进行语音增强。在[7]中，作者提出了一种端到端的方法，即语音增强生成对抗网络（SEGAN），直接从原始波形中执行增强。作者在[41, 8, 42, 21]中进一步改进了这种优化。在[37]中，作者建议使用WaveNet[45]模型，通过学习一个函数将噪声信号映射到清洁信号，来进行语音去噪。
</code></pre>
<p>Recently, the authors in [23] provide impressive results for both causal and non-causal models using a minimum mean-square error noise power spectral density tracker, which employs a temporal convolutional network (TCN) a priori SNR estimator.</p>
<pre><code>在考虑因果方法的同时，[46]中的作者提出了一个在频谱水平上的卷积递归网络来进行实时语音增强，而Xia, Yangyang, 等人[30]建议去除卷积层并应用加权损失函数来进一步改善实时设置的结果。最近，[23]中的作者使用最小均方误差噪声功率谱密度跟踪器为因果和非因果模型提供了令人印象深刻的结果，该跟踪器采用了时间卷积网络（TCN）先验SNR估计器。
</code></pre>
<h2 id="6-结论">6. 结论</h2>
<p>We have showed how DEMUCS , a state-of-the-art architecture developed for music source separation in the waveform domain, could be turned into a causal speech enhancer, processing audio in real time on consumer level CPU. We tested DEMUCS on the standard Valentini benchmark and achieved state-of-the-art result without using extra training data. We also test our model in real reverberant conditions with the DNS dataset. We empirically demonstrated how augmentation techniques (reverb with two sources, partial dereverberation) can produce a significant improvement in subjective evaluations. Finally, we showed that our model can improve the performance of an ASR model in noisy conditions even without retraining of the model.</p>
<pre><code>我们展示了DEMUCS，一个为波形域中的音乐源分离而开发的最先进的架构，可以变成一个因果语音增强器，在消费者级别的CPU上实时处理音频。我们在标准的Valentini基准上测试了DEMUCS，在不使用额外训练数据的情况下取得了最先进的结果。我们还用DNS数据集在真实的混响条件下测试了我们的模型。我们根据经验证明了增强技术（双源混响，部分去混响）如何在主观评价中产生显著的改善。最后，我们表明，我们的模型可以改善ASR模型在噪声条件下的性能，即使不对模型进行再训练。
</code></pre>
<hr>
<p><strong>参考资料</strong></p>
<p><a href="https://arxiv.org/abs/2006.12847">https://arxiv.org/abs/2006.12847</a></p>
<p><a href="https://github.com/facebookresearch/denoiser">https://github.com/facebookresearch/denoiser</a></p>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
