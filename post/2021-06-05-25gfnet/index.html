<!doctype html>
<html lang="en-us">
  <head>
    <title>25——GFNet论文阅读 // 亮的笔记</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.82.1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Liu Liang" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://ioyy900205.github.io/css/main.min.88e7083eff65effb7485b6e6f38d10afbec25093a6fac42d734ce9024d3defbd.css" />

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="25——GFNet论文阅读"/>
<meta name="twitter:description" content="NeurIPS 2020录用的一篇论文：《Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classiﬁcation
论文链接：https://arxiv.org/pdf/2010.05300.pdf
代码和预训练模型链接：https://github.com/blackfeather-wang/GFNet-Pytorch
论文第一作者：
王语霖，清华大学自动化系直博二年级，导师为吴澄院士和黄高助理教授，研究兴趣为深度学习与计算机视觉，在NeurIPS 2019/2020以第一作者发表两篇学术论文。
 @TOC
1. 研究动机及简介 推理CNN所需的计算量（FLOPs）基本与像素数目成正比，即与图形的长、宽成二次关系。
在实际应用（例如手机APP、自动驾驶系统、图片搜索引擎）中，计算量往往正比于能耗或者时间开销，显然，无论出于成本因素还是从安全性和用户体验的角度考虑，网络的计算开销都应当尽可能小。
这便是本文所提出方法的出发点，我们的目标是，对于输入图片，自适应地找到其与任务最相关的区域，进而通过使神经网络只处理这些区域，以尽可能小的计算量得到可信的结果。具体而言，我们采用的方法是，将一张分辨率较高的图片表征为若干个包含其关键部分的“小块”（Patch），而后仅将这些小块输入神经网络。以下面的示意图为例，将一张224x224的图片分解为3个96x96的Patch进行处理所需的计算量仅为原图的55.2%。
2. Method 为了实现上述目的，事实上，有两个显然的困难：
(a) 任意给定一张输入图片，如何判断其与任务最相关的区域在哪里呢？
(b) 考虑到我们的最终目的是使神经网络得到正确的预测结果，不同输入所需的计算量是不一样的，例如对于下面所示的两个输入图片，神经网络可能仅需要处理一个patch就能识别出特征非常突出的月亮，但是需要处理更多的patch才能分辨出猫咪的具体品种。
为了解决这两个问题，我们设计了一个Glance and Focus的框架，将这一思路建模为了一个序列决策过程，如下图所示。
其具体执行流程为：
 首先，对于一张任意给定的输入图片，由于我们没有任何关于它的先验知识，我们直接将其放缩为一个patch的大小，输入网络，这一方面产生了一个初步的判断结果，另一方面也提供了原始输入图片的空间分布信息；这一阶段称为扫视（Glance）。 而后，我们再以这些基本的空间分布信息为基础，逐步从原图上取得高分辨率的patch，将其不断输入网络，以此逐步更新预测结果和空间分布信息，得到更为准确的判断，并逐步寻找神经网络尚未见到过的关键区域；这一阶段称为关注（Focus）。  值得注意的是，在上述序列过程的每一步结束之后，我们会将神经网络的预测自信度（confidence）与一个预先定义的阈值进行比较，一旦confidence超过阈值，我们便视为网络已经得到了可信的结果，这一过程立即终止。此机制称为自适应推理（Adaptive Inference）。通过这种机制，我们一方面可以使不同难易度的样本具有不同的序列长度，从而动态分配计算量、提高整体效率；另一方面可以简单地通过改变阈值调整网络的整体计算开销，而不需要重新训练网络，这使得我们的模型可以动态地以最小的计算开销达到所需的性能，或者实时最大化地利用所有可用的计算资源以提升模型表现。
3. 显而易见的困难 how to identify class-ciscriminative regions? 如何识别阶级歧视性区域？
how to determine the number of class-discriminative regions? 如何确定类区分区域的数量？
4. 网络结构 GFNet共有四个组件，分别为：
全局编码器${f_g}$和局部编码器${f_l}$ （Global Encoder and Local Encoder）为两个CNN，分别用于从放缩后的原图和局部patch中提取信息，之所以用两个CNN，是因为我们发现一个CNN很难同时适应缩略图和局部patch两种尺度（scale）的输入。几乎所有现有的网络结构均可以作为这两个编码器以提升其推理效率（如MobileNet-V3、EfficientNet、RegNet等）。 分类器 [公式] （Classifier）为一个循环神经网络（RNN），输入为全局池化后的特征向量，用于整合过去所有输入的信息，以得到目前最优的分类结果。 图像块选择网络 [公式] （Patch Proposal Network）是另一个循环神经网络（RNN），输入为全局池化前的特征图（不做池化是为了避免损失空间信息），用于整合目前为止所有的空间分布信息，并决定下一个patch的位置。值得注意的是由于取得patch的crop操作不可求导，[公式]是使用强化学习中的策略梯度方法（policy gradient）训练的。"/>

    <meta property="og:title" content="25——GFNet论文阅读" />
<meta property="og:description" content="NeurIPS 2020录用的一篇论文：《Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classiﬁcation
论文链接：https://arxiv.org/pdf/2010.05300.pdf
代码和预训练模型链接：https://github.com/blackfeather-wang/GFNet-Pytorch
论文第一作者：
王语霖，清华大学自动化系直博二年级，导师为吴澄院士和黄高助理教授，研究兴趣为深度学习与计算机视觉，在NeurIPS 2019/2020以第一作者发表两篇学术论文。
 @TOC
1. 研究动机及简介 推理CNN所需的计算量（FLOPs）基本与像素数目成正比，即与图形的长、宽成二次关系。
在实际应用（例如手机APP、自动驾驶系统、图片搜索引擎）中，计算量往往正比于能耗或者时间开销，显然，无论出于成本因素还是从安全性和用户体验的角度考虑，网络的计算开销都应当尽可能小。
这便是本文所提出方法的出发点，我们的目标是，对于输入图片，自适应地找到其与任务最相关的区域，进而通过使神经网络只处理这些区域，以尽可能小的计算量得到可信的结果。具体而言，我们采用的方法是，将一张分辨率较高的图片表征为若干个包含其关键部分的“小块”（Patch），而后仅将这些小块输入神经网络。以下面的示意图为例，将一张224x224的图片分解为3个96x96的Patch进行处理所需的计算量仅为原图的55.2%。
2. Method 为了实现上述目的，事实上，有两个显然的困难：
(a) 任意给定一张输入图片，如何判断其与任务最相关的区域在哪里呢？
(b) 考虑到我们的最终目的是使神经网络得到正确的预测结果，不同输入所需的计算量是不一样的，例如对于下面所示的两个输入图片，神经网络可能仅需要处理一个patch就能识别出特征非常突出的月亮，但是需要处理更多的patch才能分辨出猫咪的具体品种。
为了解决这两个问题，我们设计了一个Glance and Focus的框架，将这一思路建模为了一个序列决策过程，如下图所示。
其具体执行流程为：
 首先，对于一张任意给定的输入图片，由于我们没有任何关于它的先验知识，我们直接将其放缩为一个patch的大小，输入网络，这一方面产生了一个初步的判断结果，另一方面也提供了原始输入图片的空间分布信息；这一阶段称为扫视（Glance）。 而后，我们再以这些基本的空间分布信息为基础，逐步从原图上取得高分辨率的patch，将其不断输入网络，以此逐步更新预测结果和空间分布信息，得到更为准确的判断，并逐步寻找神经网络尚未见到过的关键区域；这一阶段称为关注（Focus）。  值得注意的是，在上述序列过程的每一步结束之后，我们会将神经网络的预测自信度（confidence）与一个预先定义的阈值进行比较，一旦confidence超过阈值，我们便视为网络已经得到了可信的结果，这一过程立即终止。此机制称为自适应推理（Adaptive Inference）。通过这种机制，我们一方面可以使不同难易度的样本具有不同的序列长度，从而动态分配计算量、提高整体效率；另一方面可以简单地通过改变阈值调整网络的整体计算开销，而不需要重新训练网络，这使得我们的模型可以动态地以最小的计算开销达到所需的性能，或者实时最大化地利用所有可用的计算资源以提升模型表现。
3. 显而易见的困难 how to identify class-ciscriminative regions? 如何识别阶级歧视性区域？
how to determine the number of class-discriminative regions? 如何确定类区分区域的数量？
4. 网络结构 GFNet共有四个组件，分别为：
全局编码器${f_g}$和局部编码器${f_l}$ （Global Encoder and Local Encoder）为两个CNN，分别用于从放缩后的原图和局部patch中提取信息，之所以用两个CNN，是因为我们发现一个CNN很难同时适应缩略图和局部patch两种尺度（scale）的输入。几乎所有现有的网络结构均可以作为这两个编码器以提升其推理效率（如MobileNet-V3、EfficientNet、RegNet等）。 分类器 [公式] （Classifier）为一个循环神经网络（RNN），输入为全局池化后的特征向量，用于整合过去所有输入的信息，以得到目前最优的分类结果。 图像块选择网络 [公式] （Patch Proposal Network）是另一个循环神经网络（RNN），输入为全局池化前的特征图（不做池化是为了避免损失空间信息），用于整合目前为止所有的空间分布信息，并决定下一个patch的位置。值得注意的是由于取得patch的crop操作不可求导，[公式]是使用强化学习中的策略梯度方法（policy gradient）训练的。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ioyy900205.github.io/post/2021-06-05-25gfnet/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-06-05T10:54:57&#43;08:00" />
<meta property="article:modified_time" content="2021-06-05T10:54:57&#43;08:00" />



  </head>
  <body>
    <header class="app-header">
      <a href="https://ioyy900205.github.io"><img class="app-header-avatar" src="https://ss1.bdstatic.com/70cFvXSh_Q1YnxGkpoWK1HF6hhy/it/u=3520060145,2875461924&amp;fm=26&amp;gp=0.jpg" alt="Liu Liang" /></a>
      <h1>亮的笔记</h1>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/">Home</a>
             - 
          
          <a class="app-header-menu-item" href="/tags/">Tags</a>
             - 
          
          <a class="app-header-menu-item" href="/about/">About</a>
      </nav>
      <p>深度学习笔记本</p>
      <div class="app-header-social">
        
          <a href="https://github.com/ioyy900205" target="_blank" rel="noreferrer noopener">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>Github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg>
          </a>
        
          <a href="https://twitter.com/ioyy900205" target="_blank" rel="noreferrer noopener">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-twitter">
  <title>Twitter</title>
  <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path>
</svg>
          </a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">25——GFNet论文阅读</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Jun 5, 2021
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          1 min read
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7.01" y2="7"></line>
</svg>
              <a class="tag" href="https://ioyy900205.github.io/tags/paper_review/">Paper_Review</a>
        </div>
      </div>
    </header>
    <div class="post-content">
      <p>NeurIPS 2020录用的一篇论文：《Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classiﬁcation</p>
<p>论文链接：https://arxiv.org/pdf/2010.05300.pdf</p>
<p>代码和预训练模型链接：https://github.com/blackfeather-wang/GFNet-Pytorch</p>
<p>论文第一作者：</p>
<p>王语霖，清华大学自动化系直博二年级，导师为吴澄院士和黄高助理教授，研究兴趣为深度学习与计算机视觉，在NeurIPS 2019/2020以第一作者发表两篇学术论文。</p>
<hr>
<p>@<a href="%E6%9C%AC%E6%96%87%E5%86%85%E5%AE%B9">TOC</a></p>
<h2 id="1-研究动机及简介">1. 研究动机及简介</h2>
<p>推理CNN所需的计算量（FLOPs）基本与像素数目成正比，即与图形的长、宽成二次关系。</p>
<p>在实际应用（例如手机APP、自动驾驶系统、图片搜索引擎）中，计算量往往正比于能耗或者时间开销，显然，无论出于成本因素还是从安全性和用户体验的角度考虑，网络的计算开销都应当尽可能小。</p>
<p>这便是本文所提出方法的出发点，我们的目标是，对于输入图片，自适应地找到其与任务最相关的区域，进而通过使神经网络只处理这些区域，以尽可能小的计算量得到可信的结果。具体而言，我们采用的方法是，将一张分辨率较高的图片表征为若干个包含其关键部分的“小块”（Patch），而后仅将这些小块输入神经网络。以下面的示意图为例，将一张224x224的图片分解为3个96x96的Patch进行处理所需的计算量仅为原图的55.2%。</p>
<h2 id="2-method">2. Method</h2>
<p>为了实现上述目的，事实上，有两个显然的困难：</p>
<p>(a) 任意给定一张输入图片，如何判断其与任务最相关的区域在哪里呢？</p>
<p>(b) 考虑到我们的最终目的是使神经网络得到正确的预测结果，不同输入所需的计算量是不一样的，例如对于下面所示的两个输入图片，神经网络可能仅需要处理一个patch就能识别出特征非常突出的月亮，但是需要处理更多的patch才能分辨出猫咪的具体品种。</p>
<p>为了解决这两个问题，我们设计了一个Glance and Focus的框架，将这一思路建模为了一个序列决策过程，如下图所示。</p>
<p><img src="https://pic3.zhimg.com/v2-26a0e04ec9a6fd04d979b4681f1a610e_r.jpg" alt=""></p>
<p>其具体执行流程为：</p>
<ol>
<li>首先，对于一张任意给定的输入图片，由于我们没有任何关于它的先验知识，我们直接将其放缩为一个patch的大小，输入网络，这一方面产生了一个初步的判断结果，另一方面也提供了原始输入图片的空间分布信息；这一阶段称为扫视（Glance）。</li>
<li>而后，我们再以这些基本的空间分布信息为基础，逐步从原图上取得高分辨率的patch，将其不断输入网络，以此逐步更新预测结果和空间分布信息，得到更为准确的判断，并逐步寻找神经网络尚未见到过的关键区域；这一阶段称为关注（Focus）。</li>
</ol>
<p>值得注意的是，在上述序列过程的每一步结束之后，我们会将神经网络的预测自信度（confidence）与一个预先定义的阈值进行比较，一旦confidence超过阈值，我们便视为网络已经得到了可信的结果，这一过程立即终止。此机制称为自适应推理（Adaptive Inference）。通过这种机制，我们一方面可以使不同难易度的样本具有不同的序列长度，从而动态分配计算量、提高整体效率；另一方面可以简单地通过改变阈值调整网络的整体计算开销，而不需要重新训练网络，这使得我们的模型可以动态地以最小的计算开销达到所需的性能，或者实时最大化地利用所有可用的计算资源以提升模型表现。</p>
<h2 id="3-显而易见的困难">3. 显而易见的困难</h2>
<p>how to identify class-ciscriminative regions?
如何识别阶级歧视性区域？</p>
<p>how to determine the number of class-discriminative regions?
如何确定类区分区域的数量？</p>
<h2 id="4-网络结构">4. 网络结构</h2>
<p><img src="https://pic2.zhimg.com/v2-91af4cc8e58e6aabd6003b127857c459_r.jpg" alt=""></p>
<p>GFNet共有四个组件，分别为：</p>
<p>全局编码器${f_g}$和局部编码器${f_l}$ （Global Encoder and Local Encoder）为两个CNN，分别用于从放缩后的原图和局部patch中提取信息，之所以用两个CNN，是因为我们发现一个CNN很难同时适应缩略图和局部patch两种尺度（scale）的输入。几乎所有现有的网络结构均可以作为这两个编码器以提升其推理效率（如MobileNet-V3、EfficientNet、RegNet等）。
分类器 [公式] （Classifier）为一个循环神经网络（RNN），输入为全局池化后的特征向量，用于整合过去所有输入的信息，以得到目前最优的分类结果。
图像块选择网络 [公式] （Patch Proposal Network）是另一个循环神经网络（RNN），输入为全局池化前的特征图（不做池化是为了避免损失空间信息），用于整合目前为止所有的空间分布信息，并决定下一个patch的位置。值得注意的是由于取得patch的crop操作不可求导，[公式]是使用强化学习中的策略梯度方法（policy gradient）训练的。</p>
<p>两阶段框架</p>
<p>1 resize gfnet 置信度判断</p>
<p>2 缩略图 crop patch 用高分辨率输入到网络 置信度判断</p>
<p>3 Glance Focus(by Reinforcement Learning) Dynamic Inference</p>
<h2 id="5-training-method">5. training method</h2>
<p>1 分类
2 focus
3 fine tune</p>
<hr>
<p><strong>参考资料</strong></p>
<p><a href="https://zhuanlan.zhihu.com/p/266306870">https://zhuanlan.zhihu.com/p/266306870</a></p>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
