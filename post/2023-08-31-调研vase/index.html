<!doctype html>
<html lang="en-us">
  <head>
    <title>60-调研——Visual Audio Speech Enhancement // 亮的笔记</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.90.1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Liu Liang" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://ioyy900205.github.io/css/main.min.88e7083eff65effb7485b6e6f38d10afbec25093a6fac42d734ce9024d3defbd.css" />

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="60-调研——Visual Audio Speech Enhancement"/>
<meta name="twitter:description" content="在此处编辑 blog.walterlv.com 的博客摘要
  1 论文调研  1.1 ICASSP 2023  Audio for Multimedia and Multimodal Processing Human Identification and Face Recognition Learning from Multimodal Data ASR: Noise Robustness Keyword Spotting   1.2 INTERSPEECH 2023  Resources for Spoken Language Processing Analysis of Speech and Audio Signals Speech Recognition: Technologies and Systems for New Applications Spoken Dialog Systems and Conversational Analysis Speech Coding and Enhancement Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources, and Evaluation Speech, Voice, and Hearing Disorders Source Separation Speaker and Language Identification Speaker Recognition Speech Perception, Production, and Acquisition Multi-modal Systems Multi-talker Methods in Speech Processing New Computational Strategies for ASR Training and Inference Dialog Management Show and Tell: Health Applications and Emotion Recognition Show and Tell: Speech Tools, Speech Enhancement, Speech Synthesis     2 分析  2."/>

    <meta property="og:title" content="60-调研——Visual Audio Speech Enhancement" />
<meta property="og:description" content="在此处编辑 blog.walterlv.com 的博客摘要
  1 论文调研  1.1 ICASSP 2023  Audio for Multimedia and Multimodal Processing Human Identification and Face Recognition Learning from Multimodal Data ASR: Noise Robustness Keyword Spotting   1.2 INTERSPEECH 2023  Resources for Spoken Language Processing Analysis of Speech and Audio Signals Speech Recognition: Technologies and Systems for New Applications Spoken Dialog Systems and Conversational Analysis Speech Coding and Enhancement Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources, and Evaluation Speech, Voice, and Hearing Disorders Source Separation Speaker and Language Identification Speaker Recognition Speech Perception, Production, and Acquisition Multi-modal Systems Multi-talker Methods in Speech Processing New Computational Strategies for ASR Training and Inference Dialog Management Show and Tell: Health Applications and Emotion Recognition Show and Tell: Speech Tools, Speech Enhancement, Speech Synthesis     2 分析  2." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ioyy900205.github.io/post/2023-08-31-%E8%B0%83%E7%A0%94vase/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2023-08-31T11:28:02+08:00" />
<meta property="article:modified_time" content="2023-08-31T11:28:02+08:00" />



  </head>
  <body>
    <header class="app-header">
      <a href="https://ioyy900205.github.io"><img class="app-header-avatar" src="https://img95.699pic.com/element/40141/6992.png_300.png" alt="Liu Liang" /></a>
      <h1>亮的笔记</h1>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/">Home</a>
             - 
          
          <a class="app-header-menu-item" href="/tags/">Tags</a>
             - 
          
          <a class="app-header-menu-item" href="/about/">About</a>
      </nav>
      <p>深度学习笔记本</p>
      <div class="app-header-social">
        
          <a href="https://github.com/ioyy900205" target="_blank" rel="noreferrer noopener">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>Github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg>
          </a>
        
          <a href="https://twitter.com/ioyy900205" target="_blank" rel="noreferrer noopener">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-twitter">
  <title>Twitter</title>
  <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path>
</svg>
          </a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">60-调研——Visual Audio Speech Enhancement</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Aug 31, 2023
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          5 min read
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7.01" y2="7"></line>
</svg>
              <a class="tag" href="https://ioyy900205.github.io/tags/paper_review/">Paper_Review</a>
        </div>
      </div>
    </header>
    <div class="post-content">
      <p>在此处编辑 blog.walterlv.com 的博客摘要</p>
<hr>
<ul>
<li><a href="#1-%E8%AE%BA%E6%96%87%E8%B0%83%E7%A0%94">1 论文调研</a>
<ul>
<li><a href="#11-icassp-2023">1.1 ICASSP 2023</a>
<ul>
<li><a href="#audio-for-multimedia-and-multimodal-processing">Audio for Multimedia and Multimodal Processing</a></li>
<li><a href="#human-identification-and-face-recognition">Human Identification and Face Recognition</a></li>
<li><a href="#learning-from-multimodal-data">Learning from Multimodal Data</a></li>
<li><a href="#asr-noise-robustness">ASR: Noise Robustness</a></li>
<li><a href="#keyword-spotting">Keyword Spotting</a></li>
</ul>
</li>
<li><a href="#12-interspeech-2023">1.2 INTERSPEECH 2023</a>
<ul>
<li><a href="#resources-for-spoken-language-processing">Resources for Spoken Language Processing</a></li>
<li><a href="#analysis-of-speech-and-audio-signals">Analysis of Speech and Audio Signals</a></li>
<li><a href="#speech-recognition-technologies-and-systems-for-new-applications">Speech Recognition: Technologies and Systems for New Applications</a></li>
<li><a href="#spoken-dialog-systems-and-conversational-analysis">Spoken Dialog Systems and Conversational Analysis</a></li>
<li><a href="#speech-coding-and-enhancement">Speech Coding and Enhancement</a></li>
<li><a href="#spoken-language-processing-translation-information-retrieval-summarization-resources-and-evaluation">Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources, and Evaluation</a></li>
<li><a href="#speech-voice-and-hearing-disorders">Speech, Voice, and Hearing Disorders</a></li>
<li><a href="#source-separation">Source Separation</a></li>
<li><a href="#speaker-and-language-identification">Speaker and Language Identification</a></li>
<li><a href="#speaker-recognition">Speaker Recognition</a></li>
<li><a href="#speech-perception-production-and-acquisition">Speech Perception, Production, and Acquisition</a></li>
<li><a href="#multi-modal-systems">Multi-modal Systems</a></li>
<li><a href="#multi-talker-methods-in-speech-processing">Multi-talker Methods in Speech Processing</a></li>
<li><a href="#new-computational-strategies-for-asr-training-and-inference">New Computational Strategies for ASR Training and Inference</a></li>
<li><a href="#dialog-management">Dialog Management</a></li>
<li><a href="#show-and-tell-health-applications-and-emotion-recognition">Show and Tell: Health Applications and Emotion Recognition</a></li>
<li><a href="#show-and-tell-speech-tools-speech-enhancement-speech-synthesis">Show and Tell: Speech Tools, Speech Enhancement, Speech Synthesis</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#2-%E5%88%86%E6%9E%90">2 分析</a>
<ul>
<li><a href="#21-%E5%88%86%E7%A6%BB%E5%92%8C%E5%A2%9E%E5%BC%BA%E7%9B%B8%E5%85%B3">2.1 分离和增强相关</a></li>
<li><a href="#22-%E5%B8%A6%E6%9C%89github%E5%9C%B0%E5%9D%80">2.2 带有GITHUB地址</a></li>
</ul>
</li>
</ul>
<h1 id="1-论文调研">1 论文调研</h1>
<h2 id="11-icassp-2023">1.1 ICASSP 2023</h2>
<p>ICASSP 2023 Papers: A complete collection of influential and exciting research papers from the <a href="https://2023.ieeeicassp.org/"><em>ICASSP 2023</em></a> conference. Explore the latest advancements in acoustics, speech and signal processing. Code included. :star: the repository to support the advancement of audio and signal processing!</p>
<!-- raw HTML omitted -->
<h3 id="audio-for-multimedia-and-multimodal-processing">Audio for Multimedia and Multimodal Processing</h3>
<table>
<thead>
<tr>
<th>ID</th>
<th><strong>Title</strong></th>
<th style="text-align:center"><strong>Repo</strong></th>
<th style="text-align:center"><strong>Paper</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>6119</td>
<td>Incorporating Lip Features Into Audio-Visual Multi-Speaker DOA Estimation by Gated Fusion</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://ieeexplore.ieee.org/document/10095549"><img src="https://img.shields.io/badge/IEEE-10095549-E4A42C.svg" alt="IEEE Xplore"></a></td>
</tr>
<tr>
<td>6787</td>
<td>UAVM: Towards Unifying Audio and Visual Models (SPS Journal Paper)</td>
<td style="text-align:center"><a href="https://github.com/YuanGongND/uavm"><img src="https://img.shields.io/github/stars/YuanGongND/uavm" alt="GitHub"></a></td>
<td style="text-align:center"><a href="https://ieeexplore.ieee.org/document/9964072"><img src="https://img.shields.io/badge/IEEE-9964072-E4A42C.svg" alt="IEEE Xplore"></a> <!-- raw HTML omitted --> <a href="https://arxiv.org/abs/2208.00061"><img src="https://img.shields.io/badge/arXiv-2208.00061-b31b1b.svg" alt="arXiv"></a></td>
</tr>
</tbody>
</table>
<h3 id="human-identification-and-face-recognition">Human Identification and Face Recognition</h3>
<table>
<thead>
<tr>
<th>ID</th>
<th><strong>Title</strong></th>
<th style="text-align:center"><strong>Repo</strong></th>
<th style="text-align:center"><strong>Paper</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>5309</td>
<td>Recursive Joint Attention for Audio-Visual Fusion in Regression based Emotion Recognition</td>
<td style="text-align:center"><a href="https://github.com/praveena2j/RecurrentJointAttentionwithLSTMs"><img src="https://img.shields.io/github/stars/praveena2j/RecurrentJointAttentionwithLSTMs" alt="GitHub"></a></td>
<td style="text-align:center"><a href="https://ieeexplore.ieee.org/document/10095234"><img src="https://img.shields.io/badge/IEEE-10095234-E4A42C.svg" alt="IEEE Xplore"></a> <!-- raw HTML omitted --> <a href="https://arxiv.org/abs/2304.07958"><img src="https://img.shields.io/badge/arXiv-2304.07958-b31b1b.svg" alt="arXiv"></a></td>
</tr>
</tbody>
</table>
<h3 id="learning-from-multimodal-data">Learning from Multimodal Data</h3>
<table>
<thead>
<tr>
<th>ID</th>
<th><strong>Title</strong></th>
<th style="text-align:center"><strong>Repo</strong></th>
<th style="text-align:center"><strong>Paper</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>1571</td>
<td>Towards Robust Audio-based Vehicle Detection via Importance-Aware Audio-Visual Learning</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://ieeexplore.ieee.org/document/10096773"><img src="https://img.shields.io/badge/IEEE-10096773-E4A42C.svg" alt="IEEE Xplore"></a></td>
</tr>
</tbody>
</table>
<h3 id="asr-noise-robustness">ASR: Noise Robustness</h3>
<table>
<thead>
<tr>
<th>ID</th>
<th><strong>Title</strong></th>
<th style="text-align:center"><strong>Repo</strong></th>
<th style="text-align:center"><strong>Paper</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>2040</td>
<td>Robust Audio-Visual ASR with Unified Cross-Modal Attention</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://ieeexplore.ieee.org/document/10096893"><img src="https://img.shields.io/badge/IEEE-10096893-E4A42C.svg" alt="IEEE Xplore"></a></td>
</tr>
</tbody>
</table>
<h3 id="keyword-spotting">Keyword Spotting</h3>
<table>
<thead>
<tr>
<th>ID</th>
<th><strong>Title</strong></th>
<th style="text-align:center"><strong>Repo</strong></th>
<th style="text-align:center"><strong>Paper</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>6078</td>
<td>The DKU Post-Challenge Audio-Visual Wake Word Spotting System for the 2021 MISP Challenge: Deep Analysis</td>
<td style="text-align:center"><a href="https://github.com/Mashiro009/DKU_WWS_MISP"><img src="https://img.shields.io/github/stars/Mashiro009/DKU_WWS_MISP" alt="GitHub"></a></td>
<td style="text-align:center"><a href="https://ieeexplore.ieee.org/document/10095459"><img src="https://img.shields.io/badge/IEEE-10095459-E4A42C.svg" alt="IEEE Xplore"></a> <!-- raw HTML omitted --> <a href="https://arxiv.org/abs/2303.02348"><img src="https://img.shields.io/badge/arXiv-2303.02348-b31b1b.svg" alt="arXiv"></a></td>
</tr>
</tbody>
</table>
<h2 id="12-interspeech-2023">1.2 INTERSPEECH 2023</h2>
<p>INTERSPEECH 2023 Papers: A complete collection of influential and exciting research papers from the <a href="https://interspeech2023.org/"><em>INTERSPEECH 2023</em></a> conference. Explore the latest advances in speech and language processing. Code included. :star: the repository to support the advancement of speech technology!</p>
<!-- raw HTML omitted -->
<h3 id="resources-for-spoken-language-processing">Resources for Spoken Language Processing</h3>
<p><img src="https://img.shields.io/badge/Section%20Papers-6-42BA16" alt="Section Papers"> <img src="https://img.shields.io/badge/Preprint%20Papers-3-b31b1b" alt="Preprint Papers"> <img src="https://img.shields.io/badge/Papers%20with%20Open%20Code-2-1D7FBF" alt="Papers with Open Code"></p>
<table>
<thead>
<tr>
<th>ID</th>
<th><strong>Title</strong></th>
<th style="text-align:center"><strong>Repo</strong></th>
<th style="text-align:center"><strong>Paper</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>2279</td>
<td>MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation</td>
<td style="text-align:center"><a href="https://github.com/facebookresearch/muavic"><img src="https://img.shields.io/github/stars/facebookresearch/muavic" alt="GitHub"></a></td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/anwar23_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a> <!-- raw HTML omitted --> <a href="https://arxiv.org/abs/2303.00628"><img src="https://img.shields.io/badge/arXiv-2303.00628-b31b1b.svg" alt="arXiv"></a></td>
</tr>
</tbody>
</table>
<h3 id="analysis-of-speech-and-audio-signals">Analysis of Speech and Audio Signals</h3>
<p><img src="https://img.shields.io/badge/Section%20Papers-85-42BA16" alt="Section Papers"> <img src="https://img.shields.io/badge/Preprint%20Papers-32-b31b1b" alt="Preprint Papers"> <img src="https://img.shields.io/badge/Papers%20with%20Open%20Code-27-1D7FBF" alt="Papers with Open Code"></p>
<table>
<thead>
<tr>
<th>ID</th>
<th><strong>Title</strong></th>
<th style="text-align:center"><strong>Repo</strong></th>
<th style="text-align:center"><strong>Paper</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>1581</td>
<td>GRAVO: Learning to Generate Relevant Audio from Visual Features with Noisy Online Videos</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/ahn23b_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a></td>
</tr>
<tr>
<td>914</td>
<td>Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention</td>
<td style="text-align:center"><a href="https://github.com/liuxubo717/V-ACT"><img src="https://img.shields.io/github/stars/liuxubo717/V-ACT" alt="GitHub"></a></td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/liu23l_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a> <!-- raw HTML omitted --> <a href="https://arxiv.org/abs/2210.16428"><img src="https://img.shields.io/badge/arXiv-2210.16428-b31b1b.svg" alt="arXiv"></a></td>
</tr>
<tr>
<td>801</td>
<td>Audio-Visual Fusion using Multiscale Temporal Convolutional Attention for Time-Domain Speech Separation</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/liu23h_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a></td>
</tr>
<tr>
<td>889</td>
<td>PIAVE: A Pose-Invariant Audio-Visual Speaker Extraction Network</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/liu23k_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a></td>
</tr>
<tr>
<td>1309</td>
<td>Image-Driven Audio-Visual Universal Source Separation</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/li23q_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a></td>
</tr>
<tr>
<td>2545</td>
<td>Rethinking the Visual Cues in Audio-Visual Speaker Extraction</td>
<td style="text-align:center"><a href="https://github.com/mrjunjieli/DAVSE"><img src="https://img.shields.io/github/stars/mrjunjieli/DAVSE" alt="GitHub"></a></td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/li23ja_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a> <!-- raw HTML omitted --> <a href="https://arxiv.org/abs/2306.02625"><img src="https://img.shields.io/badge/arXiv-2306.02625-b31b1b.svg" alt="arXiv"></a></td>
</tr>
</tbody>
</table>
<h3 id="speech-recognition-technologies-and-systems-for-new-applications">Speech Recognition: Technologies and Systems for New Applications</h3>
<table>
<thead>
<tr>
<th>ID</th>
<th><strong>Title</strong></th>
<th style="text-align:center"><strong>Repo</strong></th>
<th style="text-align:center"><strong>Paper</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>135</td>
<td>Segmental SpeechCLIP: Utilizing Pretrained Image-Text Models for Audio-Visual Learning</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/bhati23_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a></td>
</tr>
<tr>
<td>793</td>
<td>An Efficient and Noise-Robust Audiovisual Encoder for Audiovisual Speech Recognition</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/li23k_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a></td>
</tr>
<tr>
<td>487</td>
<td>Streaming Audio-Visual Speech Recognition with Alignment Regularization</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/ma23_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a> <!-- raw HTML omitted --> <a href="https://arxiv.org/abs/2211.02133"><img src="https://img.shields.io/badge/arXiv-2211.02133-b31b1b.svg" alt="arXiv"></a></td>
</tr>
<tr>
<td>462</td>
<td>SparseVSR: Lightweight and Noise Robust Visual Speech Recognition</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/fernandezlopez23_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a> <!-- raw HTML omitted --> <a href="https://arxiv.org/abs/2307.04552"><img src="https://img.shields.io/badge/arXiv-2307.04552-b31b1b.svg" alt="arXiv"></a></td>
</tr>
</tbody>
</table>
<h3 id="spoken-dialog-systems-and-conversational-analysis">Spoken Dialog Systems and Conversational Analysis</h3>
<p><img src="https://img.shields.io/badge/Section%20Papers-37-42BA16" alt="Section Papers"> <img src="https://img.shields.io/badge/Preprint%20Papers-8-b31b1b" alt="Preprint Papers"> <img src="https://img.shields.io/badge/Papers%20with%20Open%20Code-4-1D7FBF" alt="Papers with Open Code"></p>
<table>
<thead>
<tr>
<th>ID</th>
<th><strong>Title</strong></th>
<th style="text-align:center"><strong>Repo</strong></th>
<th style="text-align:center"><strong>Paper</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>578</td>
<td>Multimodal Turn-Taking Model using Visual cues for End-of-Utterance Prediction in Spoken Dialogue Systems</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/kurata23_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a></td>
</tr>
<tr>
<td>1464</td>
<td>Audio-Visual Praise Estimation for Conversational Video based on Synchronization-Guided Multimodal Transformer</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/hojo23_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a></td>
</tr>
<tr>
<td>1717</td>
<td>A Multiple-Teacher Pruning based Self-Distillation (MT-PSD) Approach to Model Compression for Audio-Visual Wake Word Spotting</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/wang23la_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a></td>
</tr>
</tbody>
</table>
<h3 id="speech-coding-and-enhancement">Speech Coding and Enhancement</h3>
<p><img src="https://img.shields.io/badge/Section%20Papers-58-42BA16" alt="Section Papers"> <img src="https://img.shields.io/badge/Preprint%20Papers-33-b31b1b" alt="Preprint Papers"> <img src="https://img.shields.io/badge/Papers%20with%20Open%20Code-14-1D7FBF" alt="Papers with Open Code"></p>
<table>
<thead>
<tr>
<th>ID</th>
<th><strong>Title</strong></th>
<th style="text-align:center"><strong>Repo</strong></th>
<th style="text-align:center"><strong>Paper</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>780</td>
<td>Incorporating Ultrasound Tongue Images for Audio-Visual Speech Enhancement through Knowledge Distillation</td>
<td style="text-align:center"><a href="https://zhengrachel.github.io/UTIforAVSE-demo/"><img src="https://img.shields.io/badge/GitHub-Page-159957.svg" alt="GitHub Page"></a> <!-- raw HTML omitted --> <a href="https://github.com/ZhengRachel/UTIforAVSE-demo"><img src="https://img.shields.io/github/stars/ZhengRachel/UTIforAVSE-demo" alt="GitHub"></a></td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/zheng23b_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a> <!-- raw HTML omitted --> <a href="https://arxiv.org/abs/2305.14933"><img src="https://img.shields.io/badge/arXiv-2305.14933-b31b1b.svg" alt="arXiv"></a></td>
</tr>
</tbody>
</table>
<h3 id="spoken-language-processing-translation-information-retrieval-summarization-resources-and-evaluation">Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources, and Evaluation</h3>
<p><img src="https://img.shields.io/badge/Section%20Papers-48-42BA16" alt="Section Papers"> <img src="https://img.shields.io/badge/Preprint%20Papers-21-b31b1b" alt="Preprint Papers"> <img src="https://img.shields.io/badge/Papers%20with%20Open%20Code-13-1D7FBF" alt="Papers with Open Code"></p>
<table>
<thead>
<tr>
<th>ID</th>
<th><strong>Title</strong></th>
<th style="text-align:center"><strong>Repo</strong></th>
<th style="text-align:center"><strong>Paper</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>1674</td>
<td>CN-Celeb-AV: A Multi-Genre Audio-Visual Dataset for Person Recognition</td>
<td style="text-align:center"><a href="http://cnceleb.org/"><img src="https://img.shields.io/badge/WEB-Page-159957.svg" alt="WEB Page"></a></td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/li23y_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a> <!-- raw HTML omitted --> <a href="https://arxiv.org/abs/2305.16049"><img src="https://img.shields.io/badge/arXiv-2305.16049-b31b1b.svg" alt="arXiv"></a></td>
</tr>
<tr>
<td>823</td>
<td>MAVD: The First Open Large-Scale Mandarin Audio-Visual Dataset with Depth Information</td>
<td style="text-align:center"><a href="https://github.com/YangHao97/investigateAudioEncoders"><img src="https://img.shields.io/github/stars/SpringHuo/MAVD" alt="GitHub"></a></td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/wang23o_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a> <!-- raw HTML omitted --> <a href="https://arxiv.org/abs/2306.02263"><img src="https://img.shields.io/badge/arXiv-2306.02263-b31b1b.svg" alt="arXiv"></a></td>
</tr>
</tbody>
</table>
<h3 id="speech-voice-and-hearing-disorders">Speech, Voice, and Hearing Disorders</h3>
<p><img src="https://img.shields.io/badge/Section%20Papers-28-42BA16" alt="Section Papers"> <img src="https://img.shields.io/badge/Preprint%20Papers-13-b31b1b" alt="Preprint Papers"> <img src="https://img.shields.io/badge/Papers%20with%20Open%20Code-6-1D7FBF" alt="Papers with Open Code"></p>
<table>
<thead>
<tr>
<th>ID</th>
<th><strong>Title</strong></th>
<th style="text-align:center"><strong>Repo</strong></th>
<th style="text-align:center"><strong>Paper</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>866</td>
<td>Audio-Visual Mandarin Electrolaryngeal Speech Voice Conversion</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/chien23_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a> <!-- raw HTML omitted --> <a href="https://arxiv.org/abs/2306.06652"><img src="https://img.shields.io/badge/arXiv-2306.06652-b31b1b.svg" alt="arXiv"></a></td>
</tr>
</tbody>
</table>
<h3 id="source-separation">Source Separation</h3>
<p><img src="https://img.shields.io/badge/Section%20Papers-6-42BA16" alt="Section Papers"> <img src="https://img.shields.io/badge/Preprint%20Papers-2-b31b1b" alt="Preprint Papers"> <img src="https://img.shields.io/badge/Papers%20with%20Open%20Code-1-1D7FBF" alt="Papers with Open Code"></p>
<table>
<thead>
<tr>
<th>ID</th>
<th><strong>Title</strong></th>
<th style="text-align:center"><strong>Repo</strong></th>
<th style="text-align:center"><strong>Paper</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>1753</td>
<td>Audio-Visual Speech Separation in Noisy Environments with a Lightweight Iterative Model</td>
<td style="text-align:center"><a href="https://avlit-interspeech.github.io/"><img src="https://img.shields.io/badge/GitHub-Page-159957.svg" alt="GitHub Page"></a> <!-- raw HTML omitted --> <a href="https://github.com/hmartelb/avlit"><img src="https://img.shields.io/github/stars/hmartelb/avlit" alt="GitHub"></a></td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/martel23_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a> <!-- raw HTML omitted --> <a href="https://arxiv.org/abs/2306.00160"><img src="https://img.shields.io/badge/arXiv-2306.00160-b31b1b.svg" alt="arXiv"></a></td>
</tr>
</tbody>
</table>
<h3 id="speaker-and-language-identification">Speaker and Language Identification</h3>
<p><img src="https://img.shields.io/badge/Section%20Papers-59-42BA16" alt="Section Papers"> <img src="https://img.shields.io/badge/Preprint%20Papers-30-b31b1b" alt="Preprint Papers"> <img src="https://img.shields.io/badge/Papers%20with%20Open%20Code-16-1D7FBF" alt="Papers with Open Code"></p>
<table>
<thead>
<tr>
<th>ID</th>
<th><strong>Title</strong></th>
<th style="text-align:center"><strong>Repo</strong></th>
<th style="text-align:center"><strong>Paper</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>574</td>
<td>Target Active Speaker Detection with Audio-Visual Cues</td>
<td style="text-align:center"><a href="https://github.com/Jiang-Yidi/TS-TalkNet"><img src="https://img.shields.io/github/stars/Jiang-Yidi/TS-TalkNet" alt="GitHub"></a></td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/jiang23c_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a> <!-- raw HTML omitted --> <a href="https://arxiv.org/abs/2305.12831"><img src="https://img.shields.io/badge/arXiv-2305.12831-b31b1b.svg" alt="arXiv"></a></td>
</tr>
<tr>
<td>394</td>
<td>A Method of Audio-Visual Person Verification by Mining Connections between Time Series</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/sun23_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a></td>
</tr>
</tbody>
</table>
<h3 id="speaker-recognition">Speaker Recognition</h3>
<p><img src="https://img.shields.io/badge/Section%20Papers-10-42BA16" alt="Section Papers"> <img src="https://img.shields.io/badge/Preprint%20Papers-7-b31b1b" alt="Preprint Papers"> <img src="https://img.shields.io/badge/Papers%20with%20Open%20Code-2-1D7FBF" alt="Papers with Open Code"></p>
<table>
<thead>
<tr>
<th>ID</th>
<th><strong>Title</strong></th>
<th style="text-align:center"><strong>Repo</strong></th>
<th style="text-align:center"><strong>Paper</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>1298</td>
<td>Visualizing Data Augmentation in Deep Speaker Recognition</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/li23p_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a> <!-- raw HTML omitted --> <a href="https://arxiv.org/abs/2305.16070"><img src="https://img.shields.io/badge/arXiv-2305.16070-b31b1b.svg" alt="arXiv"></a></td>
</tr>
</tbody>
</table>
<h3 id="speech-perception-production-and-acquisition">Speech Perception, Production, and Acquisition</h3>
<p><img src="https://img.shields.io/badge/Section%20Papers-33-42BA16" alt="Section Papers"> <img src="https://img.shields.io/badge/Preprint%20Papers-4-b31b1b" alt="Preprint Papers"> <img src="https://img.shields.io/badge/Papers%20with%20Open%20Code-2-1D7FBF" alt="Papers with Open Code"></p>
<table>
<thead>
<tr>
<th>ID</th>
<th><strong>Title</strong></th>
<th style="text-align:center"><strong>Repo</strong></th>
<th style="text-align:center"><strong>Paper</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>2256</td>
<td>An Improved End-to-End Audio-Visual Speech Recognition Model</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/yang23w_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a></td>
</tr>
<tr>
<td>803</td>
<td>Audio, Visual and Audiovisual Intelligibility of Vowels Produced in Noise</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/garnier23_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a></td>
</tr>
</tbody>
</table>
<h3 id="multi-modal-systems">Multi-modal Systems</h3>
<p><img src="https://img.shields.io/badge/Section%20Papers-6-42BA16" alt="Section Papers"> <img src="https://img.shields.io/badge/Preprint%20Papers-4-b31b1b" alt="Preprint Papers"> <img src="https://img.shields.io/badge/Papers%20with%20Open%20Code-1-1D7FBF" alt="Papers with Open Code"></p>
<table>
<thead>
<tr>
<th>ID</th>
<th><strong>Title</strong></th>
<th style="text-align:center"><strong>Repo</strong></th>
<th style="text-align:center"><strong>Paper</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>370</td>
<td>Improving the Gap in Visual Speech Recognition Between Normal and Silent Speech based on Metric Learning</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/kashiwagi23_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a> <!-- raw HTML omitted --> <a href="https://arxiv.org/abs/2305.14203"><img src="https://img.shields.io/badge/arXiv-2305.14203-b31b1b.svg" alt="arXiv"></a></td>
</tr>
<tr>
<td>668</td>
<td>Visually Grounded Few-Shot Word Acquisition with Fewer Shots</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/nortje23_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a>  <!-- raw HTML omitted --> <a href="https://arxiv.org/abs/2305.15937"><img src="https://img.shields.io/badge/arXiv-2305.15937-b31b1b.svg" alt="arXiv"></a></td>
</tr>
</tbody>
</table>
<h3 id="multi-talker-methods-in-speech-processing">Multi-talker Methods in Speech Processing</h3>
<p><img src="https://img.shields.io/badge/Section%20Papers-16-42BA16" alt="Section Papers"> <img src="https://img.shields.io/badge/Preprint%20Papers-7-b31b1b" alt="Preprint Papers"> <img src="https://img.shields.io/badge/Papers%20with%20Open%20Code-1-1D7FBF" alt="Papers with Open Code"></p>
<table>
<thead>
<tr>
<th>ID</th>
<th><strong>Title</strong></th>
<th style="text-align:center"><strong>Repo</strong></th>
<th style="text-align:center"><strong>Paper</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>2098</td>
<td>Time-Domain Transformer-based Audiovisual Speaker Separation</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/ahmadikalkhorani23_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a></td>
</tr>
</tbody>
</table>
<h3 id="new-computational-strategies-for-asr-training-and-inference">New Computational Strategies for ASR Training and Inference</h3>
<p><img src="https://img.shields.io/badge/Section%20Papers-6-42BA16" alt="Section Papers"> <img src="https://img.shields.io/badge/Preprint%20Papers-4-b31b1b" alt="Preprint Papers"> <img src="https://img.shields.io/badge/Papers%20with%20Open%20Code-0-1D7FBF" alt="Papers with Open Code"></p>
<table>
<thead>
<tr>
<th>:id:</th>
<th><strong>Title</strong></th>
<th style="text-align:center"><strong>Repo</strong></th>
<th style="text-align:center"><strong>Paper</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>969</td>
<td>Another Point of View on Visual Speech Recognition</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/pouthier23_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a></td>
</tr>
</tbody>
</table>
<h3 id="dialog-management">Dialog Management</h3>
<p><img src="https://img.shields.io/badge/Section%20Papers-6-42BA16" alt="Section Papers"> <img src="https://img.shields.io/badge/Preprint%20Papers-4-b31b1b" alt="Preprint Papers"> <img src="https://img.shields.io/badge/Papers%20with%20Open%20Code-1-1D7FBF" alt="Papers with Open Code"></p>
<table>
<thead>
<tr>
<th>:id:</th>
<th><strong>Title</strong></th>
<th style="text-align:center"><strong>Repo</strong></th>
<th style="text-align:center"><strong>Paper</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>1983</td>
<td>Style-Transfer based Speech and Audio-Visual Scene Understanding for Robot Action Sequence Acquisition from Videos</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/hori23_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a> <!-- raw HTML omitted --> <a href="https://arxiv.org/abs/2306.15644"><img src="https://img.shields.io/badge/arXiv-2306.15644-b31b1b.svg" alt="arXiv"></a></td>
</tr>
</tbody>
</table>
<h3 id="show-and-tell-health-applications-and-emotion-recognition">Show and Tell: Health Applications and Emotion Recognition</h3>
<p><img src="https://img.shields.io/badge/Section%20Papers-12-42BA16" alt="Section Papers"> <img src="https://img.shields.io/badge/Preprint%20Papers-1-b31b1b" alt="Preprint Papers"> <img src="https://img.shields.io/badge/Papers%20with%20Open%20Code-0-1D7FBF" alt="Papers with Open Code"></p>
<table>
<thead>
<tr>
<th>:id:</th>
<th><strong>Title</strong></th>
<th style="text-align:center"><strong>Repo</strong></th>
<th style="text-align:center"><strong>Paper</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>2656</td>
<td>5G-IoT Cloud based Demonstration of Real-Time Audio-Visual Speech Enhancement for Multimodal Hearing-aids</td>
<td style="text-align:center"><a href="https://cogmhear.org/index.html"><img src="https://img.shields.io/badge/WEB-Page-159957.svg" alt="WEB Page"></a></td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/gupta23b_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a></td>
</tr>
</tbody>
</table>
<h3 id="show-and-tell-speech-tools-speech-enhancement-speech-synthesis">Show and Tell: Speech Tools, Speech Enhancement, Speech Synthesis</h3>
<p><img src="https://img.shields.io/badge/Section%20Papers-10-42BA16" alt="Section Papers"> <img src="https://img.shields.io/badge/Preprint%20Papers-2-b31b1b" alt="Preprint Papers"> <img src="https://img.shields.io/badge/Papers%20with%20Open%20Code-3-1D7FBF" alt="Papers with Open Code"></p>
<table>
<thead>
<tr>
<th>ID</th>
<th><strong>Title</strong></th>
<th style="text-align:center"><strong>Repo</strong></th>
<th style="text-align:center"><strong>Paper</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>2625</td>
<td>Sp1NY: A Quick and Flexible Python Speech Visualization Tool</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/lemaguer23_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a></td>
</tr>
<tr>
<td>2667</td>
<td>Application for Real-Time Audio-Visual Speech Enhancement</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/gogate23_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a></td>
</tr>
</tbody>
</table>
<h1 id="2-分析">2 分析</h1>
<h2 id="21-分离和增强相关">2.1 分离和增强相关</h2>
<table>
<thead>
<tr>
<th>ID</th>
<th><strong>Title</strong></th>
<th style="text-align:center"><strong>Repo</strong></th>
<th style="text-align:center"><strong>Paper</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>2667</td>
<td>Application for Real-Time Audio-Visual Speech Enhancement</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/gogate23_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a></td>
</tr>
<tr>
<td>2656</td>
<td>5G-IoT Cloud based Demonstration of Real-Time Audio-Visual Speech Enhancement for Multimodal Hearing-aids</td>
<td style="text-align:center"><a href="https://cogmhear.org/index.html"><img src="https://img.shields.io/badge/WEB-Page-159957.svg" alt="WEB Page"></a></td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/gupta23b_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a></td>
</tr>
<tr>
<td>2098</td>
<td>Time-Domain Transformer-based Audiovisual Speaker Separation</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/ahmadikalkhorani23_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a></td>
</tr>
<tr>
<td>1753</td>
<td>Audio-Visual Speech Separation in Noisy Environments with a Lightweight Iterative Model</td>
<td style="text-align:center"><a href="https://avlit-interspeech.github.io/"><img src="https://img.shields.io/badge/GitHub-Page-159957.svg" alt="GitHub Page"></a> <!-- raw HTML omitted --> <a href="https://github.com/hmartelb/avlit"><img src="https://img.shields.io/github/stars/hmartelb/avlit" alt="GitHub"></a></td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/martel23_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a> <!-- raw HTML omitted --> <a href="https://arxiv.org/abs/2306.00160"><img src="https://img.shields.io/badge/arXiv-2306.00160-b31b1b.svg" alt="arXiv"></a></td>
</tr>
<tr>
<td>823</td>
<td>MAVD: The First Open Large-Scale Mandarin Audio-Visual Dataset with Depth Information</td>
<td style="text-align:center"><a href="https://github.com/YangHao97/investigateAudioEncoders"><img src="https://img.shields.io/github/stars/SpringHuo/MAVD" alt="GitHub"></a></td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/wang23o_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a> <!-- raw HTML omitted --> <a href="https://arxiv.org/abs/2306.02263"><img src="https://img.shields.io/badge/arXiv-2306.02263-b31b1b.svg" alt="arXiv"></a></td>
</tr>
<tr>
<td>780</td>
<td>Incorporating Ultrasound Tongue Images for Audio-Visual Speech Enhancement through Knowledge Distillation</td>
<td style="text-align:center"><a href="https://zhengrachel.github.io/UTIforAVSE-demo/"><img src="https://img.shields.io/badge/GitHub-Page-159957.svg" alt="GitHub Page"></a> <!-- raw HTML omitted --> <a href="https://github.com/ZhengRachel/UTIforAVSE-demo"><img src="https://img.shields.io/github/stars/ZhengRachel/UTIforAVSE-demo" alt="GitHub"></a></td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/zheng23b_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a> <!-- raw HTML omitted --> <a href="https://arxiv.org/abs/2305.14933"><img src="https://img.shields.io/badge/arXiv-2305.14933-b31b1b.svg" alt="arXiv"></a></td>
</tr>
<tr>
<td>2545</td>
<td>Rethinking the Visual Cues in Audio-Visual Speaker Extraction</td>
<td style="text-align:center"><a href="https://github.com/mrjunjieli/DAVSE"><img src="https://img.shields.io/github/stars/mrjunjieli/DAVSE" alt="GitHub"></a></td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/li23ja_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a> <!-- raw HTML omitted --> <a href="https://arxiv.org/abs/2306.02625"><img src="https://img.shields.io/badge/arXiv-2306.02625-b31b1b.svg" alt="arXiv"></a></td>
</tr>
<tr>
<td>1309</td>
<td>Image-Driven Audio-Visual Universal Source Separation</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/li23q_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a></td>
</tr>
<tr>
<td>889</td>
<td>PIAVE: A Pose-Invariant Audio-Visual Speaker Extraction Network</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/liu23k_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a></td>
</tr>
<tr>
<td>801</td>
<td>Audio-Visual Fusion using Multiscale Temporal Convolutional Attention for Time-Domain Speech Separation</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/liu23h_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a></td>
</tr>
</tbody>
</table>
<h2 id="22-带有github地址">2.2 带有GITHUB地址</h2>
<table>
<thead>
<tr>
<th>ID</th>
<th><strong>Title</strong></th>
<th style="text-align:center"><strong>Repo</strong></th>
<th style="text-align:center"><strong>Paper</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>1753</td>
<td>Audio-Visual Speech Separation in Noisy Environments with a Lightweight Iterative Model</td>
<td style="text-align:center"><a href="https://avlit-interspeech.github.io/"><img src="https://img.shields.io/badge/GitHub-Page-159957.svg" alt="GitHub Page"></a> <!-- raw HTML omitted --> <a href="https://github.com/hmartelb/avlit"><img src="https://img.shields.io/github/stars/hmartelb/avlit" alt="GitHub"></a></td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/martel23_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a> <!-- raw HTML omitted --> <a href="https://arxiv.org/abs/2306.00160"><img src="https://img.shields.io/badge/arXiv-2306.00160-b31b1b.svg" alt="arXiv"></a></td>
</tr>
<tr>
<td>823</td>
<td>MAVD: The First Open Large-Scale Mandarin Audio-Visual Dataset with Depth Information</td>
<td style="text-align:center"><a href="https://github.com/YangHao97/investigateAudioEncoders"><img src="https://img.shields.io/github/stars/SpringHuo/MAVD" alt="GitHub"></a></td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/wang23o_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a> <!-- raw HTML omitted --> <a href="https://arxiv.org/abs/2306.02263"><img src="https://img.shields.io/badge/arXiv-2306.02263-b31b1b.svg" alt="arXiv"></a></td>
</tr>
<tr>
<td>780</td>
<td>Incorporating Ultrasound Tongue Images for Audio-Visual Speech Enhancement through Knowledge Distillation</td>
<td style="text-align:center"><a href="https://zhengrachel.github.io/UTIforAVSE-demo/"><img src="https://img.shields.io/badge/GitHub-Page-159957.svg" alt="GitHub Page"></a> <!-- raw HTML omitted --> <a href="https://github.com/ZhengRachel/UTIforAVSE-demo"><img src="https://img.shields.io/github/stars/ZhengRachel/UTIforAVSE-demo" alt="GitHub"></a></td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/zheng23b_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a> <!-- raw HTML omitted --> <a href="https://arxiv.org/abs/2305.14933"><img src="https://img.shields.io/badge/arXiv-2305.14933-b31b1b.svg" alt="arXiv"></a></td>
</tr>
<tr>
<td>2545</td>
<td>Rethinking the Visual Cues in Audio-Visual Speaker Extraction</td>
<td style="text-align:center"><a href="https://github.com/mrjunjieli/DAVSE"><img src="https://img.shields.io/github/stars/mrjunjieli/DAVSE" alt="GitHub"></a></td>
<td style="text-align:center"><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/li23ja_interspeech.pdf"><img src="https://img.shields.io/badge/isca-version-355778.svg" alt="ISCA"></a> <!-- raw HTML omitted --> <a href="https://arxiv.org/abs/2306.02625"><img src="https://img.shields.io/badge/arXiv-2306.02625-b31b1b.svg" alt="arXiv"></a></td>
</tr>
</tbody>
</table>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
