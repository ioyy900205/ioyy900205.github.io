<!doctype html>
<html lang="en-us">
  <head>
    <title>56-S-DCCRN论文阅读 // 亮的笔记</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.90.1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Liu Liang" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://ioyy900205.github.io/css/main.min.88e7083eff65effb7485b6e6f38d10afbec25093a6fac42d734ce9024d3defbd.css" />

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="56-S-DCCRN论文阅读"/>
<meta name="twitter:description" content="在语音增强方面，复杂的神经网络由于其在处理复值频谱方面的有效性而显示出良好的性能。最近的大多数语音增强方法主要集中在采样率为16K Hz的宽频信号上。然而，由于难以对更多的频段，特别是高频成分进行建模，因此仍然缺乏对超宽频段（如32K Hz）甚至全频段（48K）去噪的研究。在本文中，我们将之前的深度复杂卷积递归神经网络（DCCRN）大幅扩展为超宽频带版本&ndash;S-DCCRN，以对32K Hz采样率的语音进行去噪。
具体操作：
 我们首先采用了一个级联的子带和全带处理模块，它由两个小尺寸的DCCRN组成&ndash;一个操作子带信号，一个操作全带信号，旨在从局部和全局频率信息中受益。此外，我们没有简单地采用STFT特征作为输入，而是使用了一个以端到端方式训练的复杂特征编码器来完善不同频段的信息。 我们还使用了一个复杂的特征解码器来将特征还原到时频域。最后，采用一种可学习的频谱压缩方法来调整不同频段的能量，这有利于神经网络的学习。   所提出的模型S-DCCRN已经超过了PercepNet以及几个有竞争力的模型，在语音质量和可懂度方面达到了最先进的性能。消融研究也证明了不同贡献的有效性。   @TOC
1. 引言 随着电话会议和其他实时语音通信场景的快速发展，对高质量高保真语音的需求急剧增加。随着采样率的提高，语音将包含更丰富的信息和更精细的细节，特别是在更高的频段。然而，目前大多数基于深度学习的语音增强方法主要关注采样率为16K Hz的宽频信号。在超宽频[1]甚至全频段信号上进行语音增强的潜力仍有待探索，因为在对更多频段尤其是高频成分进行建模方面存在着挑战。此外，用更大维度的特征进行建模将导致更高的建模复杂性，使实时实施变得更加困难。一些语音增强器采用了压缩的特征，如吠声频谱[2]来模拟高频信号，而特征压缩可能会不可避免地失去频段的重要信息，导致次优性能。
长期以来，基于DNN的语音前端算法只试图增强噪声幅度，而将噪声相位直接用于语音波形重建。其原因可以归结为相位的结构不明确，被认为是具有挑战性的估计。随后，Williamson等人提出了复杂比率掩码（CRM）[3]，它可以通过同时增强噪声语音的实部和虚部来完美重建语音。基于宽带场景的SOTA方法，如SDD-Net[4]和DCCRN[5]，已经显示出出色的性能，特别是对于低信噪比和突发噪声的复杂去噪情况。DCCRN结合了DCUNET[6]和CRN[7]的优点，使用LSTM对时间背景进行建模，可训练的参数和计算成本大大降低。SDD-Net应用功率压缩频谱[8]作为输入特征，并采用了四个特别设计的阶段，极大地提高了同步去杂和去噪的语音质量。对于全波段的情况，RNNoise[2]的研究采用树皮谱，而不是STFT，作为模型的输入。树皮谱在频率轴上完全只有22个维度[9]，这可以大大减少模型的大小，并加快模型推理的速度。树皮谱假设语音和噪声的频谱包络足够平坦[2]。然而，由于真实声学场景的复杂性（如突发噪声和混响），这种方法在真实声学场景中可能导致严重的衰减，从而导致过多的噪声残留。最近，PercepNet[10]提出了一种感知频带表示法，它只对32个三角谱带进行操作，根据等效的entrectangular带宽（ERB）尺度进行间隔[9]。然而，树皮尺度和ERB尺度的分辨率比STFT的线性频谱更粗糙，导致了频带信息的泄漏。最近，超宽频带/全频带信号的语音增强引起了广泛关注&ndash;深度噪声抑制挑战（DNS）[11]特别设立了全频带轨道。
本文提出了超宽频DCCRN（S-DCCRN），用于32K Hz采样率的超宽频场景中的语音增强。这项工作的贡献有三个方面，在Voicebank和Demond数据集上进行客观评估，在DNS-2021盲测集上进行主观评估。
我们提出了两个轻量级的DCCRN子模块，分别用于子频段和全频段（SAF）建模，因为我们认为低频段含有较高的能量，而高频段对主观感受有很大影响[12]。因此，采用子波段处理模块对低频段和高频段分别建模。然而，由于低频和高频成分之间没有明确的信息交互，仅采用子带处理可能会导致频段之间的不平滑连接。因此，我们进一步应用全频段处理模块来平滑不同频段的边界。详细地说，子频带处理模块包括一个子频带DCCRN，它用群复卷积代替原始DCCRN的复卷积，以分别模拟低频段和高频段。全频带和子频带处理模块的编码器和解码器之间的卷积通路[13]被用来进行更好的信息交互，避免全频带的信息丢失。在较小的模型尺寸下，与神谕DCCRN相比，SAF模块导致了0.17的PESQ改进。
受宽带去噪中频谱压缩的启发[8]，我们在模型中引入了可学习频谱压缩（LSC），它可以动态地调整不同频段的能量。LSC的使用使得高频段的模式更加清晰，这种更新带来了0.07的额外PESQ增益。
受DPT-FSNet[14]的编码器/解码器块的启发，我们在STFT之后采用了复杂特征编码器（CFE），在iSTFT之前采用了复杂特征解码器（CFD）。我们保持与大多数宽频语音增强模型相同的STFT点。尽管在高采样率的情况下，频率分辨率相对较低，但CFE块可以细化STFT频谱的不同频段的信息。通过可学习的频谱压缩，这种更新带来了0.07的额外PESQ增益。
所提出的S-DCCRN模型超过了所有测试过的SOTA模型，包括RNNoise和DCCRN，并在Interspeech 2021 DNS挑战赛的盲测集上获得了3.62的MOS分数，表现优异[11]。
2. 超宽带DCCRN 2.1. 复数编解码器 研究人员通常采用Bark频谱作为宽频带场景下全频带语音增强的网络输入，它可以将物理频率转换成基于人类感知的心理声学频率[2]。然而，通过这种转换，原来的物理频段被压缩了，而且相位信息也被丢弃了。此外，基于人类感知的特征可能不适合于网络的输入。另一方面，直接使用STFT特征也会引起一些问题。随着STFT点数的增加，由于高维输入特征难以建模，网络的复杂性会增加。相反，使用点数较少的STFT特征也会造成频率分辨率的下降。本文受DPT-FSNet[14]的编码器/解码器块的启发，在STFT之后采用复杂特征编码器/解码器，基于512维的复杂STFT特征来细化不同频段的信息。
如图2（a）所示，复杂特征编码器（CFE）模块的输入是通过STFT得到的T-F频谱。我们采用核大小为1的复数conv2d来提取高维信息。然后使用扩张的密集块[15]来捕捉时间尺度上的长期背景特征。最后，采用 采用复杂的conv2d来提取复杂的局部特征。在每次卷积之后，都会相继进行LayerNorm和PReLU激活。
如图2（b）所示，复杂特征解码器（CFD）模块的输入是SAF模块输出的实/图像特征。具体来说，我们采用扩展的密集块来处理估计的实/图像特征。然后，扩张密集块的输出被复杂像素卷积处理，用复杂卷积代替像素卷积[16]中的卷积。像素卷积被认为是替代转置卷积的更好方法，以避免棋盘式伪影[17]。最后，进行复杂卷积，将高维特征还原到时频域。如图2（c）所示，每个密集块由五层conv2d组成。各帧之间的卷积是因果关系。与之前所有层的密集连接避免了梯度消失的问题[15]。
2.2. 子带和全带处理模块 随着采样率的增加，频段的数量也在很大程度上增加。由于低频和高频之间的信息有很大的不同，不同的频段很难只通过全频段处理来建模[12]。在一个模块中对它们进行建模并不是最佳选择。另一方面，由于不同频段之间没有信息交互，子频段处理会在不同频段的边界造成一定的不平滑连接。基于上述考虑，我们提出了一个子波段和全波段处理（SAF）模块，以发挥两者的优势。
如图1所示，我们使用来自CFE的编码特征作为SAF模块的输入，该模块主要由子带DCCRN和全带DCCRN组成。在SAF模块中，特征首先被一个子带DCCRN处理。子带DCCRN的输出与来自CFE的编码特征（被认为有助于平滑频带）的连接被视为全带DCCRN的输入。全波段DCCRN的输出是来自CFE的编码特征的复数比率掩码（CRM）。
子波段DCCRN的结构如图3所示。子波段DCCRN的总体设计与神谕DCCRN相似，但神谕DCCRN中的复杂卷积块被复杂群卷积块所取代。如图4所示，复杂组卷积块的目的是对低频段和高频段分别建模。此外，为了从每个编码器层聚集更丰富的信息，并缓解各频段之间的不平滑连接，我们采用了编码器和解码器之间的卷积通路块，这在DCCRN&#43;[13]中被证明是有用的。具体来说，编码器和解码器之间的卷积通路由一个复杂的卷积块和批量归一化组成。
在将来自CFE的编码特征与子带DCCRN的输出相连接后，我们使用另一个全带DCCRN，它采取普通复数卷积而不是复数组卷积，以进一步再生和平滑不同的频段。全波段DCCRN还采用了编码器和解码器之间的卷积途径，以实现更好的信息交互。
2.3. 可学习的频谱压缩 有学者指出，频谱中的局部模式在每个频段中往往是不同的：低频段往往包含高能量、音调以及长时间持续的声音，而高频段则可能有低能量成分、噪声和快速衰减的声音[12]。最近，关于宽频带去噪的频谱压缩显示了有希望的结果，它可以通过0.5的压缩率增加高频带的能量[8]。
我们认为，频段的压缩率应该是不同的，因为高频率的频段可能需要较低的压缩率来保持其高能量。这激发了我们开发一个可学习的频谱压缩模块，使用一组网络层来压缩STFT频谱。在可学习的网络层之后进行sigmoid激活，目的是将输出压缩到0∼1。
详细来说，可学习频谱压缩可以描述为 YLSC = |Y |α ejjY (1) 其中Y和α分别表示噪声频谱和可学习参数。
2.4. 损失函数 对于学习目标，我们首先应用SI-SNR[18]损失，它是一个时域损失函数。此外，我们采用复杂的均方误差（MSE）损失和Kullback-Leibler Divergence[19]来提高估计频谱和复杂域中的清洁频谱之间的相似性。KL Divergence的目的是从概率分布的角度来优化清洁频谱和估计频谱。这三种损失是通过以下方式共同优化的
3. 实验 3.1. 数据集 我们对32K采样率的音频样本进行了语音增强实验。我们首先在Voice Bank和DEMAND数据集[20]上进行消融实验，以证明每个提议的子模块的有效性。具体来说，源语音来自VoiceBank语料库[21]，其中包含28个扬声器用于训练，另外2个扬声器用于测试。10种噪声类型，其中2种是人工生成的，8种是来自DEMAND[22]的真实录音，用于训练。请注意，所有的数据在实验前都从48K降频到32K Hz。总的来说，固定的训练和验证集分别包含11,572个语料（10小时）和872个语料（30分钟）。我们还在这个数据集上将其他SOTA模型（包括PercepNet[10]）与S-DCCRN进行比较。 然后，S-DCCRN被进一步训练，并用Interspeech 2021 DNS挑战数据集进行评估，以显示其在更复杂和真实的声学场景中的性能。源语音数据来自DNS-2021全频段数据集，其中包含672小时的语音数据。180小时的DNS-2021噪声集，包括来自150个噪声类别的65000个噪声片段，被选作源噪声数据。训练集包含605小时源语音数据，而验证集分别包含67小时源语音数据。训练数据是以32K Hz的采样率即时生成的，并在一个批次中被分割成8 s的小块，信噪比范围为-5至20 dB。经过14个历时的训练，该模型 &quot;看到 &quot;的总数据超过9000小时。  3."/>

    <meta property="og:title" content="56-S-DCCRN论文阅读" />
<meta property="og:description" content="在语音增强方面，复杂的神经网络由于其在处理复值频谱方面的有效性而显示出良好的性能。最近的大多数语音增强方法主要集中在采样率为16K Hz的宽频信号上。然而，由于难以对更多的频段，特别是高频成分进行建模，因此仍然缺乏对超宽频段（如32K Hz）甚至全频段（48K）去噪的研究。在本文中，我们将之前的深度复杂卷积递归神经网络（DCCRN）大幅扩展为超宽频带版本&ndash;S-DCCRN，以对32K Hz采样率的语音进行去噪。
具体操作：
 我们首先采用了一个级联的子带和全带处理模块，它由两个小尺寸的DCCRN组成&ndash;一个操作子带信号，一个操作全带信号，旨在从局部和全局频率信息中受益。此外，我们没有简单地采用STFT特征作为输入，而是使用了一个以端到端方式训练的复杂特征编码器来完善不同频段的信息。 我们还使用了一个复杂的特征解码器来将特征还原到时频域。最后，采用一种可学习的频谱压缩方法来调整不同频段的能量，这有利于神经网络的学习。   所提出的模型S-DCCRN已经超过了PercepNet以及几个有竞争力的模型，在语音质量和可懂度方面达到了最先进的性能。消融研究也证明了不同贡献的有效性。   @TOC
1. 引言 随着电话会议和其他实时语音通信场景的快速发展，对高质量高保真语音的需求急剧增加。随着采样率的提高，语音将包含更丰富的信息和更精细的细节，特别是在更高的频段。然而，目前大多数基于深度学习的语音增强方法主要关注采样率为16K Hz的宽频信号。在超宽频[1]甚至全频段信号上进行语音增强的潜力仍有待探索，因为在对更多频段尤其是高频成分进行建模方面存在着挑战。此外，用更大维度的特征进行建模将导致更高的建模复杂性，使实时实施变得更加困难。一些语音增强器采用了压缩的特征，如吠声频谱[2]来模拟高频信号，而特征压缩可能会不可避免地失去频段的重要信息，导致次优性能。
长期以来，基于DNN的语音前端算法只试图增强噪声幅度，而将噪声相位直接用于语音波形重建。其原因可以归结为相位的结构不明确，被认为是具有挑战性的估计。随后，Williamson等人提出了复杂比率掩码（CRM）[3]，它可以通过同时增强噪声语音的实部和虚部来完美重建语音。基于宽带场景的SOTA方法，如SDD-Net[4]和DCCRN[5]，已经显示出出色的性能，特别是对于低信噪比和突发噪声的复杂去噪情况。DCCRN结合了DCUNET[6]和CRN[7]的优点，使用LSTM对时间背景进行建模，可训练的参数和计算成本大大降低。SDD-Net应用功率压缩频谱[8]作为输入特征，并采用了四个特别设计的阶段，极大地提高了同步去杂和去噪的语音质量。对于全波段的情况，RNNoise[2]的研究采用树皮谱，而不是STFT，作为模型的输入。树皮谱在频率轴上完全只有22个维度[9]，这可以大大减少模型的大小，并加快模型推理的速度。树皮谱假设语音和噪声的频谱包络足够平坦[2]。然而，由于真实声学场景的复杂性（如突发噪声和混响），这种方法在真实声学场景中可能导致严重的衰减，从而导致过多的噪声残留。最近，PercepNet[10]提出了一种感知频带表示法，它只对32个三角谱带进行操作，根据等效的entrectangular带宽（ERB）尺度进行间隔[9]。然而，树皮尺度和ERB尺度的分辨率比STFT的线性频谱更粗糙，导致了频带信息的泄漏。最近，超宽频带/全频带信号的语音增强引起了广泛关注&ndash;深度噪声抑制挑战（DNS）[11]特别设立了全频带轨道。
本文提出了超宽频DCCRN（S-DCCRN），用于32K Hz采样率的超宽频场景中的语音增强。这项工作的贡献有三个方面，在Voicebank和Demond数据集上进行客观评估，在DNS-2021盲测集上进行主观评估。
我们提出了两个轻量级的DCCRN子模块，分别用于子频段和全频段（SAF）建模，因为我们认为低频段含有较高的能量，而高频段对主观感受有很大影响[12]。因此，采用子波段处理模块对低频段和高频段分别建模。然而，由于低频和高频成分之间没有明确的信息交互，仅采用子带处理可能会导致频段之间的不平滑连接。因此，我们进一步应用全频段处理模块来平滑不同频段的边界。详细地说，子频带处理模块包括一个子频带DCCRN，它用群复卷积代替原始DCCRN的复卷积，以分别模拟低频段和高频段。全频带和子频带处理模块的编码器和解码器之间的卷积通路[13]被用来进行更好的信息交互，避免全频带的信息丢失。在较小的模型尺寸下，与神谕DCCRN相比，SAF模块导致了0.17的PESQ改进。
受宽带去噪中频谱压缩的启发[8]，我们在模型中引入了可学习频谱压缩（LSC），它可以动态地调整不同频段的能量。LSC的使用使得高频段的模式更加清晰，这种更新带来了0.07的额外PESQ增益。
受DPT-FSNet[14]的编码器/解码器块的启发，我们在STFT之后采用了复杂特征编码器（CFE），在iSTFT之前采用了复杂特征解码器（CFD）。我们保持与大多数宽频语音增强模型相同的STFT点。尽管在高采样率的情况下，频率分辨率相对较低，但CFE块可以细化STFT频谱的不同频段的信息。通过可学习的频谱压缩，这种更新带来了0.07的额外PESQ增益。
所提出的S-DCCRN模型超过了所有测试过的SOTA模型，包括RNNoise和DCCRN，并在Interspeech 2021 DNS挑战赛的盲测集上获得了3.62的MOS分数，表现优异[11]。
2. 超宽带DCCRN 2.1. 复数编解码器 研究人员通常采用Bark频谱作为宽频带场景下全频带语音增强的网络输入，它可以将物理频率转换成基于人类感知的心理声学频率[2]。然而，通过这种转换，原来的物理频段被压缩了，而且相位信息也被丢弃了。此外，基于人类感知的特征可能不适合于网络的输入。另一方面，直接使用STFT特征也会引起一些问题。随着STFT点数的增加，由于高维输入特征难以建模，网络的复杂性会增加。相反，使用点数较少的STFT特征也会造成频率分辨率的下降。本文受DPT-FSNet[14]的编码器/解码器块的启发，在STFT之后采用复杂特征编码器/解码器，基于512维的复杂STFT特征来细化不同频段的信息。
如图2（a）所示，复杂特征编码器（CFE）模块的输入是通过STFT得到的T-F频谱。我们采用核大小为1的复数conv2d来提取高维信息。然后使用扩张的密集块[15]来捕捉时间尺度上的长期背景特征。最后，采用 采用复杂的conv2d来提取复杂的局部特征。在每次卷积之后，都会相继进行LayerNorm和PReLU激活。
如图2（b）所示，复杂特征解码器（CFD）模块的输入是SAF模块输出的实/图像特征。具体来说，我们采用扩展的密集块来处理估计的实/图像特征。然后，扩张密集块的输出被复杂像素卷积处理，用复杂卷积代替像素卷积[16]中的卷积。像素卷积被认为是替代转置卷积的更好方法，以避免棋盘式伪影[17]。最后，进行复杂卷积，将高维特征还原到时频域。如图2（c）所示，每个密集块由五层conv2d组成。各帧之间的卷积是因果关系。与之前所有层的密集连接避免了梯度消失的问题[15]。
2.2. 子带和全带处理模块 随着采样率的增加，频段的数量也在很大程度上增加。由于低频和高频之间的信息有很大的不同，不同的频段很难只通过全频段处理来建模[12]。在一个模块中对它们进行建模并不是最佳选择。另一方面，由于不同频段之间没有信息交互，子频段处理会在不同频段的边界造成一定的不平滑连接。基于上述考虑，我们提出了一个子波段和全波段处理（SAF）模块，以发挥两者的优势。
如图1所示，我们使用来自CFE的编码特征作为SAF模块的输入，该模块主要由子带DCCRN和全带DCCRN组成。在SAF模块中，特征首先被一个子带DCCRN处理。子带DCCRN的输出与来自CFE的编码特征（被认为有助于平滑频带）的连接被视为全带DCCRN的输入。全波段DCCRN的输出是来自CFE的编码特征的复数比率掩码（CRM）。
子波段DCCRN的结构如图3所示。子波段DCCRN的总体设计与神谕DCCRN相似，但神谕DCCRN中的复杂卷积块被复杂群卷积块所取代。如图4所示，复杂组卷积块的目的是对低频段和高频段分别建模。此外，为了从每个编码器层聚集更丰富的信息，并缓解各频段之间的不平滑连接，我们采用了编码器和解码器之间的卷积通路块，这在DCCRN&#43;[13]中被证明是有用的。具体来说，编码器和解码器之间的卷积通路由一个复杂的卷积块和批量归一化组成。
在将来自CFE的编码特征与子带DCCRN的输出相连接后，我们使用另一个全带DCCRN，它采取普通复数卷积而不是复数组卷积，以进一步再生和平滑不同的频段。全波段DCCRN还采用了编码器和解码器之间的卷积途径，以实现更好的信息交互。
2.3. 可学习的频谱压缩 有学者指出，频谱中的局部模式在每个频段中往往是不同的：低频段往往包含高能量、音调以及长时间持续的声音，而高频段则可能有低能量成分、噪声和快速衰减的声音[12]。最近，关于宽频带去噪的频谱压缩显示了有希望的结果，它可以通过0.5的压缩率增加高频带的能量[8]。
我们认为，频段的压缩率应该是不同的，因为高频率的频段可能需要较低的压缩率来保持其高能量。这激发了我们开发一个可学习的频谱压缩模块，使用一组网络层来压缩STFT频谱。在可学习的网络层之后进行sigmoid激活，目的是将输出压缩到0∼1。
详细来说，可学习频谱压缩可以描述为 YLSC = |Y |α ejjY (1) 其中Y和α分别表示噪声频谱和可学习参数。
2.4. 损失函数 对于学习目标，我们首先应用SI-SNR[18]损失，它是一个时域损失函数。此外，我们采用复杂的均方误差（MSE）损失和Kullback-Leibler Divergence[19]来提高估计频谱和复杂域中的清洁频谱之间的相似性。KL Divergence的目的是从概率分布的角度来优化清洁频谱和估计频谱。这三种损失是通过以下方式共同优化的
3. 实验 3.1. 数据集 我们对32K采样率的音频样本进行了语音增强实验。我们首先在Voice Bank和DEMAND数据集[20]上进行消融实验，以证明每个提议的子模块的有效性。具体来说，源语音来自VoiceBank语料库[21]，其中包含28个扬声器用于训练，另外2个扬声器用于测试。10种噪声类型，其中2种是人工生成的，8种是来自DEMAND[22]的真实录音，用于训练。请注意，所有的数据在实验前都从48K降频到32K Hz。总的来说，固定的训练和验证集分别包含11,572个语料（10小时）和872个语料（30分钟）。我们还在这个数据集上将其他SOTA模型（包括PercepNet[10]）与S-DCCRN进行比较。 然后，S-DCCRN被进一步训练，并用Interspeech 2021 DNS挑战数据集进行评估，以显示其在更复杂和真实的声学场景中的性能。源语音数据来自DNS-2021全频段数据集，其中包含672小时的语音数据。180小时的DNS-2021噪声集，包括来自150个噪声类别的65000个噪声片段，被选作源噪声数据。训练集包含605小时源语音数据，而验证集分别包含67小时源语音数据。训练数据是以32K Hz的采样率即时生成的，并在一个批次中被分割成8 s的小块，信噪比范围为-5至20 dB。经过14个历时的训练，该模型 &quot;看到 &quot;的总数据超过9000小时。  3." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ioyy900205.github.io/post/2021-12-28-56-s-dccrn%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-12-28T15:32:05+08:00" />
<meta property="article:modified_time" content="2021-12-28T15:32:05+08:00" />



  </head>
  <body>
    <header class="app-header">
      <a href="https://ioyy900205.github.io"><img class="app-header-avatar" src="https://img95.699pic.com/element/40141/6992.png_300.png" alt="Liu Liang" /></a>
      <h1>亮的笔记</h1>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/">Home</a>
             - 
          
          <a class="app-header-menu-item" href="/tags/">Tags</a>
             - 
          
          <a class="app-header-menu-item" href="/about/">About</a>
      </nav>
      <p>深度学习笔记本</p>
      <div class="app-header-social">
        
          <a href="https://github.com/ioyy900205" target="_blank" rel="noreferrer noopener">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>Github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg>
          </a>
        
          <a href="https://twitter.com/ioyy900205" target="_blank" rel="noreferrer noopener">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-twitter">
  <title>Twitter</title>
  <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path>
</svg>
          </a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">56-S-DCCRN论文阅读</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Dec 28, 2021
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          1 min read
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7.01" y2="7"></line>
</svg>
              <a class="tag" href="https://ioyy900205.github.io/tags/paper_review/">Paper_Review</a>
        </div>
      </div>
    </header>
    <div class="post-content">
      <p>在语音增强方面，复杂的神经网络由于其在处理复值频谱方面的有效性而显示出良好的性能。最近的大多数语音增强方法主要集中在采样率为16K Hz的宽频信号上。然而，由于难以对更多的频段，特别是高频成分进行建模，因此仍然缺乏对超宽频段（如32K Hz）甚至全频段（48K）去噪的研究。在本文中，我们将之前的深度复杂卷积递归神经网络（DCCRN）大幅扩展为超宽频带版本&ndash;S-DCCRN，以对32K Hz采样率的语音进行去噪。</p>
<p>具体操作：</p>
<ol>
<li>我们首先采用了一个级联的子带和全带处理模块，它由两个小尺寸的DCCRN组成&ndash;一个操作子带信号，一个操作全带信号，旨在从局部和全局频率信息中受益。此外，我们没有简单地采用STFT特征作为输入，而是使用了一个以端到端方式训练的复杂特征编码器来完善不同频段的信息。</li>
<li>我们还使用了一个复杂的特征解码器来将特征还原到时频域。最后，采用一种可学习的频谱压缩方法来调整不同频段的能量，这有利于神经网络的学习。</li>
</ol>
<ul>
<li>所提出的模型S-DCCRN已经超过了PercepNet以及几个有竞争力的模型，在语音质量和可懂度方面达到了最先进的性能。消融研究也证明了不同贡献的有效性。</li>
</ul>
<hr>
<p>@<a href="%E6%9C%AC%E6%96%87%E5%86%85%E5%AE%B9">TOC</a></p>
<h2 id="1-引言">1. 引言</h2>
<p>随着电话会议和其他实时语音通信场景的快速发展，对高质量高保真语音的需求急剧增加。随着采样率的提高，语音将包含更丰富的信息和更精细的细节，特别是在更高的频段。然而，目前大多数基于深度学习的语音增强方法主要关注采样率为16K Hz的宽频信号。在超宽频[1]甚至全频段信号上进行语音增强的潜力仍有待探索，因为在对更多频段尤其是高频成分进行建模方面存在着挑战。此外，用更大维度的特征进行建模将导致更高的建模复杂性，使实时实施变得更加困难。一些语音增强器采用了压缩的特征，如吠声频谱[2]来模拟高频信号，而特征压缩可能会不可避免地失去频段的重要信息，导致次优性能。</p>
<p>长期以来，基于DNN的语音前端算法只试图增强噪声幅度，而将噪声相位直接用于语音波形重建。其原因可以归结为相位的结构不明确，被认为是具有挑战性的估计。随后，Williamson等人提出了复杂比率掩码（CRM）[3]，它可以通过同时增强噪声语音的实部和虚部来完美重建语音。基于宽带场景的SOTA方法，如SDD-Net[4]和DCCRN[5]，已经显示出出色的性能，特别是对于低信噪比和突发噪声的复杂去噪情况。DCCRN结合了DCUNET[6]和CRN[7]的优点，使用LSTM对时间背景进行建模，可训练的参数和计算成本大大降低。SDD-Net应用功率压缩频谱[8]作为输入特征，并采用了四个特别设计的阶段，极大地提高了同步去杂和去噪的语音质量。对于全波段的情况，RNNoise[2]的研究采用树皮谱，而不是STFT，作为模型的输入。树皮谱在频率轴上完全只有22个维度[9]，这可以大大减少模型的大小，并加快模型推理的速度。树皮谱假设语音和噪声的频谱包络足够平坦[2]。然而，由于真实声学场景的复杂性（如突发噪声和混响），这种方法在真实声学场景中可能导致严重的衰减，从而导致过多的噪声残留。最近，PercepNet[10]提出了一种感知频带表示法，它只对32个三角谱带进行操作，根据等效的entrectangular带宽（ERB）尺度进行间隔[9]。然而，树皮尺度和ERB尺度的分辨率比STFT的线性频谱更粗糙，导致了频带信息的泄漏。最近，超宽频带/全频带信号的语音增强引起了广泛关注&ndash;深度噪声抑制挑战（DNS）[11]特别设立了全频带轨道。</p>
<p>本文提出了超宽频DCCRN（S-DCCRN），用于32K Hz采样率的超宽频场景中的语音增强。这项工作的贡献有三个方面，在Voicebank和Demond数据集上进行客观评估，在DNS-2021盲测集上进行主观评估。</p>
<p>我们提出了两个轻量级的DCCRN子模块，分别用于子频段和全频段（SAF）建模，因为我们认为低频段含有较高的能量，而高频段对主观感受有很大影响[12]。因此，采用子波段处理模块对低频段和高频段分别建模。然而，由于低频和高频成分之间没有明确的信息交互，仅采用子带处理可能会导致频段之间的不平滑连接。因此，我们进一步应用全频段处理模块来平滑不同频段的边界。详细地说，子频带处理模块包括一个子频带DCCRN，它用群复卷积代替原始DCCRN的复卷积，以分别模拟低频段和高频段。全频带和子频带处理模块的编码器和解码器之间的卷积通路[13]被用来进行更好的信息交互，避免全频带的信息丢失。在较小的模型尺寸下，与神谕DCCRN相比，SAF模块导致了0.17的PESQ改进。</p>
<p>受宽带去噪中频谱压缩的启发[8]，我们在模型中引入了可学习频谱压缩（LSC），它可以动态地调整不同频段的能量。LSC的使用使得高频段的模式更加清晰，这种更新带来了0.07的额外PESQ增益。</p>
<p>受DPT-FSNet[14]的编码器/解码器块的启发，我们在STFT之后采用了复杂特征编码器（CFE），在iSTFT之前采用了复杂特征解码器（CFD）。我们保持与大多数宽频语音增强模型相同的STFT点。尽管在高采样率的情况下，频率分辨率相对较低，但CFE块可以细化STFT频谱的不同频段的信息。通过可学习的频谱压缩，这种更新带来了0.07的额外PESQ增益。</p>
<p>所提出的S-DCCRN模型超过了所有测试过的SOTA模型，包括RNNoise和DCCRN，并在Interspeech 2021 DNS挑战赛的盲测集上获得了3.62的MOS分数，表现优异[11]。</p>
<h2 id="2-超宽带dccrn">2. 超宽带DCCRN</h2>
<h3 id="21-复数编解码器">2.1. 复数编解码器</h3>
<p>研究人员通常采用Bark频谱作为宽频带场景下全频带语音增强的网络输入，它可以将物理频率转换成基于人类感知的心理声学频率[2]。然而，通过这种转换，原来的物理频段被压缩了，而且相位信息也被丢弃了。此外，基于人类感知的特征可能不适合于网络的输入。另一方面，直接使用STFT特征也会引起一些问题。随着STFT点数的增加，由于高维输入特征难以建模，网络的复杂性会增加。相反，使用点数较少的STFT特征也会造成频率分辨率的下降。本文受DPT-FSNet[14]的编码器/解码器块的启发，在STFT之后采用复杂特征编码器/解码器，基于512维的复杂STFT特征来细化不同频段的信息。</p>
<p>如图2（a）所示，复杂特征编码器（CFE）模块的输入是通过STFT得到的T-F频谱。我们采用核大小为1的复数conv2d来提取高维信息。然后使用扩张的密集块[15]来捕捉时间尺度上的长期背景特征。最后，采用
采用复杂的conv2d来提取复杂的局部特征。在每次卷积之后，都会相继进行LayerNorm和PReLU激活。</p>
<p>如图2（b）所示，复杂特征解码器（CFD）模块的输入是SAF模块输出的实/图像特征。具体来说，我们采用扩展的密集块来处理估计的实/图像特征。然后，扩张密集块的输出被复杂像素卷积处理，用复杂卷积代替像素卷积[16]中的卷积。像素卷积被认为是替代转置卷积的更好方法，以避免棋盘式伪影[17]。最后，进行复杂卷积，将高维特征还原到时频域。如图2（c）所示，每个密集块由五层conv2d组成。各帧之间的卷积是因果关系。与之前所有层的密集连接避免了梯度消失的问题[15]。</p>
<h3 id="22-子带和全带处理模块">2.2. 子带和全带处理模块</h3>
<p>随着采样率的增加，频段的数量也在很大程度上增加。由于低频和高频之间的信息有很大的不同，不同的频段很难只通过全频段处理来建模[12]。在一个模块中对它们进行建模并不是最佳选择。另一方面，由于不同频段之间没有信息交互，子频段处理会在不同频段的边界造成一定的不平滑连接。基于上述考虑，我们提出了一个子波段和全波段处理（SAF）模块，以发挥两者的优势。</p>
<p>如图1所示，我们使用来自CFE的编码特征作为SAF模块的输入，该模块主要由子带DCCRN和全带DCCRN组成。在SAF模块中，特征首先被一个子带DCCRN处理。子带DCCRN的输出与来自CFE的编码特征（被认为有助于平滑频带）的连接被视为全带DCCRN的输入。全波段DCCRN的输出是来自CFE的编码特征的复数比率掩码（CRM）。</p>
<p>子波段DCCRN的结构如图3所示。子波段DCCRN的总体设计与神谕DCCRN相似，但神谕DCCRN中的复杂卷积块被复杂群卷积块所取代。如图4所示，复杂组卷积块的目的是对低频段和高频段分别建模。此外，为了从每个编码器层聚集更丰富的信息，并缓解各频段之间的不平滑连接，我们采用了编码器和解码器之间的卷积通路块，这在DCCRN+[13]中被证明是有用的。具体来说，编码器和解码器之间的卷积通路由一个复杂的卷积块和批量归一化组成。</p>
<p>在将来自CFE的编码特征与子带DCCRN的输出相连接后，我们使用另一个全带DCCRN，它采取普通复数卷积而不是复数组卷积，以进一步再生和平滑不同的频段。全波段DCCRN还采用了编码器和解码器之间的卷积途径，以实现更好的信息交互。</p>
<h3 id="23-可学习的频谱压缩">2.3. 可学习的频谱压缩</h3>
<p>有学者指出，频谱中的局部模式在每个频段中往往是不同的：低频段往往包含高能量、音调以及长时间持续的声音，而高频段则可能有低能量成分、噪声和快速衰减的声音[12]。最近，关于宽频带去噪的频谱压缩显示了有希望的结果，它可以通过0.5的压缩率增加高频带的能量[8]。</p>
<p>我们认为，频段的压缩率应该是不同的，因为高频率的频段可能需要较低的压缩率来保持其高能量。这激发了我们开发一个可学习的频谱压缩模块，使用一组网络层来压缩STFT频谱。在可学习的网络层之后进行sigmoid激活，目的是将输出压缩到0∼1。</p>
<p>详细来说，可学习频谱压缩可以描述为
YLSC = |Y |α ejjY (1)
其中Y和α分别表示噪声频谱和可学习参数。</p>
<h3 id="24-损失函数">2.4. 损失函数</h3>
<p>对于学习目标，我们首先应用SI-SNR[18]损失，它是一个时域损失函数。此外，我们采用复杂的均方误差（MSE）损失和Kullback-Leibler Divergence[19]来提高估计频谱和复杂域中的清洁频谱之间的相似性。KL Divergence的目的是从概率分布的角度来优化清洁频谱和估计频谱。这三种损失是通过以下方式共同优化的</p>
<h2 id="3-实验">3. 实验</h2>
<h3 id="31-数据集">3.1. 数据集</h3>
<pre><code>我们对32K采样率的音频样本进行了语音增强实验。我们首先在Voice Bank和DEMAND数据集[20]上进行消融实验，以证明每个提议的子模块的有效性。具体来说，源语音来自VoiceBank语料库[21]，其中包含28个扬声器用于训练，另外2个扬声器用于测试。10种噪声类型，其中2种是人工生成的，8种是来自DEMAND[22]的真实录音，用于训练。请注意，所有的数据在实验前都从48K降频到32K Hz。总的来说，固定的训练和验证集分别包含11,572个语料（10小时）和872个语料（30分钟）。我们还在这个数据集上将其他SOTA模型（包括PercepNet[10]）与S-DCCRN进行比较。

然后，S-DCCRN被进一步训练，并用Interspeech 2021 DNS挑战数据集进行评估，以显示其在更复杂和真实的声学场景中的性能。源语音数据来自DNS-2021全频段数据集，其中包含672小时的语音数据。180小时的DNS-2021噪声集，包括来自150个噪声类别的65000个噪声片段，被选作源噪声数据。训练集包含605小时源语音数据，而验证集分别包含67小时源语音数据。训练数据是以32K Hz的采样率即时生成的，并在一个批次中被分割成8 s的小块，信噪比范围为-5至20 dB。经过14个历时的训练，该模型 &quot;看到 &quot;的总数据超过9000小时。
</code></pre>
<h3 id="32-训练参数设置和基线">3.2. 训练参数设置和基线</h3>
<pre><code>对于所提出的模型，窗长和帧移分别为15ms和5ms，导致模型的运行时间为20ms。STFT的长度为512。对于在语音库和DEMAND数据集上训练的模型，所有的模型都训练了36个历时，学习率安排如下：

在前10个历时中使用0.000025的约束性学习率进行预热；然后学习率被重置为0.001。对于在DNS-2021数据集上训练的模型，初始学习率为0.001，如果验证集上没有损失减少，学习率将减半。我们还将提议的S-DCCRN模型及其在语音库和DEMAND数据集上的消融组件与其他SOTA模型进行比较。它们的描述如下。

DCCRN：DCCRN的通道数为{16,32,64,128,256,256}，卷积核和步长分别设置为（5,2）和（2,1）。采用两个LSTM层，节点数为256。在LSTM之后有一个1024×256的全连接层。每个编码器/解码器模块处理当前帧和一个前帧。

S-DCCRN：子带DCCRN的通道数为{32,64,64,64,128,128}，卷积核和步长分别设为（5,2）和（2,1）。此外，全DCCRN第一层的通道数为64。子波段DCCRN和全波段DCCRN分别采用一个LSTM层，节点数为256。在LSTM之后有一个256×256的全连接层。每个编码器/解码器模块处理当前帧和一个前帧。复杂特征编码器/解码器模块的隐藏通道为32个，DenseBlock的深度为5。除了CFD模块的最后一层外，每次卷积后都进行LayerNorm和PReLU。
</code></pre>
<h3 id="33-实验结果和讨论">3.3. 实验结果和讨论</h3>
<pre><code>如表1所示，进行了消融研究，以评估S-DCCRN不同模型组件的有效性，包括a）子带处理模块（SP），b）子带和全带处理模块（SAF），c）没有复杂特征编码器/解码器模块（CFE/CFD）的S-DCCRN，d）没有CFE/CFD和用频谱压缩（SC）替代可学习频谱压缩（LSC）的S-DCCRN，e）用LSC代替SC。应该注意的是，SP只包括一个子带DC-CRN，因此，SP的数量是有限的。CRN，因此编码器和解码器的通道数为{2,64,64,128,128,256}。SP的LSTM层和单元分别为2和256。我们采用CSIG、COVL、CBAK[23]、STOI[24]和PESQ[25]作为五个评价指标。
</code></pre>
<hr>
<p><strong>参考资料</strong></p>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
