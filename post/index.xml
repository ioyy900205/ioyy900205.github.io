<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 亮的笔记</title>
    <link>https://ioyy900205.github.io/post/</link>
    <description>Recent content in Posts on 亮的笔记</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 27 Apr 2021 09:43:30 +0800</lastBuildDate><atom:link href="https://ioyy900205.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>03——nn.module学习</title>
      <link>https://ioyy900205.github.io/post/nn.module/</link>
      <pubDate>Tue, 27 Apr 2021 09:43:30 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/nn.module/</guid>
      <description>1. nn.moduel初步  1.1. Linear类 1.2. Conv2d类 1.3. 自定义层的步骤   2. 自定义层  2.1. 定义一个自定义层MyLayer 2.2. 自定义模型并且训练    1. nn.moduel初步 keras更加注重的是层Layer、pytorch更加注重的是模型Module。
1.1. Linear类 import math import torch from torch.nn.parameter import Parameter from .. import functional as F from .. import init from .module import Module from ..._jit_internal import weak_module, weak_script_method class Linear(Module): __constants__ = [&amp;#39;bias&amp;#39;] def __init__(self, in_features, out_features, bias=True): super(Linear, self).__init__() self.in_features = in_features self.out_features = out_features self.</description>
    </item>
    
    <item>
      <title>01——ResNeXt学习</title>
      <link>https://ioyy900205.github.io/post/resnext/</link>
      <pubDate>Mon, 26 Apr 2021 17:05:30 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/resnext/</guid>
      <description>1. ResNeXt  1.1. 概述 1.2. 思路 1.3. Block 1.4. 组卷积 1.5. 模型复杂度 1.6. Shortcut 1.7. 结果    1. ResNeXt 1.1. 概述 论文：Aggregated Residual Transformations for Deep Neural Networks
论文链接：https://arxiv.org/abs/1611.05431
PyTorch代码：https://github.com/miraclewkf/ResNeXt-PyTorch 2016年,ISCLVCR 2016 no.2
1.2. 思路 Split-Transform-Merge （来源于inception）
堆叠（来源于VGG）
1.3. Block cardinality ，原文的解释是the size of the set of transformations，如上图右边是 cardinality=32 的样子，这里注意每个被聚合的拓扑结构都是一样的(这也是和 Inception 的差别，减轻设计负担)
1.4. 组卷积 最早可以追溯到AlexNet。
32x4d 中 32为组卷积数目，4d为每组卷积4个卷积核。
组卷积可以有不同的配置，但是不同的配置需要通过实验判断效果。
可以发现通过组卷积能够有效降低parameter的大小。
1.5. 模型复杂度 如果想增加模型复杂度，几个选择：
1）宽度；
2）深度；
3）cardinality。</description>
    </item>
    
    <item>
      <title>02——SeNet学习</title>
      <link>https://ioyy900205.github.io/post/senet/</link>
      <pubDate>Mon, 26 Apr 2021 17:05:30 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/senet/</guid>
      <description>1. SENET  1.1. 链接 1.2. 贡献  1.2.1. 提供了子结构 1.2.2. SOTA   1.3. 核心思想 1.4. Squeeze 1.5. Excitation   2. 思考  2.1. 浅层作用较大    1. SENET 1.1. 链接 论文：Squeeze-and-Excitation Networks论文链接：https://arxiv.org/abs/1709.01507代码地址：https://github.com/hujie-frank/SENetPyTorch代码地址：https://github.com/miraclewkf/SENet-PyTorch
A central theme of computer vision research is the search for more powerful representations that capture only those properties of an image that are most salient for a given task
1.2. 贡献 1.2.1. 提供了子结构 Sequeeze-and-Excitation(SE) block并不是一个完整的网络结构，而是一个子结构，可以嵌到其他分类或检测模型中。</description>
    </item>
    
  </channel>
</rss>
