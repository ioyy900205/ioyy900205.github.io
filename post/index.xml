<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 亮的笔记</title>
    <link>https://ioyy900205.github.io/post/</link>
    <description>Recent content in Posts on 亮的笔记</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 31 Aug 2023 11:28:02 +0800</lastBuildDate><atom:link href="https://ioyy900205.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>60-调研——Visual Audio Speech Enhancement</title>
      <link>https://ioyy900205.github.io/post/2023-08-31-%E8%B0%83%E7%A0%94vase/</link>
      <pubDate>Thu, 31 Aug 2023 11:28:02 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2023-08-31-%E8%B0%83%E7%A0%94vase/</guid>
      <description>在此处编辑 blog.walterlv.com 的博客摘要
  1 论文调研  1.1 ICASSP 2023  Audio for Multimedia and Multimodal Processing Human Identification and Face Recognition Learning from Multimodal Data ASR: Noise Robustness Keyword Spotting   1.2 INTERSPEECH 2023  Resources for Spoken Language Processing Analysis of Speech and Audio Signals Speech Recognition: Technologies and Systems for New Applications Spoken Dialog Systems and Conversational Analysis Speech Coding and Enhancement Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources, and Evaluation Speech, Voice, and Hearing Disorders Source Separation Speaker and Language Identification Speaker Recognition Speech Perception, Production, and Acquisition Multi-modal Systems Multi-talker Methods in Speech Processing New Computational Strategies for ASR Training and Inference Dialog Management Show and Tell: Health Applications and Emotion Recognition Show and Tell: Speech Tools, Speech Enhancement, Speech Synthesis     2 基于论文调研的分析  2.</description>
    </item>
    
    <item>
      <title>59-时频通道双重注意力</title>
      <link>https://ioyy900205.github.io/post/2022-02-14-59-duality-temporal-channel-frequency-attention-enhanced-speaker-representation-learning/</link>
      <pubDate>Mon, 14 Feb 2022 14:43:42 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2022-02-14-59-duality-temporal-channel-frequency-attention-enhanced-speaker-representation-learning/</guid>
      <description>水一篇论文。。
 @TOC
标题  参考资料 @TOC
谢磊老师团队作品 2021.12
Audio, Speech and Language Processing Group (ASLP@NPU), School of Computer Science, Northwestern Polytechnical University, Xi’an, China
标题：时间-通道-频率的双重性注意增强说话人的表征学习
简单来说就是将图像中的CBAM重新拆分整合了一下。
AI Labs - Trident &amp;gt; 7. DUALITY TEMPORAL-CHANNEL-FREQUENCY ATTENTION ENHANCED SPEAKER REPRESENTATION LEARNING &amp;gt; image2022-2-14 14:25:48.png
我接着查看了参考文献
Qibin Hou, Daquan Zhou, and Jiashi Feng, “Coordinate attention for efficient mobile network design,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp.</description>
    </item>
    
    <item>
      <title>58-facebook denoiser 翻译</title>
      <link>https://ioyy900205.github.io/post/2022-01-10-58facebook-denoiser-%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Mon, 10 Jan 2022 14:44:57 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2022-01-10-58facebook-denoiser-%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/</guid>
      <description>Real Time Speech Enhancement in the Waveform Domain
网上找不到相关的翻译文章。 这里就做贡献了
  1. 摘要 2. 简介 3. 模型  3.1. 符号和问题设置 3.2. DEMUCS architecture 3.3. 目标   4. 实验  4.1. 实施详情 4.2. 结果 4.3. 消融实验 4.4. 实时性评估 4.5. 对ASR模型的影响   5. 相关工作 6. 结论  1. 摘要 We present a causal speech enhancement model working on the raw waveform that runs in real-time on a laptop CPU. The proposed model is based on an encoder-decoder architecture with skip-connections.</description>
    </item>
    
    <item>
      <title>2022-01-05 框架太难受</title>
      <link>https://ioyy900205.github.io/post/2022-01-05-%E6%A1%86%E6%9E%B6%E5%A4%AA%E9%9A%BE%E5%8F%97/</link>
      <pubDate>Wed, 05 Jan 2022 17:30:24 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2022-01-05-%E6%A1%86%E6%9E%B6%E5%A4%AA%E9%9A%BE%E5%8F%97/</guid>
      <description>记录一些以后可以用到修改的东西
 @TOC
选择模型 model_module, model_name = xx, xx model = getattr(importlib.import_module(model_module),model_name)  参考资料</description>
    </item>
    
    <item>
      <title>57-pytorch-lightning伪代码训练过程</title>
      <link>https://ioyy900205.github.io/post/2022-01-05-57-pytorch-lightning-%E4%BC%AA%E4%BB%A3%E7%A0%81%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/</link>
      <pubDate>Wed, 05 Jan 2022 14:32:39 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2022-01-05-57-pytorch-lightning-%E4%BC%AA%E4%BB%A3%E7%A0%81%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/</guid>
      <description>这里有宝藏
请自行提取
 @TOC
1. 训练伪代码 def fit(...): on_fit_start() if global_rank == 0: # prepare data is called on GLOBAL_ZERO only prepare_data() for gpu/tpu in gpu/tpus: train_on_device(model.copy()) on_fit_end() def train_on_device(model): # setup is called PER DEVICE setup() configure_optimizers() on_pretrain_routine_start() for epoch in epochs: train_loop() teardown() def train_loop(): on_train_epoch_start() train_outs = [] for train_batch in train_dataloader(): on_train_batch_start() # ----- train_step methods ------- out = training_step(batch) train_outs.append(out) loss = out.loss backward() on_after_backward() optimizer_step() on_before_zero_grad() optimizer_zero_grad() on_train_batch_end(out) if should_check_val: val_loop() # end training epoch logs = training_epoch_end(outs) def val_loop(): model.</description>
    </item>
    
    <item>
      <title>56-S-DCCRN论文阅读</title>
      <link>https://ioyy900205.github.io/post/2021-12-28-56-s-dccrn%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</link>
      <pubDate>Tue, 28 Dec 2021 15:32:05 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-12-28-56-s-dccrn%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</guid>
      <description>在语音增强方面，复杂的神经网络由于其在处理复值频谱方面的有效性而显示出良好的性能。最近的大多数语音增强方法主要集中在采样率为16K Hz的宽频信号上。然而，由于难以对更多的频段，特别是高频成分进行建模，因此仍然缺乏对超宽频段（如32K Hz）甚至全频段（48K）去噪的研究。在本文中，我们将之前的深度复杂卷积递归神经网络（DCCRN）大幅扩展为超宽频带版本&amp;ndash;S-DCCRN，以对32K Hz采样率的语音进行去噪。
具体操作：
 我们首先采用了一个级联的子带和全带处理模块，它由两个小尺寸的DCCRN组成&amp;ndash;一个操作子带信号，一个操作全带信号，旨在从局部和全局频率信息中受益。此外，我们没有简单地采用STFT特征作为输入，而是使用了一个以端到端方式训练的复杂特征编码器来完善不同频段的信息。 我们还使用了一个复杂的特征解码器来将特征还原到时频域。最后，采用一种可学习的频谱压缩方法来调整不同频段的能量，这有利于神经网络的学习。   所提出的模型S-DCCRN已经超过了PercepNet以及几个有竞争力的模型，在语音质量和可懂度方面达到了最先进的性能。消融研究也证明了不同贡献的有效性。   @TOC
1. 引言 随着电话会议和其他实时语音通信场景的快速发展，对高质量高保真语音的需求急剧增加。随着采样率的提高，语音将包含更丰富的信息和更精细的细节，特别是在更高的频段。然而，目前大多数基于深度学习的语音增强方法主要关注采样率为16K Hz的宽频信号。在超宽频[1]甚至全频段信号上进行语音增强的潜力仍有待探索，因为在对更多频段尤其是高频成分进行建模方面存在着挑战。此外，用更大维度的特征进行建模将导致更高的建模复杂性，使实时实施变得更加困难。一些语音增强器采用了压缩的特征，如吠声频谱[2]来模拟高频信号，而特征压缩可能会不可避免地失去频段的重要信息，导致次优性能。
长期以来，基于DNN的语音前端算法只试图增强噪声幅度，而将噪声相位直接用于语音波形重建。其原因可以归结为相位的结构不明确，被认为是具有挑战性的估计。随后，Williamson等人提出了复杂比率掩码（CRM）[3]，它可以通过同时增强噪声语音的实部和虚部来完美重建语音。基于宽带场景的SOTA方法，如SDD-Net[4]和DCCRN[5]，已经显示出出色的性能，特别是对于低信噪比和突发噪声的复杂去噪情况。DCCRN结合了DCUNET[6]和CRN[7]的优点，使用LSTM对时间背景进行建模，可训练的参数和计算成本大大降低。SDD-Net应用功率压缩频谱[8]作为输入特征，并采用了四个特别设计的阶段，极大地提高了同步去杂和去噪的语音质量。对于全波段的情况，RNNoise[2]的研究采用树皮谱，而不是STFT，作为模型的输入。树皮谱在频率轴上完全只有22个维度[9]，这可以大大减少模型的大小，并加快模型推理的速度。树皮谱假设语音和噪声的频谱包络足够平坦[2]。然而，由于真实声学场景的复杂性（如突发噪声和混响），这种方法在真实声学场景中可能导致严重的衰减，从而导致过多的噪声残留。最近，PercepNet[10]提出了一种感知频带表示法，它只对32个三角谱带进行操作，根据等效的entrectangular带宽（ERB）尺度进行间隔[9]。然而，树皮尺度和ERB尺度的分辨率比STFT的线性频谱更粗糙，导致了频带信息的泄漏。最近，超宽频带/全频带信号的语音增强引起了广泛关注&amp;ndash;深度噪声抑制挑战（DNS）[11]特别设立了全频带轨道。
本文提出了超宽频DCCRN（S-DCCRN），用于32K Hz采样率的超宽频场景中的语音增强。这项工作的贡献有三个方面，在Voicebank和Demond数据集上进行客观评估，在DNS-2021盲测集上进行主观评估。
我们提出了两个轻量级的DCCRN子模块，分别用于子频段和全频段（SAF）建模，因为我们认为低频段含有较高的能量，而高频段对主观感受有很大影响[12]。因此，采用子波段处理模块对低频段和高频段分别建模。然而，由于低频和高频成分之间没有明确的信息交互，仅采用子带处理可能会导致频段之间的不平滑连接。因此，我们进一步应用全频段处理模块来平滑不同频段的边界。详细地说，子频带处理模块包括一个子频带DCCRN，它用群复卷积代替原始DCCRN的复卷积，以分别模拟低频段和高频段。全频带和子频带处理模块的编码器和解码器之间的卷积通路[13]被用来进行更好的信息交互，避免全频带的信息丢失。在较小的模型尺寸下，与神谕DCCRN相比，SAF模块导致了0.17的PESQ改进。
受宽带去噪中频谱压缩的启发[8]，我们在模型中引入了可学习频谱压缩（LSC），它可以动态地调整不同频段的能量。LSC的使用使得高频段的模式更加清晰，这种更新带来了0.07的额外PESQ增益。
受DPT-FSNet[14]的编码器/解码器块的启发，我们在STFT之后采用了复杂特征编码器（CFE），在iSTFT之前采用了复杂特征解码器（CFD）。我们保持与大多数宽频语音增强模型相同的STFT点。尽管在高采样率的情况下，频率分辨率相对较低，但CFE块可以细化STFT频谱的不同频段的信息。通过可学习的频谱压缩，这种更新带来了0.07的额外PESQ增益。
所提出的S-DCCRN模型超过了所有测试过的SOTA模型，包括RNNoise和DCCRN，并在Interspeech 2021 DNS挑战赛的盲测集上获得了3.62的MOS分数，表现优异[11]。
2. 超宽带DCCRN 2.1. 复数编解码器 研究人员通常采用Bark频谱作为宽频带场景下全频带语音增强的网络输入，它可以将物理频率转换成基于人类感知的心理声学频率[2]。然而，通过这种转换，原来的物理频段被压缩了，而且相位信息也被丢弃了。此外，基于人类感知的特征可能不适合于网络的输入。另一方面，直接使用STFT特征也会引起一些问题。随着STFT点数的增加，由于高维输入特征难以建模，网络的复杂性会增加。相反，使用点数较少的STFT特征也会造成频率分辨率的下降。本文受DPT-FSNet[14]的编码器/解码器块的启发，在STFT之后采用复杂特征编码器/解码器，基于512维的复杂STFT特征来细化不同频段的信息。
如图2（a）所示，复杂特征编码器（CFE）模块的输入是通过STFT得到的T-F频谱。我们采用核大小为1的复数conv2d来提取高维信息。然后使用扩张的密集块[15]来捕捉时间尺度上的长期背景特征。最后，采用 采用复杂的conv2d来提取复杂的局部特征。在每次卷积之后，都会相继进行LayerNorm和PReLU激活。
如图2（b）所示，复杂特征解码器（CFD）模块的输入是SAF模块输出的实/图像特征。具体来说，我们采用扩展的密集块来处理估计的实/图像特征。然后，扩张密集块的输出被复杂像素卷积处理，用复杂卷积代替像素卷积[16]中的卷积。像素卷积被认为是替代转置卷积的更好方法，以避免棋盘式伪影[17]。最后，进行复杂卷积，将高维特征还原到时频域。如图2（c）所示，每个密集块由五层conv2d组成。各帧之间的卷积是因果关系。与之前所有层的密集连接避免了梯度消失的问题[15]。
2.2. 子带和全带处理模块 随着采样率的增加，频段的数量也在很大程度上增加。由于低频和高频之间的信息有很大的不同，不同的频段很难只通过全频段处理来建模[12]。在一个模块中对它们进行建模并不是最佳选择。另一方面，由于不同频段之间没有信息交互，子频段处理会在不同频段的边界造成一定的不平滑连接。基于上述考虑，我们提出了一个子波段和全波段处理（SAF）模块，以发挥两者的优势。
如图1所示，我们使用来自CFE的编码特征作为SAF模块的输入，该模块主要由子带DCCRN和全带DCCRN组成。在SAF模块中，特征首先被一个子带DCCRN处理。子带DCCRN的输出与来自CFE的编码特征（被认为有助于平滑频带）的连接被视为全带DCCRN的输入。全波段DCCRN的输出是来自CFE的编码特征的复数比率掩码（CRM）。
子波段DCCRN的结构如图3所示。子波段DCCRN的总体设计与神谕DCCRN相似，但神谕DCCRN中的复杂卷积块被复杂群卷积块所取代。如图4所示，复杂组卷积块的目的是对低频段和高频段分别建模。此外，为了从每个编码器层聚集更丰富的信息，并缓解各频段之间的不平滑连接，我们采用了编码器和解码器之间的卷积通路块，这在DCCRN+[13]中被证明是有用的。具体来说，编码器和解码器之间的卷积通路由一个复杂的卷积块和批量归一化组成。
在将来自CFE的编码特征与子带DCCRN的输出相连接后，我们使用另一个全带DCCRN，它采取普通复数卷积而不是复数组卷积，以进一步再生和平滑不同的频段。全波段DCCRN还采用了编码器和解码器之间的卷积途径，以实现更好的信息交互。
2.3. 可学习的频谱压缩 有学者指出，频谱中的局部模式在每个频段中往往是不同的：低频段往往包含高能量、音调以及长时间持续的声音，而高频段则可能有低能量成分、噪声和快速衰减的声音[12]。最近，关于宽频带去噪的频谱压缩显示了有希望的结果，它可以通过0.5的压缩率增加高频带的能量[8]。
我们认为，频段的压缩率应该是不同的，因为高频率的频段可能需要较低的压缩率来保持其高能量。这激发了我们开发一个可学习的频谱压缩模块，使用一组网络层来压缩STFT频谱。在可学习的网络层之后进行sigmoid激活，目的是将输出压缩到0∼1。
详细来说，可学习频谱压缩可以描述为 YLSC = |Y |α ejjY (1) 其中Y和α分别表示噪声频谱和可学习参数。
2.4. 损失函数 对于学习目标，我们首先应用SI-SNR[18]损失，它是一个时域损失函数。此外，我们采用复杂的均方误差（MSE）损失和Kullback-Leibler Divergence[19]来提高估计频谱和复杂域中的清洁频谱之间的相似性。KL Divergence的目的是从概率分布的角度来优化清洁频谱和估计频谱。这三种损失是通过以下方式共同优化的
3. 实验 3.1. 数据集 我们对32K采样率的音频样本进行了语音增强实验。我们首先在Voice Bank和DEMAND数据集[20]上进行消融实验，以证明每个提议的子模块的有效性。具体来说，源语音来自VoiceBank语料库[21]，其中包含28个扬声器用于训练，另外2个扬声器用于测试。10种噪声类型，其中2种是人工生成的，8种是来自DEMAND[22]的真实录音，用于训练。请注意，所有的数据在实验前都从48K降频到32K Hz。总的来说，固定的训练和验证集分别包含11,572个语料（10小时）和872个语料（30分钟）。我们还在这个数据集上将其他SOTA模型（包括PercepNet[10]）与S-DCCRN进行比较。 然后，S-DCCRN被进一步训练，并用Interspeech 2021 DNS挑战数据集进行评估，以显示其在更复杂和真实的声学场景中的性能。源语音数据来自DNS-2021全频段数据集，其中包含672小时的语音数据。180小时的DNS-2021噪声集，包括来自150个噪声类别的65000个噪声片段，被选作源噪声数据。训练集包含605小时源语音数据，而验证集分别包含67小时源语音数据。训练数据是以32K Hz的采样率即时生成的，并在一个批次中被分割成8 s的小块，信噪比范围为-5至20 dB。经过14个历时的训练，该模型 &amp;quot;看到 &amp;quot;的总数据超过9000小时。  3.</description>
    </item>
    
    <item>
      <title>55-python多进程</title>
      <link>https://ioyy900205.github.io/post/2021-12-24-55-python%E5%A4%9A%E8%BF%9B%E7%A8%8B/</link>
      <pubDate>Fri, 24 Dec 2021 13:43:04 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-12-24-55-python%E5%A4%9A%E8%BF%9B%E7%A8%8B/</guid>
      <description>处理数据需要
  1. 基本原理 2. 实现代码  1. 基本原理 创建一个池子p=Pool()，将子任务通过p.apply_async(传入的任务，传入任务的参数)，添加进去。
2. 实现代码 &amp;#39;&amp;#39;&amp;#39; Date: 2021-12-23 21:37:55 LastEditors: Liuliang LastEditTime: 2021-12-23 21:39:15 Description: &amp;#39;&amp;#39;&amp;#39; import os import random import time from multiprocessing import Pool, Process def long_time_task(name): print (&amp;#39;Run task %s(%s)...&amp;#39; % (name, os.getpid())) start = time.time() time.sleep(random.random() * 3) end = time.time() print (&amp;#39;Task %sruns %0.2fseconds.&amp;#39; % (name, (end - start)) ) if __name__==&amp;#39;__main__&amp;#39;: print (&amp;#39;Parent process %s.&amp;#39; % os.</description>
    </item>
    
    <item>
      <title>54-ConvTasnet解读</title>
      <link>https://ioyy900205.github.io/post/2021-12-23-54-convtasnet%E8%A7%A3%E8%AF%BB/</link>
      <pubDate>Thu, 23 Dec 2021 09:53:44 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-12-23-54-convtasnet%E8%A7%A3%E8%AF%BB/</guid>
      <description>挖坑挖坑
 @TOC
1. 原来的缺点  信号相位和幅度的解耦 语音分离时频表示的次优性 以及计算频谱的时间延迟等  2. STFT的问题  第一，STFT是一种通用的信号变换方法，它对语音分离来说不一定是最佳的。 第二，对干净声源的相位进行精确重建是一个重要的问题，并且相位的错误估计- 会拉低重建音频的上限。尽管一些方法通过重建相位来缓解此问题，但性能仍然欠佳。 第三，成功地从时频表示中分离声源，要求对混合信号进行高分辨-率的频率分解，这需要较长的时间窗来计算STFT。该要求增加了系统的最小延迟，这限制了其在实时、低延迟应用中的适用性。例如，在大多数语音分离系统中，STFT的窗口长度至少为32 ms，在音乐分离应用中甚至更大，而音乐分离应用则需要更高分辨率的频谱（高于90 ms ）。  3. LSTM的问题 原始TasNet将深度长短时记忆（LSTM）网络作为分离模块会大大限制其适用性。
 第一，在编码器中选择较小的核大小（即波形段的长度）会增加编码器输出的长度，这使得LSTM的训练变得难以管理。 第二，深度LSTM网络中的大量参数显著增加了其计算成本，并限制了其在低资源、低功耗平台（例如可穿戴式听力设备）中的适用性。 第三个问题是LSTM网络在时间上的长期依赖性，这经常导致分离准确性不一致，  4. 代码给你的启发 我这里参考的是官方pytorch example里面实现的代码
def forward(self, input: torch.Tensor) -&amp;gt; torch.Tensor: &amp;#34;&amp;#34;&amp;#34;Perform source separation. Generate audio source waveforms. Args: input (torch.Tensor): 3D Tensor with shape [batch, channel==1, frames] Returns: Tensor: 3D Tensor with shape [batch, channel==num_sources, frames] &amp;#34;&amp;#34;&amp;#34; if input.ndim != 3 or input.</description>
    </item>
    
    <item>
      <title>53-model.train解读</title>
      <link>https://ioyy900205.github.io/post/2021-12-21-53-model.train%E8%A7%A3%E8%AF%BB/</link>
      <pubDate>Tue, 21 Dec 2021 19:40:52 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-12-21-53-model.train%E8%A7%A3%E8%AF%BB/</guid>
      <description>还没有看到有人从源码的角度去解决这个问题，所以研究了一下。
 @TOC
寄生关系 model.train()实际上是 nn.moduel 下的一个函数
打开网页： https://github.com/pytorch/pytorch/tree/master/torch/nn/modules 可以看到nn.modules下面有各种py文件，其中module.py 中实现的是非常重要的基类。
官网的注释是：Base class for all neural network modules,your models should also subclass this class.
其中：默认了一个self.training=True
training: bool _is_full_backward_hook: Optional[bool] def __init__(self) -&amp;gt; None: &amp;#34;&amp;#34;&amp;#34; Initializes internal Module state, shared by both nn.Module and ScriptModule. &amp;#34;&amp;#34;&amp;#34; torch._C._log_api_usage_once(&amp;#34;python.nn_module&amp;#34;) self.training = True self._parameters: Dict[str, Optional[Parameter]] = OrderedDict() self._buffers: Dict[str, Optional[Tensor]] = OrderedDict() self._non_persistent_buffers_set: Set[str] = set() 接下来看train函数
def train(self: T, mode: bool = True) -&amp;gt; T: r&amp;#34;&amp;#34;&amp;#34;Sets the module in training mode.</description>
    </item>
    
    <item>
      <title>52-torch.cuda.stream的使用 </title>
      <link>https://ioyy900205.github.io/post/2021-12-18-52-cuda.stream/</link>
      <pubDate>Mon, 20 Dec 2021 10:27:57 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-12-18-52-cuda.stream/</guid>
      <description>How to use CUDA stream in Pytorch?
  1. 流程 2. 最佳答案  1. 流程 stream0 = torch.get_stream() stream1 = torch.get_stream() with torch.now_stream(stream0): // task A with torch.now_stream(stream1): // task B torch.synchronize() // get A and B&amp;#39;s answer 2. 最佳答案 s1 = torch.cuda.Stream() s2 = torch.cuda.Stream() # Initialise cuda tensors here. E.g.: A = torch.rand(1000, 1000, device = ‘cuda’) B = torch.rand(1000, 1000, device = ‘cuda’) # Wait for the above tensors to initialise.</description>
    </item>
    
    <item>
      <title>51-pytorch lightning中train.fit函数的解读 </title>
      <link>https://ioyy900205.github.io/post/2021-12-17-51-pytorch_lighting%E4%B8%AD-train.fit%E5%87%BD%E6%95%B0%E8%A7%A3%E8%AF%BB-copy/</link>
      <pubDate>Fri, 17 Dec 2021 10:27:57 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-12-17-51-pytorch_lighting%E4%B8%AD-train.fit%E5%87%BD%E6%95%B0%E8%A7%A3%E8%AF%BB-copy/</guid>
      <description>train.fit 用于训练。
先挖了个坑，以后填
 @TOC
传入参数  参考资料</description>
    </item>
    
    <item>
      <title>50-旷视NBNet</title>
      <link>https://ioyy900205.github.io/post/2021-12-16-50-%E6%97%B7%E8%A7%86nbnet/</link>
      <pubDate>Thu, 16 Dec 2021 16:31:52 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-12-16-50-%E6%97%B7%E8%A7%86nbnet/</guid>
      <description>简单学习了一下旷视NBNet
  1. 模型的基本参数 2. 子空间投影  1. 模型的基本参数 输入数据 [1,3,128,128]
2. 子空间投影 NBNet的创新在于子空间投影，包括两个主要的步骤：基底向量生成和投影。
 参考资料
https://github.com/leonmakise/NBNet_Pytorch https://github.com/pminhtam/NBNet
https://arxiv.org/abs/2012.15028</description>
    </item>
    
    <item>
      <title>49-字节残差unet的思考</title>
      <link>https://ioyy900205.github.io/post/2021-12-16-49-%E5%AD%97%E8%8A%82%E6%AE%8B%E5%B7%AEunet%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/</link>
      <pubDate>Thu, 16 Dec 2021 16:03:44 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-12-16-49-%E5%AD%97%E8%8A%82%E6%AE%8B%E5%B7%AEunet%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/</guid>
      <description>1. 无需LSTM结构 2. 无需将网络的 freq_bins 维度压缩至1. 3. 数据归一化的方式 4. 在融合了之后还进行了信息整理 5. 将频带切分 堆叠 送入网络 6. 优化目标 7. 关于Mask  1. 无需LSTM结构 中间做信息处理的部分是block7a-block7d
每个block都有4个残差conv块
每个残差conv块有2层卷积
(x_center, _) = self.conv_block7a(x6_pool) # (bs, 384, T / 32, F / 64) (x_center, _) = self.conv_block7b(x_center) # (bs, 384, T / 32, F / 64) (x_center, _) = self.conv_block7c(x_center) # (bs, 384, T / 32, F / 64) (x_center, _) = self.conv_block7d(x_center) # (bs, 384, T / 32, F / 64) 2.</description>
    </item>
    
    <item>
      <title>48-python自动化(1)</title>
      <link>https://ioyy900205.github.io/post/2021-12-15-48-python%E8%87%AA%E5%8A%A8%E5%8C%961/</link>
      <pubDate>Wed, 15 Dec 2021 13:58:12 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-12-15-48-python%E8%87%AA%E5%8A%A8%E5%8C%961/</guid>
      <description>方便结果记录
 @TOC
1. 获取文件名 import pathlib pathlib.Path(__file__).stem 可以拿到运行xxx.py 的 xxx。简单来说是获取文件的名字
 参考资料</description>
    </item>
    
    <item>
      <title>Pytorch Lightning的调试bug</title>
      <link>https://ioyy900205.github.io/post/2021-12-14-lightning%E5%A4%AA%E5%9D%91%E4%BA%861/</link>
      <pubDate>Tue, 14 Dec 2021 14:13:16 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-12-14-lightning%E5%A4%AA%E5%9D%91%E4%BA%861/</guid>
      <description>一些非常奇怪的问题
 @TOC
DeadLock detected from rank: 0 这真的是个头疼的问题。
这个问题与模型参数的保存有关。我还没有发现比较好的解决办法。
网上查的解决办法是在ModelCheckpoint中设置save_weights_only=True 感觉不完美
在我的实践中，即使设置了，也不起作用。这就头疼了。。 经过1天时间研究我找到了一个比较合适的方法，逐步发现了问题：
解决方法： 找到 torch_lightning.utilities.cloud_io.py 定位到最后torch.save处
加入一行代码: checkpoint.pop(&amp;lsquo;hyper_parameters&amp;rsquo;)
备注： checkopint字典的k,v有 1.epoch 2.global_step 3.pytorch-lightning_version 4.state_dict 5.hparams_name 6.hyper_parameters
 参考资料
1</description>
    </item>
    
    <item>
      <title>48-论文阅读小目标检测综述</title>
      <link>https://ioyy900205.github.io/post/2021-09-27-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%B0%8F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/</link>
      <pubDate>Mon, 27 Sep 2021 09:10:57 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-09-27-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%B0%8F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/</guid>
      <description>小目标检测方法的问题和方法整理。
 @TOC
0.1. 多尺度方法 YOLO、Faster-RCNN只用最后一层，SSD首次采用了多尺度预测的方式。
0.1.1. 图像金字塔方法 首先将图像缩放的不同的分辨率，通过不同分辨率的图像上分别提取特征来形成多尺度的特征表达。
缺点：由于需要多种分辨率图像分别提取特征，严重增加了推理时间，限制了该方法在实时性要求比较高的条件下应用。
0.1.2. DSDD算法  用101替代VGG16 将高层特征的语义信息融入底层特征（反卷积）  0.1.3. 特征金字塔方法 自底向上的分支用于产生多尺度的特征。自顶向下的方法用于将丰富语义信息传递到底层。
方法：高层特征进行2倍上采样得到和相邻底层一样的分辨率，然后底层特征经过1x1卷积和上采样之后的高层特征进行元素级别的相加，再经过3x3卷积得到最终特征图。
FPN目前已经成为了一个标准配置，也有很多基于FPN的优化工作相继涌现出来。
0.1.4. PANet算法 作者再FPN的基础上又增加了一个自底向上的路径增强分支。
FPN中，高层特征与低层特征之间路径较长，造成在金字塔的顶部含有的底层信息较少。为了解决这个问题，PANet使用较少数量的卷积层构建了路径增强模块，尽可能多的保留底层信息。同时增加了自适应的池化层模块，使得感兴趣的区域中包含多层特征，而不是单层特征，进行了进一步的特征融合。
0.1.5. ASFF算法 注意到一个问题，FPN中，目标在某层被当作正类时，但是在其他层可能会被当成负类。这样在特征金字塔的某一层单独检测时会引入其他层的矛盾信息。
自适应的空间特征融合方法。该方法在FPN的基础上通过学习权重参数的方式将不同特征层融合到一起，得到融合之后的特征图用于最终的预测。
在论文中，作者将 ASFF 应用到 YOLO3 中，为了验证 ASFF 的有效性，首先在 YOLO3 应用了一系列的技巧，对 YOLO3 进行优化，将其在 COCO 2017验证集上的APs 指标由 18. 3 提升到 24. 6，将优化之后的 YOLO3 作为一个强的基线． 然后，在此基础上加入 ASFF，APs 指标由 24. 6 提升到 27. 5，提升了将近 3 个百分点，由此可见 ASFF 对于小目标检测的有效性。
0.1.6. Libra-RCNN算法 改进的方法分别提取了 4 个级别的多尺度特征{ C2，C3，C4，C5} ，然后将 { C2，C3，C5 } 缩放到和 C4 同样大小，进行集成操作，也就是将这 4 个尺度的特征进行 求和取平均得到集成之后的特征，再将得到的特征送入设计的增强模块中进行一个加强操作，最后再将加强后的特征和{ C2，C3，C4，C5} 相加，增强原特征．</description>
    </item>
    
    <item>
      <title>论文阅读BN</title>
      <link>https://ioyy900205.github.io/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-bn/</link>
      <pubDate>Fri, 24 Sep 2021 14:07:57 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-bn/</guid>
      <description>原来在训练深层网络的时候非常困难。这是因为在训练过程中，每一层的输入分布都会随着前几层的参数变化而变化。
原来的问题：
 训练速度慢：降低学习率和谨慎的参数初始化 训练困难：训练具有饱和非线性的模型非常困难  内部协变量转移
 @TOC
标题  参考资料</description>
    </item>
    
    <item>
      <title>47-今日阅读记录</title>
      <link>https://ioyy900205.github.io/post/2021-09-24-47-%E4%BB%8A%E6%97%A5%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95/</link>
      <pubDate>Fri, 24 Sep 2021 10:05:01 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-09-24-47-%E4%BB%8A%E6%97%A5%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95/</guid>
      <description>现在时间比较少了，只能做一下阅读记录
 @TOC
1. 工具类 Pytorch中requires_grad_(), detach(), torch.no_grad()的区别
https://noahsnail.com/2020/05/29/2020-05-29-Pytorch%E4%B8%ADrequires_grad_(),%20detach(),%20torch.no_grad()%E7%9A%84%E5%8C%BA%E5%88%AB/
a = torch.tensor([1.0, 2.0]) b = a.data a.data返回的是一个新的Tensor对象b，a, b的id不同，说明二者不是同一个Tensor，但b与a共享数据的存储空间，即二者的数据部分指向同一块内存，因此修改b的元素时，a的元素也对应修改。
 参考资料</description>
    </item>
    
    <item>
      <title>46——binarySearch很好的理解方式</title>
      <link>https://ioyy900205.github.io/post/2021-08-24-46-%E4%BA%8C%E5%88%86%E5%8F%98%E7%A7%8D%E5%BE%88%E5%A5%BD%E7%90%86%E8%A7%A3%E7%9A%84%E6%96%B9%E5%BC%8F/</link>
      <pubDate>Tue, 24 Aug 2021 09:52:47 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-08-24-46-%E4%BA%8C%E5%88%86%E5%8F%98%E7%A7%8D%E5%BE%88%E5%A5%BD%E7%90%86%E8%A7%A3%E7%9A%84%E6%96%B9%E5%BC%8F/</guid>
      <description>本文介绍四种二分查找：
 查找第⼀个值等于给定值的元素 查找最后⼀个值等于给定值的元素 查找第⼀个⼤于等于给定值的元素 查找最后⼀个⼩于等于给定值的元素    1. 查找第⼀个值等于给定值的元素 2. 查找最后⼀个值等于给定值的元素 3. 查找第⼀个⼤于等于给定值的元素 4. 查找最后⼀个⼩于等于给定值的元素 2. 对于a[mid]&amp;lt;=value，如果mid = len -1 或者 mid 后面的值 大于 value 则返回mid ,否则 mid+1:high  1. 查找第⼀个值等于给定值的元素 a[mid]跟要查找的value的⼤⼩关系有三种情况：⼤于、⼩于、等于。
1. 对于a[mid]&amp;gt;value的情况，我们需要更新high= mid-1。2. 对于a[mid]&amp;lt;value的情况，我们需要更新 low=mid+1。3. 那当a[mid]=value的时候， 我们就需要确认⼀下这个a[mid]是不是第⼀个值等于给定值的元素。3.1 如果mid等于0，那这个元素已经是数组的第⼀个元素，那它肯定是我们要 找的；3.2 如果mid不等于0，但a[mid]的前⼀个元素a[mid-1]不等于value，那也说明a[mid]就是我们要 找的第⼀个值等于给定值的元素。 3.3 如果经过检查之后发现a[mid]前⾯的⼀个元素a[mid-1]也等于value，那说明此时的a[mid]肯定不是我 们要查找的第⼀个值等于给定值的元素。那我们就更新high=mid-1，因为要找的元素肯定出现在 [low, mid-1]之间。 2. 查找最后⼀个值等于给定值的元素 1. 对于a[mid]&amp;gt;value的情况，我们需要更新high= mid-1。2. 对于a[mid]&amp;lt;value的情况，我们需要更新 low=mid+1。3. 那当a[mid]=value的时候， 我们就需要确认⼀下这个a[mid]是不是最后一个等于给定值的元素。3.1 如果mid = len(nums)-1，那么这个元素已经数组最后一个元素，那它肯定是需要找的。3.</description>
    </item>
    
    <item>
      <title>45-PyTorch在imagenet上的推理验证</title>
      <link>https://ioyy900205.github.io/post/2021-08-18-45-pytorch%E5%9C%A8imagenet%E4%B8%8A%E7%9A%84%E6%8E%A8%E7%90%86%E9%AA%8C%E8%AF%81/</link>
      <pubDate>Wed, 18 Aug 2021 10:11:35 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-08-18-45-pytorch%E5%9C%A8imagenet%E4%B8%8A%E7%9A%84%E6%8E%A8%E7%90%86%E9%AA%8C%E8%AF%81/</guid>
      <description>给出在imagenet上的推理验证代码
这里有一个小意外，之前的NCNN和ONNX都是在这个框架上实现的。但是实现pytorch版本的时候，却始终显示结果不对。
检查问题之后，发现是model.eval()没有写。
经查： Batch Normalization 和 Dropout 方法模式不同。BN不会取平均，而是用训练好的值。 Dropout也不会有作用，这个是在训练时防止过拟合用的。
 @TOC
代码 &amp;#39;&amp;#39;&amp;#39; Date: 2021-08-10 14:12:17 LastEditors: Liuliang LastEditTime: 2021-08-17 18:16:08 Description: onnx在imagenet上的推理验证 &amp;#39;&amp;#39;&amp;#39; import argparse import os import random import shutil import time import warnings from Average_ import AverageMeter from Progress_ import ProgressMeter import torch import torch.nn as nn import torch.nn.parallel import torch.backends.cudnn as cudnn import torch.distributed as dist import torch.optim import torch.multiprocessing as mp import torch.utils.data import torch.</description>
    </item>
    
    <item>
      <title>44-用递归树分析复杂度</title>
      <link>https://ioyy900205.github.io/post/2021-08-17-44-%E7%94%A8%E9%80%92%E5%BD%92%E6%A0%91%E5%88%86%E6%9E%90%E5%A4%8D%E6%9D%82%E5%BA%A6/</link>
      <pubDate>Tue, 17 Aug 2021 15:41:06 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-08-17-44-%E7%94%A8%E9%80%92%E5%BD%92%E6%A0%91%E5%88%86%E6%9E%90%E5%A4%8D%E6%9D%82%E5%BA%A6/</guid>
      <description>用递归树分析复杂度
 @TOC
原理 归并排序每次会 将数据规模⼀分为⼆。
因为每次分解都是⼀分为⼆，所以代价很低，我们把时间上的消耗记作常量$1$。归并算法中⽐较 耗时的是归并操作，也就是把两个⼦数组合并为⼤数组。从图中我们可以看出，每⼀层归并操作消 耗的时间总和是⼀样的，跟要排序的数据规模有关。我们把每⼀层归并操作消耗的时间记作$n$。 现在，我们只需要知道这棵树的⾼度$h$，⽤⾼度$h$乘以每⼀层的时间消耗$n$，就可以得到总的 时间复杂度$O(n*h)$。
从归并排序的原理和递归树，可以看出来，归并排序递归树是⼀棵满⼆叉树。我们前两节中讲到， 满⼆叉树的⾼度⼤约是$\log_{2}n$，所以，归并排序递归实现的时间复杂度就是$O(n\log n)$。我 这⾥的时间复杂度都是估算的，对树的⾼度的计算也没有那么精确，但是这并不影响复杂度的计算 结果。 利⽤递归树的时间复杂度分析⽅法并不难理解，关键还是在实战，所以，接下来我会通过三个实际 的递归算法，带你实战⼀下递归的复杂度分析。学完这节课之后，你应该能真正掌握递归代码的复 杂度分析。
实战1 在⽤递归树推导之前，我们先来回忆⼀下⽤递推公式的分析⽅法。你可以回想⼀下，当时，我们为 什么说⽤递推公式来求解平均时间复杂度⾮常复杂？ 快速排序在最好情况下，每次分区都能⼀分为⼆，这个时候⽤递推公式$T(n)=2T(\frac{n}{2})+n$， 很容易就能推导出时间复杂度是$O(n\log n)$。但是，我们并不可能每次分区都这么幸运，正好⼀ 分为⼆。 我们假设平均情况下，每次分区之后，两个分区的⼤⼩⽐例为$1:k$。当$k=9$时，如果⽤递推公式 的⽅法来求解时间复杂度的话，递推公式就写成$T(n)=T(\frac{n}{10})+T(\frac{9n}{10})+n$。 这个公式可以推导出时间复杂度，但是推导过程⾮常复杂。那我们来看看，⽤递归树来分析快速排 序的平均情况时间复杂度，是不是⽐较简单呢？ 我们还是取$k$等于$9$，也就是说，每次分区都很不平均，⼀个分区是另⼀个分区的$9$倍。如果 我们把递归分解的过程画成递归树，就是下⾯这个样⼦：
快速排序的过程中，每次分区都要遍历待分区区间的所有数据，所以，每⼀层分区操作所遍历的数 据的个数之和就是$n$。我们现在只要求出递归树的⾼度$h$，这个快排过程遍历的数据个数就是 $h * n$ ，也就是说，时间复杂度就是$O(h * n)$。 因为每次分区并不是均匀地⼀分为⼆，所以递归树并不是满⼆叉树。这样⼀个递归树的⾼度是多少 呢？我们知道，快速排序结束的条件就是待排序的⼩区间，⼤⼩为$1$，也就是说叶⼦节点⾥的数据规 模是$1$。从根节点$n$到叶⼦节点$1$，递归树中最短的⼀个路径每次都乘以$\frac{1}{10}$，最⻓ 的⼀个路径每次都乘以$\frac{9}{10}$。通过计算，我们可以得到，从根节点到叶⼦节点的最短路径 是$\log_{10}n$，最⻓的路径是$\log_{\frac{10}{9}}n$。
 参考资料</description>
    </item>
    
    <item>
      <title>43-NCNN模型在imagenet上的推理验证</title>
      <link>https://ioyy900205.github.io/post/2021-08-17-43-ncnn%E6%A8%A1%E5%9E%8B%E5%9C%A8imagenet%E4%B8%8A%E7%9A%84%E6%8E%A8%E7%90%86%E9%AA%8C%E8%AF%81/</link>
      <pubDate>Tue, 17 Aug 2021 14:33:12 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-08-17-43-ncnn%E6%A8%A1%E5%9E%8B%E5%9C%A8imagenet%E4%B8%8A%E7%9A%84%E6%8E%A8%E7%90%86%E9%AA%8C%E8%AF%81/</guid>
      <description>这里给出NCNN模型的推理代码，供大家参考。
说一句题外话，这里我的tensor操作感觉有点繁琐，导致推理速度非常非常慢，也希望各位有比较好的方法能够提供给我。
我的联系方式是18811555730
  1. NCNN推理代码 2. 结果验证  1. NCNN推理代码 &amp;#39;&amp;#39;&amp;#39; Date: 2021-08-10 14:12:17 LastEditors: Liuliang LastEditTime: 2021-08-17 14:08:50 Description: NCNN在imagenet上的推理验证 &amp;#39;&amp;#39;&amp;#39; import argparse import os import random import shutil import time import warnings from Average_ import AverageMeter from Progress_ import ProgressMeter import torch import torch.nn as nn import torch.nn.parallel import torch.backends.cudnn as cudnn import torch.distributed as dist import torch.optim import torch.multiprocessing as mp import torch.utils.data import torch.utils.data.distributed import torchvision.</description>
    </item>
    
    <item>
      <title>42-ONNX模型在imagenet上的推理验证</title>
      <link>https://ioyy900205.github.io/post/2021-08-17-42-onnx%E6%A8%A1%E5%9E%8B%E5%9C%A8imagenet%E4%B8%8A%E7%9A%84%E6%8E%A8%E7%90%86%E9%AA%8C%E8%AF%81/</link>
      <pubDate>Tue, 17 Aug 2021 14:28:07 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-08-17-42-onnx%E6%A8%A1%E5%9E%8B%E5%9C%A8imagenet%E4%B8%8A%E7%9A%84%E6%8E%A8%E7%90%86%E9%AA%8C%E8%AF%81/</guid>
      <description>当训练模型转到ONNX模型之后，我一直在思考是否需要对转换后的ONNX模型进行验证。网上也找不到代码，这里我提供一个，方便大家验证。
  1. ONNX验证代码 2. 结果展示  1. ONNX验证代码 &amp;#39;&amp;#39;&amp;#39; Date: 2021-08-10 14:12:17 LastEditors: Liuliang LastEditTime: 2021-08-17 11:11:54 Description: onnx在imagenet上的推理验证 &amp;#39;&amp;#39;&amp;#39; import argparse import os import random import shutil import time import warnings from Average_ import AverageMeter from Progress_ import ProgressMeter import torch import torch.nn as nn import torch.nn.parallel import torch.backends.cudnn as cudnn import torch.distributed as dist import torch.optim import torch.multiprocessing as mp import torch.utils.data import torch.utils.data.distributed import torchvision.transforms as transforms import torchvision.</description>
    </item>
    
    <item>
      <title>41-NCNN模型部署</title>
      <link>https://ioyy900205.github.io/post/2021-08-06-41-ncnn%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E4%B8%80/</link>
      <pubDate>Fri, 06 Aug 2021 14:30:00 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-08-06-41-ncnn%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E4%B8%80/</guid>
      <description>争取比全网做的都要清晰
 @TOC
文件变化 整个工作流程文件有：
1.pth文件
2.onnx文件 （普通onnx、onnxsim）
3.ncnn文件 （普通ncnn、ncnnoptimal、ncnn压缩）
编译问题 多的就解释不了了，看代码
-std=c++11 -I/usr/local/include/opencv4 -I/home/liuliang/ncnn/build/install/include/ncnn -L/home/liuliang/ncnn/build/install/lib -lncnn -fopenmp -lopencv_imgcodecs -lopencv_flann -lopencv_core  参考资料</description>
    </item>
    
    <item>
      <title>40——C&#43;&#43;代码调试</title>
      <link>https://ioyy900205.github.io/post/2021-07-31-40_c&#43;&#43;%E8%B0%83%E8%AF%95%E9%97%AE%E9%A2%98/</link>
      <pubDate>Sat, 31 Jul 2021 16:09:29 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-07-31-40_c&#43;&#43;%E8%B0%83%E8%AF%95%E9%97%AE%E9%A2%98/</guid>
      <description>json文件配置，复制粘贴即可
 1. 神奇的json文件 ctrl+c ctrl+v 深藏功与名
{ &amp;#34;version&amp;#34;: &amp;#34;2.0.0&amp;#34;, &amp;#34;options&amp;#34;: { &amp;#34;cwd&amp;#34;: &amp;#34;${workspaceFolder}/build&amp;#34; }, &amp;#34;tasks&amp;#34;: [ { &amp;#34;type&amp;#34;: &amp;#34;shell&amp;#34;, &amp;#34;label&amp;#34;: &amp;#34;cmake&amp;#34;, &amp;#34;command&amp;#34;: &amp;#34;cmake&amp;#34;, &amp;#34;args&amp;#34;: [ &amp;#34;..&amp;#34; ] }, { &amp;#34;label&amp;#34;: &amp;#34;make&amp;#34;, &amp;#34;group&amp;#34;: { &amp;#34;kind&amp;#34;: &amp;#34;build&amp;#34;, &amp;#34;isDefault&amp;#34;: true }, &amp;#34;command&amp;#34;: &amp;#34;make&amp;#34;, &amp;#34;args&amp;#34;: [ ] }, { &amp;#34;label&amp;#34;: &amp;#34;Build&amp;#34;, &amp;#34;dependsOrder&amp;#34;: &amp;#34;sequence&amp;#34;, // 按列出的顺序执行任务依赖项  &amp;#34;dependsOn&amp;#34;:[ &amp;#34;cmake&amp;#34;, &amp;#34;make&amp;#34; ] } ] }  参考资料</description>
    </item>
    
    <item>
      <title>39--迭代对象问题</title>
      <link>https://ioyy900205.github.io/post/2021-07-17-39%E8%BF%AD%E4%BB%A3%E5%AF%B9%E8%B1%A1%E9%97%AE%E9%A2%98/</link>
      <pubDate>Sat, 17 Jul 2021 14:14:14 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-07-17-39%E8%BF%AD%E4%BB%A3%E5%AF%B9%E8%B1%A1%E9%97%AE%E9%A2%98/</guid>
      <description>简单讨论 Iterable\Iterator\Generator的概念
  1. Iterable 2. Iterator 3. Generator  1. Iterable  一个可迭代的对象是实现了__iter__()方法的对象 它要在for循环中使用，就必须满足iter()的调用(即调用这个函数不会出错，能够正确转成一个Iterator对象) 可以通过已知的可迭代对象来辅助实现我们自定义的可迭代对象。 一个对象实现了__getitem__()方法可以通过iter()函数转成Iterator，即可以在for循环中使用，但它不是一个可迭代对象(可用isinstance方法检测())  2. Iterator 一个对象实现了__iter__()和__next__()方法，那么它就是一个迭代器对象
class IterObj: def __init__(self): self.a = [3, 5, 7, 11, 13, 17, 19] self.n = len(self.a) self.i = 0 def __iter__(self): return iter(self.a) def __next__(self): while self.i &amp;lt; self.n: v = self.a[self.i] self.i += 1 return v else: self.i = 0 raise StopIteration() 集合和序列对象是可迭代的但不是迭代器. 文件对象是迭代器
一个迭代器(Iterator)对象不仅可以在for循环中使用，还可以通过内置函数next()函数进行调用。 例如
it = IterObj() next(it) # 3 next(it) # 5 3.</description>
    </item>
    
    <item>
      <title>38--binary_search</title>
      <link>https://ioyy900205.github.io/post/2021-07-13-38binary_search/</link>
      <pubDate>Tue, 13 Jul 2021 14:29:59 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-07-13-38binary_search/</guid>
      <description>Algorithm is amazing
 @TOC
1. 传统二分的问题 1.1. 取中位数索引的代码有问题 int mid = (left + right) / 2 这行代码是有问题的，在 left 和 right 都比较大的时候，left + right 很有可能超过 int 类型能表示的最大值，即整型溢出，为了避免这个问题，应该写成：
int mid = left + (right - left) / 2 2. 神奇二分查找 （1）首先把循环可以进行的条件写成 while(left &amp;lt; right)，在退出循环的时候，一定有 left == right 成立，此时返回 left 或者 right 都可以
&amp;#39;&amp;#39;&amp;#39; Date: 2021-07-13 15:13:49 LastEditors: Liuliang LastEditTime: 2021-07-13 15:20:27 Description: &amp;#39;&amp;#39;&amp;#39; class solution: def search_left(self,nums,target): left = 0 right = len(nums) if right == 0: return 0 while left &amp;lt; right: mid = (left + right) // 2 if nums[mid] &amp;lt; target: left = mid + 1 else: right = mid return left def search_right(self,nums,target): left = 0 right = len(nums) if right == 0: return 0 while left &amp;lt; right: mid = (left + right) // 2 if nums[mid] &amp;gt; target: right = mid - 1 else: left = mid return left c = solution() list = [2,4,4,4,5,6] m = c.</description>
    </item>
    
    <item>
      <title>37--如何写模型（二）</title>
      <link>https://ioyy900205.github.io/post/2021-07-09-37%E5%A6%82%E4%BD%95%E5%86%99%E6%A8%A1%E5%9E%8B%E4%BA%8C/</link>
      <pubDate>Fri, 09 Jul 2021 11:00:21 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-07-09-37%E5%A6%82%E4%BD%95%E5%86%99%E6%A8%A1%E5%9E%8B%E4%BA%8C/</guid>
      <description>如何写模型（二）
上次的核心是make_layer,通过返回sequential（）这种方法能够在该层中实现自动化操作。
这次希望能够实现多尺度架构的设计。
 @TOC
标题  参考资料</description>
    </item>
    
    <item>
      <title>36——如何写模型（一）</title>
      <link>https://ioyy900205.github.io/post/2021-07-08-36%E5%A6%82%E4%BD%95%E5%86%99%E6%A8%A1%E5%9E%8B%E4%B8%80/</link>
      <pubDate>Thu, 08 Jul 2021 17:43:31 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-07-08-36%E5%A6%82%E4%BD%95%E5%86%99%E6%A8%A1%E5%9E%8B%E4%B8%80/</guid>
      <description>如何写模型（一） 以ResNet为例
 @TOC
1. ResNet的构建 以ResNet为例，forward函数中，是前向传播过程，也是计算的核心内容。
def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) if self.include_top: x = self.avgpool(x) x = torch.flatten(x, 1) x = self.fc(x) return x 这里conv1、bn1、relu、maxpool、avgpool、fc都已经在__init__内很直观的定义好了。
问题就在于layer1-4的构建。那么我们以Restnet50为例，中间4个层每个block都要重复[3, 4, 6, 3]这么多次。e.g.第一层3次, 第二层4次，第三层6次，第四层3次。
那么可以看一下self.layer1-4是如何定义的。
self.layer1 = self._make_layer(block, 64, blocks_num[0]) self.layer2 = self._make_layer(block, 128, blocks_num[1], stride=2) self.layer3 = self._make_layer(block, 256, blocks_num[2], stride=2) self.</description>
    </item>
    
    <item>
      <title>35——NMS非极大值抑制</title>
      <link>https://ioyy900205.github.io/post/2021-07-05-35nms/</link>
      <pubDate>Mon, 05 Jul 2021 15:14:54 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-07-05-35nms/</guid>
      <description>代码实现
 @TOC
1. 代码 &amp;#39;&amp;#39;&amp;#39; Date: 2021-07-05 14:21:06 LastEditors: Liuliang LastEditTime: 2021-07-05 15:16:44 Description: &amp;#39;&amp;#39;&amp;#39; import numpy as np # import cv2 import matplotlib.pyplot as plt def py_cpu_nms(dets, thresh): &amp;#34;&amp;#34;&amp;#34;Pure Python NMS baseline.&amp;#34;&amp;#34;&amp;#34; # x1、y1、x2、y2、以及score赋值 x1 = dets[:, 0] y1 = dets[:, 1] x2 = dets[:, 2] y2 = dets[:, 3] scores = dets[:, 4] # 每一个检测框的面积 areas = (x2 - x1 ) * (y2 - y1 ) # 按照score置信度降序排序 order = scores.</description>
    </item>
    
    <item>
      <title>34——List中三个点是什么</title>
      <link>https://ioyy900205.github.io/post/2021-07-05-34list%E4%B8%AD%E7%9A%84%E4%B8%89%E4%B8%AA%E7%82%B9/</link>
      <pubDate>Mon, 05 Jul 2021 09:10:42 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-07-05-34list%E4%B8%AD%E7%9A%84%E4%B8%89%E4%B8%AA%E7%82%B9/</guid>
      <description>a[&amp;hellip;]代表什么
 @TOC
1. 初始化一个a import random import numpy as np a = np.arange(25).reshape((5,5)) print(a) print(a.shape) 显示
[[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14] [15 16 17 18 19] [20 21 22 23 24]] (5, 5) 2. 解释 2.1. 这个符号省略了所有的:,:,: c = a[...] print(&amp;#39;---------------------------------&amp;#39;) print(c) print(c.shape) 3. 实例 3.1. &amp;hellip;,None import random import numpy as np a = np.arange(25).reshape((5,5)) print(a) print(a.</description>
    </item>
    
    <item>
      <title>33——BCELoss和BCELosswithlogic</title>
      <link>https://ioyy900205.github.io/post/2021-07-03-33torch.nn.bceloss%E5%92%8Cbcelosswithlogic/</link>
      <pubDate>Sat, 03 Jul 2021 18:19:19 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-07-03-33torch.nn.bceloss%E5%92%8Cbcelosswithlogic/</guid>
      <description>损失函数BCELoss和BCELosswithlogic
参考了一下别人的资料，自己写了一下代码
 @TOC
1. 代码 &amp;#39;&amp;#39;&amp;#39; Date: 2021-07-03 17:36:14 LastEditors: Liuliang LastEditTime: 2021-07-03 17:51:05 Description: BCE_logic &amp;#39;&amp;#39;&amp;#39; import torch import torch.nn as nn from torch.random import seed import random SEED = 0 torch.manual_seed(SEED) # torch.cuda.manual_seed(SEED) input = torch.rand(3,3) sig = nn.Sigmoid() c = sig(input) # print(input) print(c) target = torch.FloatTensor([ [0,1,1], [0,0,1], [1,0,1] ]) print(target) loss = nn.BCELoss() val_loss = loss(c,target) print(val_loss) #验证BCEloss loss_2 = nn.BCEWithLogitsLoss() val_loss_2 = loss_2(input,target) print(val_loss_2) 2.</description>
    </item>
    
    <item>
      <title>32——torch.garther操作</title>
      <link>https://ioyy900205.github.io/post/2021-06-25-32torch.garther%E6%93%8D%E4%BD%9C/</link>
      <pubDate>Fri, 25 Jun 2021 10:49:40 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-25-32torch.garther%E6%93%8D%E4%BD%9C/</guid>
      <description>在深度学习中有pytorch.garther操作。这里做一个简单解释
 @TOC
示例 &amp;#39;&amp;#39;&amp;#39; Date: 2021-06-25 10:40:56 LastEditors: Liuliang LastEditTime: 2021-06-25 10:46:39 Description: torch_garther &amp;#39;&amp;#39;&amp;#39; import torch b = torch.Tensor([[1,2,3],[4,5,6]]) print(b) index_1 = torch.LongTensor([[0,1],[2,0]]) print(index_1) index_2 = torch.LongTensor([[0,1,1],[0,0,0]]) print (torch.gather(b, dim=1, index=index_1)) # print torch.gather(b, dim=0, index=index_2) 结果
tensor([[1., 2., 3.], [4., 5., 6.]]) tensor([[0, 1], [2, 0]]) tensor([[1., 2.], [6., 4.]]) 很简单，第二个tensor相当于是一个索引。在第一个tensor上查找对应index的数值。输出在第三个tensor上。
例如，tensor_2 [0][0]位置的值是0
所以输出 找到tensor1的1
 参考资料</description>
    </item>
    
    <item>
      <title>31——装饰器python</title>
      <link>https://ioyy900205.github.io/post/2021-06-21-31%E8%A3%85%E9%A5%B0%E5%99%A8python/</link>
      <pubDate>Mon, 21 Jun 2021 12:00:10 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-21-31%E8%A3%85%E9%A5%B0%E5%99%A8python/</guid>
      <description>一个装饰器的例子，很方便理解
 @TOC
标题 先定义一个简单的函数foo
def foo(): print &amp;#39;call foo()&amp;#39; 现在我们想要计算次函数执行的时间，于是修改代码如下
import time def foo(): start = time.clock() print &amp;#39;call foo()&amp;#39; end = time.clock() print &amp;#39;using time:&amp;#39;,end - start 然后有一个更快的方法
import time def foo(): print &amp;#39;call foo()&amp;#39; def cal_time(func): start = time.clock() func() end =time.clock() print &amp;#39;using time:&amp;#39;, end - start cal_time(foo) 但是这样比较麻烦，如果foo换了，还得重新写其他的例如foo_1
import time def foo(): print &amp;#39;call foo()&amp;#39; # 定义一个计时器，传入一个函数，并返回另一个附加了计时功能的方法 def cal_time(func): # 定义一个内嵌的包装函数，给传入的函数加上计时功能的包装 def wrapper(): start = time.</description>
    </item>
    
    <item>
      <title>30——强化学习</title>
      <link>https://ioyy900205.github.io/post/2021-06-16-30%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Wed, 16 Jun 2021 11:25:16 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-16-30%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</guid>
      <description>维基百科：
强化学习（RL）是机器学习的一个领域，涉及软件代理如何在环境中采取行动以最大化一些累积奖励的概念。该问题由于其一般性，在许多其他学科中得到研究，如博弈论，控制理论，运筹学，信息论，基于仿真的优化，多智能体系统，群智能，统计和遗传算法。在运筹学和控制文献中，强化学习被称为近似动态规划或神经动态规划。
百度：
强化学习(reinforcement learning)，又称再励学习、评价学习，是一种重要的机器学习方法，在智能控制机器人及分析预测等领域有许多应用。
但在传统的机器学习分类中没有提到过强化学习，而在连接主义学习中，把学习算法分为三种类型，即非监督学习(unsupervised learning)、监督学习(supervised leaning)和强化学习。
 @TOC
强化学习的主流算法 免模型学习（Model-Free） vs 有模型学习（Model-Based）
在介绍详细算法之前，我们先来了解一下强化学习算法的2大分类。这2个分类的重要差异是：智能体是否能完整了解或学习到所在环境的模型
有模型学习（Model-Based）对环境有提前的认知，可以提前考虑规划，但是缺点是如果模型跟真实世界不一致，那么在实际使用场景下会表现的不好。
免模型学习（Model-Free）放弃了模型学习，在效率上不如前者，但是这种方式更加容易实现，也容易在真实场景下调整到很好的状态。所以免模型学习方法更受欢迎，得到更加广泛的开发和测试。
 参考资料
https://easyai.tech/ai-definition/reinforcement-learning/</description>
    </item>
    
    <item>
      <title>29——GRU 3步介绍</title>
      <link>https://ioyy900205.github.io/post/2021-06-09-29gru%E4%B8%89%E6%AD%A5/</link>
      <pubDate>Wed, 09 Jun 2021 16:05:03 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-09-29gru%E4%B8%89%E6%AD%A5/</guid>
      <description>29——GRU 3步介绍
GRU它引⼊了**重置⻔（reset gate）和更新⻔（update gate）**的概念，从而修改了循环神经⽹络中隐藏状态的计算⽅式。
 @TOC
1. ⻔控循环单元 1.1. 重置门和更新门 1.2. 候选隐藏状态 1.3. 隐藏状态 我们对⻔控循环单元的设计稍作总结：
重置⻔有助于捕捉时间序列⾥短期的依赖关系； 更新⻔有助于捕捉时间序列⾥⻓期的依赖关系。
 参考资料</description>
    </item>
    
    <item>
      <title>28——训练trick之优化器</title>
      <link>https://ioyy900205.github.io/post/2021-06-09-28%E8%AE%AD%E7%BB%83trick%E4%B9%8B%E4%BC%98%E5%8C%96%E5%99%A8/</link>
      <pubDate>Wed, 09 Jun 2021 15:24:43 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-09-28%E8%AE%AD%E7%BB%83trick%E4%B9%8B%E4%BC%98%E5%8C%96%E5%99%A8/</guid>
      <description>在机器学习的场景下，梯度下降学习的目标通常是最小化机器学习问题的损失函数。 一个好的算法能够快速可靠地找到最小值.(也就是说，它不会陷入局部极小值、鞍点或高原区域，而是寻找全局最小值)。
 @TOC
标题  参考资料
https://zhuanlan.zhihu.com/p/147275344</description>
    </item>
    
    <item>
      <title>27——做计算机视觉还得懂LSTM</title>
      <link>https://ioyy900205.github.io/post/2021-06-09-27%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%98%E5%BE%97%E6%87%82lstm/</link>
      <pubDate>Wed, 09 Jun 2021 12:14:22 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-09-27%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%98%E5%BE%97%E6%87%82lstm/</guid>
      <description>Long Short-Term Memory Networks
LSTMs are a special kind of Recurrent Neural Network — capable of learning long-term dependencies by remembering information for long periods is the default behavior.
All recurrent neural networks are in the form of a chain of repeating modules of a neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.
  1. Step 1: Decide how much past data it should remember 2.</description>
    </item>
    
    <item>
      <title>26——GFNet源码解读</title>
      <link>https://ioyy900205.github.io/post/2021-06-07-26gfnet-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/</link>
      <pubDate>Mon, 07 Jun 2021 10:45:08 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-07-26gfnet-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/</guid>
      <description>26——GFNet源码解读
  标题  1. inference模块 1.1. initial 设置参数：数据集地址;checkpoint_path;eval_model
1.2. 载入模型部分 导入checkpoint,并读取数据
model_arch = checkpoint[&amp;#39;model_name&amp;#39;] patch_size = checkpoint[&amp;#39;patch_size&amp;#39;] prime_size = checkpoint[&amp;#39;patch_size&amp;#39;] flops = checkpoint[&amp;#39;flops&amp;#39;] model_flops = checkpoint[&amp;#39;model_flops&amp;#39;] policy_flops = checkpoint[&amp;#39;policy_flops&amp;#39;] fc_flops = checkpoint[&amp;#39;fc_flops&amp;#39;] anytime_classification = checkpoint[&amp;#39;anytime_classification&amp;#39;] budgeted_batch_classification = checkpoint[&amp;#39;budgeted_batch_classification&amp;#39;] dynamic_threshold = checkpoint[&amp;#39;dynamic_threshold&amp;#39;] maximum_length = len(checkpoint[&amp;#39;flops&amp;#39;]) model_configurations载入字典对应文件。
检查eval_model 并载入模型 (model and model_prime)
1.3. 载入数据集部分 traindir = args.data_url + &amp;#39;train/&amp;#39; valdir = args.data_url + &amp;#39;val/&amp;#39; normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.</description>
    </item>
    
    <item>
      <title>[2021-06-05 【old】conda命令]</title>
      <link>https://ioyy900205.github.io/post/2021-06-05-oldconda%E5%91%BD%E4%BB%A4/</link>
      <pubDate>Sat, 05 Jun 2021 13:55:38 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-05-oldconda%E5%91%BD%E4%BB%A4/</guid>
      <description>环境配置
  1. 安装环境 2. 克隆环境 3. 删除环境 4. mmdetection安装 5. detection2安装  1. 安装环境 conda create -n env_ori python=3.7 -y conda activate env_ori pip install torch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 2. 克隆环境 conda create -n env_ori_1 &amp;ndash;clone env_ori
3. 删除环境 conda remove -n py36 &amp;ndash;all
4. mmdetection安装 conda create -n mmdet &amp;ndash;clone env_ori
git clone https://github.com/open-mmlab/mmdetection.git
cd mmdetection
pip install mmcv
python setup.py develop
以上图片是网上随便down了一个，然后在跑了一下demo，测试成功！
5. detection2安装  参考资料
https://github.com/open-mmlab/mmdetection</description>
    </item>
    
    <item>
      <title>25——GFNet论文阅读</title>
      <link>https://ioyy900205.github.io/post/2021-06-05-25gfnet/</link>
      <pubDate>Sat, 05 Jun 2021 10:54:57 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-05-25gfnet/</guid>
      <description>NeurIPS 2020录用的一篇论文：《Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classiﬁcation
论文链接：https://arxiv.org/pdf/2010.05300.pdf
代码和预训练模型链接：https://github.com/blackfeather-wang/GFNet-Pytorch
论文第一作者：
王语霖，清华大学自动化系直博二年级，导师为吴澄院士和黄高助理教授，研究兴趣为深度学习与计算机视觉，在NeurIPS 2019/2020以第一作者发表两篇学术论文。
 @TOC
1. 研究动机及简介 推理CNN所需的计算量（FLOPs）基本与像素数目成正比，即与图形的长、宽成二次关系。
在实际应用（例如手机APP、自动驾驶系统、图片搜索引擎）中，计算量往往正比于能耗或者时间开销，显然，无论出于成本因素还是从安全性和用户体验的角度考虑，网络的计算开销都应当尽可能小。
这便是本文所提出方法的出发点，我们的目标是，对于输入图片，自适应地找到其与任务最相关的区域，进而通过使神经网络只处理这些区域，以尽可能小的计算量得到可信的结果。具体而言，我们采用的方法是，将一张分辨率较高的图片表征为若干个包含其关键部分的“小块”（Patch），而后仅将这些小块输入神经网络。以下面的示意图为例，将一张224x224的图片分解为3个96x96的Patch进行处理所需的计算量仅为原图的55.2%。
2. Method 为了实现上述目的，事实上，有两个显然的困难：
(a) 任意给定一张输入图片，如何判断其与任务最相关的区域在哪里呢？
(b) 考虑到我们的最终目的是使神经网络得到正确的预测结果，不同输入所需的计算量是不一样的，例如对于下面所示的两个输入图片，神经网络可能仅需要处理一个patch就能识别出特征非常突出的月亮，但是需要处理更多的patch才能分辨出猫咪的具体品种。
为了解决这两个问题，我们设计了一个Glance and Focus的框架，将这一思路建模为了一个序列决策过程，如下图所示。
其具体执行流程为：
 首先，对于一张任意给定的输入图片，由于我们没有任何关于它的先验知识，我们直接将其放缩为一个patch的大小，输入网络，这一方面产生了一个初步的判断结果，另一方面也提供了原始输入图片的空间分布信息；这一阶段称为扫视（Glance）。 而后，我们再以这些基本的空间分布信息为基础，逐步从原图上取得高分辨率的patch，将其不断输入网络，以此逐步更新预测结果和空间分布信息，得到更为准确的判断，并逐步寻找神经网络尚未见到过的关键区域；这一阶段称为关注（Focus）。  值得注意的是，在上述序列过程的每一步结束之后，我们会将神经网络的预测自信度（confidence）与一个预先定义的阈值进行比较，一旦confidence超过阈值，我们便视为网络已经得到了可信的结果，这一过程立即终止。此机制称为自适应推理（Adaptive Inference）。通过这种机制，我们一方面可以使不同难易度的样本具有不同的序列长度，从而动态分配计算量、提高整体效率；另一方面可以简单地通过改变阈值调整网络的整体计算开销，而不需要重新训练网络，这使得我们的模型可以动态地以最小的计算开销达到所需的性能，或者实时最大化地利用所有可用的计算资源以提升模型表现。
3. 显而易见的困难 how to identify class-ciscriminative regions? 如何识别阶级歧视性区域？
how to determine the number of class-discriminative regions? 如何确定类区分区域的数量？
4. 网络结构 GFNet共有四个组件，分别为：
全局编码器${f_g}$和局部编码器${f_l}$ （Global Encoder and Local Encoder）为两个CNN，分别用于从放缩后的原图和局部patch中提取信息，之所以用两个CNN，是因为我们发现一个CNN很难同时适应缩略图和局部patch两种尺度（scale）的输入。几乎所有现有的网络结构均可以作为这两个编码器以提升其推理效率（如MobileNet-V3、EfficientNet、RegNet等）。 分类器 [公式] （Classifier）为一个循环神经网络（RNN），输入为全局池化后的特征向量，用于整合过去所有输入的信息，以得到目前最优的分类结果。 图像块选择网络 [公式] （Patch Proposal Network）是另一个循环神经网络（RNN），输入为全局池化前的特征图（不做池化是为了避免损失空间信息），用于整合目前为止所有的空间分布信息，并决定下一个patch的位置。值得注意的是由于取得patch的crop操作不可求导，[公式]是使用强化学习中的策略梯度方法（policy gradient）训练的。</description>
    </item>
    
    <item>
      <title>25——Noisy Student训练</title>
      <link>https://ioyy900205.github.io/post/2021-06-04-24noisy-student%E6%8F%90%E9%AB%98%E7%B2%BE%E5%BA%A6%E7%9A%84%E7%A5%9E%E5%99%A8/</link>
      <pubDate>Fri, 04 Jun 2021 14:47:02 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-04-24noisy-student%E6%8F%90%E9%AB%98%E7%B2%BE%E5%BA%A6%E7%9A%84%E7%A5%9E%E5%99%A8/</guid>
      <description>想要提高模型的精度和鲁棒性，尝试考虑使用无标注数据！
实际在普通的 ImageNet 数据集上，近年来的模型不断在刷新 Top-1 识别率，幅度不高，趋近于饱和，但是该模型在一些鲁棒性测试数据集上的提高确实惊人的。
论文地址：https://arxiv.org/pdf/1911.04252.pdf
  1. 数据集简介 2. 2.Self-training 方法 3. Noisy Student  1. 数据集简介 mage-A/C/P 是三种不同的鲁棒性测试数据集。
Image-A: A 是 Adversarial，对抗的意思。从真实世界搜集了 7500 张未经修改、完全自然生成的图片作为对抗样本测试常规 ImageNet 下训练的模型鲁棒性，以 DenseNet-121 为例，其测试准确率仅为 2%，准确率下降了约 90%，由此可知该数据集的难度。
其中红色的是标签是 ResNet-50 模型给出的，黑色的标签是实际真实标签，而且以很高的信任度识别错误，要注意虽然这些样本是对抗样本，但都是来自真实世界未加对抗调整的自然图片。
Image-C/P：C 是 Corruption 腐蚀、污染的意思，即在图片中引入 75 种不同的噪音形式，测试模型的抗击能力。
这里给出 15 种不同类型的算法干扰，如引入噪声，模糊处理，模仿天气因素干扰以及基于色彩空间数字化干扰等；P 是 Perturbations 扰动的意思，它与 C 差不多，但是会在一个连续的时间步内连续对一个干净的原始图像做处理，且每一次处理都是微小的扰动，比如平移像素点，旋转，缩放以及 C 中使用的干扰模糊等扰动。
2. 2.Self-training 方法 Self-training是最简单的半监督方法之一，其主要思想是找到一种方法，用未标记的数据集来扩充已标记的数据集。算法流程如下：
（1）首先，利用已标记的数据来训练一个好的模型，然后使用这个模型对未标记的数据进行标记。
（2）然后，进行伪标签的生成，因为我们知道，已训练好的模型对未标记数据的所有预测都不可能都是好的，因此对于经典的Self-training，通常是使用分数阈值过滤部分预测，以选择出未标记数据的预测标签的一个子集。
（3）其次，将生成的伪标签与原始的标记数据相结合，并在合并后数据上进行联合训练。
（4）整个过程可以重复n次，直到达到收敛。
3. Noisy Student Self-training是利用未标记数据的好方法。但这篇来自Google的文章却强调了Noisy Student。怎么回事？它与经典方法有什么不同吗？
Noisy Student的作者发现，要使这种方法发挥作用，student model在训练过程中应加噪声，如dropout, stochastic depth andaugmentation等。而teacher model在产生伪标签时不应加噪声。因为Noisy 是整个算法的一个重要部分，所以他们称之为“Noisy Student”。</description>
    </item>
    
    <item>
      <title>23-【pytorch Lightning】</title>
      <link>https://ioyy900205.github.io/post/2021-06-03-23pytorch-lightning/</link>
      <pubDate>Thu, 03 Jun 2021 15:13:20 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-03-23pytorch-lightning/</guid>
      <description>大部分的DL/ML代码都可以分为以下这三部分：
 研究代码 Research code (LightningModule ) 工程代码 Engineering code ( Trainer ) 非必要代码 Non-essential code ( Callbacks )   @TOC
研究代码  参考资料</description>
    </item>
    
    <item>
      <title>22——VSCode中plt.show()无法显示图片问题</title>
      <link>https://ioyy900205.github.io/post/2021-06-03-22vscode%E4%B8%ADplt.show%E6%97%A0%E6%B3%95%E6%98%BE%E7%A4%BA%E5%9B%BE%E7%89%87%E9%97%AE%E9%A2%98/</link>
      <pubDate>Thu, 03 Jun 2021 14:24:36 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-03-22vscode%E4%B8%ADplt.show%E6%97%A0%E6%B3%95%E6%98%BE%E7%A4%BA%E5%9B%BE%E7%89%87%E9%97%AE%E9%A2%98/</guid>
      <description>远程SSh时候，VSCode无法显示图片
  1. 解决方法  1. 解决方法 终端中输入：
export DISPLAY=:10.0  参考资料 https://www.pythonheidong.com/blog/article/714470/11e6dbde8921d93ae464/</description>
    </item>
    
    <item>
      <title>timm库的使用</title>
      <link>https://ioyy900205.github.io/post/2021-06-02-timm%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Wed, 02 Jun 2021 11:02:33 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-06-02-timm%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/</guid>
      <description>在研究transformer时，发现timm库可以直接使用预训练的模型。
  1. timm简介 2. 已有的预训练模型 3. 验证模式查看预训练结果 4. 使用ViT模型 5. 使用tf_efficientnet_l2_ns模型  1. timm简介 PyTorchImageModels，简称timm，是一个巨大的PyTorch代码集合
旨在将各种SOTA模型整合在一起，并具有复现ImageNet训练结果的能力。
2. 已有的预训练模型 &#39;adv_inception_v3&#39;,&#39;cait_m36_384&#39;,&#39;cait_m48_448&#39;,&#39;cait_s24_224&#39;,&#39;cait_s24_384&#39;,&#39;cait_s36_384&#39;,&#39;cait_xs24_384&#39;,&#39;cait_xxs24_224&#39;,&#39;cait_xxs24_384&#39;,&#39;cait_xxs36_224&#39;,&#39;cait_xxs36_384&#39;,&#39;coat_lite_mini&#39;,&#39;coat_lite_tiny&#39;,&#39;cspdarknet53&#39;,&#39;cspresnet50&#39;,&#39;cspresnext50&#39;,&#39;densenet121&#39;,&#39;densenet161&#39;,&#39;densenet169&#39;,&#39;densenet201&#39;,&#39;densenetblur121d&#39;,&#39;dla34&#39;,&#39;dla46_c&#39;,&#39;dla46x_c&#39;,&#39;dla60&#39;,&#39;dla60_res2net&#39;,&#39;dla60_res2next&#39;,&#39;dla60x&#39;,&#39;dla60x_c&#39;,&#39;dla102&#39;,&#39;dla102x&#39;,&#39;dla102x2&#39;,&#39;dla169&#39;,&#39;dm_nfnet_f0&#39;,&#39;dm_nfnet_f1&#39;,&#39;dm_nfnet_f2&#39;,&#39;dm_nfnet_f3&#39;,&#39;dm_nfnet_f4&#39;,&#39;dm_nfnet_f5&#39;,&#39;dm_nfnet_f6&#39;,&#39;dpn68&#39;,&#39;dpn68b&#39;,&#39;dpn92&#39;,&#39;dpn98&#39;,&#39;dpn107&#39;,&#39;dpn131&#39;,&#39;eca_nfnet_l0&#39;,&#39;eca_nfnet_l1&#39;,&#39;ecaresnet26t&#39;,&#39;ecaresnet50d&#39;,&#39;ecaresnet50d_pruned&#39;,&#39;ecaresnet50t&#39;,&#39;ecaresnet101d&#39;,&#39;ecaresnet101d_pruned&#39;,&#39;ecaresnet269d&#39;,&#39;ecaresnetlight&#39;,&#39;efficientnet_b0&#39;,&#39;efficientnet_b1&#39;,&#39;efficientnet_b1_pruned&#39;,&#39;efficientnet_b2&#39;,&#39;efficientnet_b2_pruned&#39;,&#39;efficientnet_b3&#39;,&#39;efficientnet_b3_pruned&#39;,&#39;efficientnet_b4&#39;,&#39;efficientnet_el&#39;,&#39;efficientnet_el_pruned&#39;,&#39;efficientnet_em&#39;,&#39;efficientnet_es&#39;,&#39;efficientnet_es_pruned&#39;,&#39;efficientnet_lite0&#39;,&#39;efficientnetv2_rw_s&#39;,&#39;ens_adv_inception_resnet_v2&#39;,&#39;ese_vovnet19b_dw&#39;,&#39;ese_vovnet39b&#39;,&#39;fbnetc_100&#39;,&#39;gernet_l&#39;,&#39;gernet_m&#39;,&#39;gernet_s&#39;,&#39;ghostnet_100&#39;,&#39;gluon_inception_v3&#39;,&#39;gluon_resnet18_v1b&#39;,&#39;gluon_resnet34_v1b&#39;,&#39;gluon_resnet50_v1b&#39;,&#39;gluon_resnet50_v1c&#39;,&#39;gluon_resnet50_v1d&#39;,&#39;gluon_resnet50_v1s&#39;,&#39;gluon_resnet101_v1b&#39;,&#39;gluon_resnet101_v1c&#39;,&#39;gluon_resnet101_v1d&#39;,&#39;gluon_resnet101_v1s&#39;,&#39;gluon_resnet152_v1b&#39;,&#39;gluon_resnet152_v1c&#39;,&#39;gluon_resnet152_v1d&#39;,&#39;gluon_resnet152_v1s&#39;,&#39;gluon_resnext50_32x4d&#39;,&#39;gluon_resnext101_32x4d&#39;,&#39;gluon_resnext101_64x4d&#39;,&#39;gluon_senet154&#39;,&#39;gluon_seresnext50_32x4d&#39;,&#39;gluon_seresnext101_32x4d&#39;,&#39;gluon_seresnext101_64x4d&#39;,&#39;gluon_xception65&#39;,&#39;hardcorenas_a&#39;,&#39;hardcorenas_b&#39;,&#39;hardcorenas_c&#39;,&#39;hardcorenas_d&#39;,&#39;hardcorenas_e&#39;,&#39;hardcorenas_f&#39;,&#39;hrnet_w18&#39;,&#39;hrnet_w18_small&#39;,&#39;hrnet_w18_small_v2&#39;,&#39;hrnet_w30&#39;,&#39;hrnet_w32&#39;,&#39;hrnet_w40&#39;,&#39;hrnet_w44&#39;,&#39;hrnet_w48&#39;,&#39;hrnet_w64&#39;,&#39;ig_resnext101_32x8d&#39;,&#39;ig_resnext101_32x16d&#39;,&#39;ig_resnext101_32x32d&#39;,&#39;ig_resnext101_32x48d&#39;,&#39;inception_resnet_v2&#39;,&#39;inception_v3&#39;,&#39;inception_v4&#39;,&#39;legacy_senet154&#39;,&#39;legacy_seresnet18&#39;,&#39;legacy_seresnet34&#39;,&#39;legacy_seresnet50&#39;,&#39;legacy_seresnet101&#39;,&#39;legacy_seresnet152&#39;,&#39;legacy_seresnext26_32x4d&#39;,&#39;legacy_seresnext50_32x4d&#39;,&#39;legacy_seresnext101_32x4d&#39;,&#39;mixer_b16_224&#39;,&#39;mixer_b16_224_in21k&#39;,&#39;mixer_l16_224&#39;,&#39;mixer_l16_224_in21k&#39;,&#39;mixnet_l&#39;,&#39;mixnet_m&#39;,&#39;mixnet_s&#39;,&#39;mixnet_xl&#39;,&#39;mnasnet_100&#39;,&#39;mobilenetv2_100&#39;,&#39;mobilenetv2_110d&#39;,&#39;mobilenetv2_120d&#39;,&#39;mobilenetv2_140&#39;,&#39;mobilenetv3_large_100&#39;,&#39;mobilenetv3_large_100_miil&#39;,&#39;mobilenetv3_large_100_miil_in21k&#39;,&#39;mobilenetv3_rw&#39;,&#39;nasnetalarge&#39;,&#39;nf_regnet_b1&#39;,&#39;nf_resnet50&#39;,&#39;nfnet_l0&#39;,&#39;pit_b_224&#39;,&#39;pit_b_distilled_224&#39;,&#39;pit_s_224&#39;,&#39;pit_s_distilled_224&#39;,&#39;pit_ti_224&#39;,&#39;pit_ti_distilled_224&#39;,&#39;pit_xs_224&#39;,&#39;pit_xs_distilled_224&#39;,&#39;pnasnet5large&#39;,&#39;regnetx_002&#39;,&#39;regnetx_004&#39;,&#39;regnetx_006&#39;,&#39;regnetx_008&#39;,&#39;regnetx_016&#39;,&#39;regnetx_032&#39;,&#39;regnetx_040&#39;,&#39;regnetx_064&#39;,&#39;regnetx_080&#39;,&#39;regnetx_120&#39;,&#39;regnetx_160&#39;,&#39;regnetx_320&#39;,&#39;regnety_002&#39;,&#39;regnety_004&#39;,&#39;regnety_006&#39;,&#39;regnety_008&#39;,&#39;regnety_016&#39;,&#39;regnety_032&#39;,&#39;regnety_040&#39;,&#39;regnety_064&#39;,&#39;regnety_080&#39;,&#39;regnety_120&#39;,&#39;regnety_160&#39;,&#39;regnety_320&#39;,&#39;repvgg_a2&#39;,&#39;repvgg_b0&#39;,&#39;repvgg_b1&#39;,&#39;repvgg_b1g4&#39;,&#39;repvgg_b2&#39;,&#39;repvgg_b2g4&#39;,&#39;repvgg_b3&#39;,&#39;repvgg_b3g4&#39;,&#39;res2net50_14w_8s&#39;,&#39;res2net50_26w_4s&#39;,&#39;res2net50_26w_6s&#39;,&#39;res2net50_26w_8s&#39;,&#39;res2net50_48w_2s&#39;,&#39;res2net101_26w_4s&#39;,&#39;res2next50&#39;,&#39;resnest14d&#39;,&#39;resnest26d&#39;,&#39;resnest50d&#39;,&#39;resnest50d_1s4x24d&#39;,&#39;resnest50d_4s2x40d&#39;,&#39;resnest101e&#39;,&#39;resnest200e&#39;,&#39;resnest269e&#39;,&#39;resnet18&#39;,&#39;resnet18d&#39;,&#39;resnet26&#39;,&#39;resnet26d&#39;,&#39;resnet34&#39;,&#39;resnet34d&#39;,&#39;resnet50&#39;,&#39;resnet50d&#39;,&#39;resnet101d&#39;,&#39;resnet152d&#39;,&#39;resnet200d&#39;,&#39;resnetblur50&#39;,&#39;resnetrs50&#39;,&#39;resnetrs101&#39;,&#39;resnetrs152&#39;,&#39;resnetrs200&#39;,&#39;resnetrs270&#39;,&#39;resnetrs350&#39;,&#39;resnetrs420&#39;,&#39;resnetv2_50x1_bitm&#39;,&#39;resnetv2_50x1_bitm_in21k&#39;,&#39;resnetv2_50x3_bitm&#39;,&#39;resnetv2_50x3_bitm_in21k&#39;,&#39;resnetv2_101x1_bitm&#39;,&#39;resnetv2_101x1_bitm_in21k&#39;,&#39;resnetv2_101x3_bitm&#39;,&#39;resnetv2_101x3_bitm_in21k&#39;,&#39;resnetv2_152x2_bitm&#39;,&#39;resnetv2_152x2_bitm_in21k&#39;,&#39;resnetv2_152x4_bitm&#39;,&#39;resnetv2_152x4_bitm_in21k&#39;,&#39;resnext50_32x4d&#39;,&#39;resnext50d_32x4d&#39;,&#39;resnext101_32x8d&#39;,&#39;rexnet_100&#39;,&#39;rexnet_130&#39;,&#39;rexnet_150&#39;,&#39;rexnet_200&#39;,&#39;selecsls42b&#39;,&#39;selecsls60&#39;,&#39;selecsls60b&#39;,&#39;semnasnet_100&#39;,&#39;seresnet50&#39;,&#39;seresnet152d&#39;,&#39;seresnext26d_32x4d&#39;,&#39;seresnext26t_32x4d&#39;,&#39;seresnext50_32x4d&#39;,&#39;skresnet18&#39;,&#39;skresnet34&#39;,&#39;skresnext50_32x4d&#39;,&#39;spnasnet_100&#39;,&#39;ssl_resnet18&#39;,&#39;ssl_resnet50&#39;,&#39;ssl_resnext50_32x4d&#39;,&#39;ssl_resnext101_32x4d&#39;,&#39;ssl_resnext101_32x8d&#39;,&#39;ssl_resnext101_32x16d&#39;,&#39;swin_base_patch4_window7_224&#39;,&#39;swin_base_patch4_window7_224_in22k&#39;,&#39;swin_base_patch4_window12_384&#39;,&#39;swin_base_patch4_window12_384_in22k&#39;,&#39;swin_large_patch4_window7_224&#39;,&#39;swin_large_patch4_window7_224_in22k&#39;,&#39;swin_large_patch4_window12_384&#39;,&#39;swin_large_patch4_window12_384_in22k&#39;,&#39;swin_small_patch4_window7_224&#39;,&#39;swin_tiny_patch4_window7_224&#39;,&#39;swsl_resnet18&#39;,&#39;swsl_resnet50&#39;,&#39;swsl_resnext50_32x4d&#39;,&#39;swsl_resnext101_32x4d&#39;,&#39;swsl_resnext101_32x8d&#39;,&#39;swsl_resnext101_32x16d&#39;,&#39;tf_efficientnet_b0&#39;,&#39;tf_efficientnet_b0_ap&#39;,&#39;tf_efficientnet_b0_ns&#39;,&#39;tf_efficientnet_b1&#39;,&#39;tf_efficientnet_b1_ap&#39;,&#39;tf_efficientnet_b1_ns&#39;,&#39;tf_efficientnet_b2&#39;,&#39;tf_efficientnet_b2_ap&#39;,&#39;tf_efficientnet_b2_ns&#39;,&#39;tf_efficientnet_b3&#39;,&#39;tf_efficientnet_b3_ap&#39;,&#39;tf_efficientnet_b3_ns&#39;,&#39;tf_efficientnet_b4&#39;,&#39;tf_efficientnet_b4_ap&#39;,&#39;tf_efficientnet_b4_ns&#39;,&#39;tf_efficientnet_b5&#39;,&#39;tf_efficientnet_b5_ap&#39;,&#39;tf_efficientnet_b5_ns&#39;,&#39;tf_efficientnet_b6&#39;,&#39;tf_efficientnet_b6_ap&#39;,&#39;tf_efficientnet_b6_ns&#39;,&#39;tf_efficientnet_b7&#39;,&#39;tf_efficientnet_b7_ap&#39;,&#39;tf_efficientnet_b7_ns&#39;,&#39;tf_efficientnet_b8&#39;,&#39;tf_efficientnet_b8_ap&#39;,&#39;tf_efficientnet_cc_b0_4e&#39;,&#39;tf_efficientnet_cc_b0_8e&#39;,&#39;tf_efficientnet_cc_b1_8e&#39;,&#39;tf_efficientnet_el&#39;,&#39;tf_efficientnet_em&#39;,&#39;tf_efficientnet_es&#39;,&#39;tf_efficientnet_l2_ns&#39;,&#39;tf_efficientnet_l2_ns_475&#39;,&#39;tf_efficientnet_lite0&#39;,&#39;tf_efficientnet_lite1&#39;,&#39;tf_efficientnet_lite2&#39;,&#39;tf_efficientnet_lite3&#39;,&#39;tf_efficientnet_lite4&#39;,&#39;tf_efficientnetv2_b0&#39;,&#39;tf_efficientnetv2_b1&#39;,&#39;tf_efficientnetv2_b2&#39;,&#39;tf_efficientnetv2_b3&#39;,&#39;tf_efficientnetv2_l&#39;,&#39;tf_efficientnetv2_l_in21ft1k&#39;,&#39;tf_efficientnetv2_l_in21k&#39;,&#39;tf_efficientnetv2_m&#39;,&#39;tf_efficientnetv2_m_in21ft1k&#39;,&#39;tf_efficientnetv2_m_in21k&#39;,&#39;tf_efficientnetv2_s&#39;,&#39;tf_efficientnetv2_s_in21ft1k&#39;,&#39;tf_efficientnetv2_s_in21k&#39;,&#39;tf_inception_v3&#39;,&#39;tf_mixnet_l&#39;,&#39;tf_mixnet_m&#39;,&#39;tf_mixnet_s&#39;,&#39;tf_mobilenetv3_large_075&#39;,&#39;tf_mobilenetv3_large_100&#39;,&#39;tf_mobilenetv3_large_minimal_100&#39;,&#39;tf_mobilenetv3_small_075&#39;,&#39;tf_mobilenetv3_small_100&#39;,&#39;tf_mobilenetv3_small_minimal_100&#39;,&#39;tnt_s_patch16_224&#39;,&#39;tresnet_l&#39;,&#39;tresnet_l_448&#39;,&#39;tresnet_m&#39;,&#39;tresnet_m_448&#39;,&#39;tresnet_m_miil_in21k&#39;,&#39;tresnet_xl&#39;,&#39;tresnet_xl_448&#39;,&#39;tv_densenet121&#39;,&#39;tv_resnet34&#39;,&#39;tv_resnet50&#39;,&#39;tv_resnet101&#39;,&#39;tv_resnet152&#39;,&#39;tv_resnext50_32x4d&#39;,&#39;vgg11&#39;,&#39;vgg11_bn&#39;,&#39;vgg13&#39;,&#39;vgg13_bn&#39;,&#39;vgg16&#39;,&#39;vgg16_bn&#39;,&#39;vgg19&#39;,&#39;vgg19_bn&#39;,&#39;vit_base_patch16_224&#39;,&#39;vit_base_patch16_224_in21k&#39;,&#39;vit_base_patch16_224_miil&#39;,&#39;vit_base_patch16_224_miil_in21k&#39;,&#39;vit_base_patch16_384&#39;,&#39;vit_base_patch32_224_in21k&#39;,&#39;vit_base_patch32_384&#39;,&#39;vit_base_r50_s16_224_in21k&#39;,&#39;vit_base_r50_s16_384&#39;,&#39;vit_deit_base_distilled_patch16_224&#39;,&#39;vit_deit_base_distilled_patch16_384&#39;,&#39;vit_deit_base_patch16_224&#39;,&#39;vit_deit_base_patch16_384&#39;,&#39;vit_deit_small_distilled_patch16_224&#39;,&#39;vit_deit_small_patch16_224&#39;,&#39;vit_deit_tiny_distilled_patch16_224&#39;,&#39;vit_deit_tiny_patch16_224&#39;,&#39;vit_large_patch16_224&#39;,&#39;vit_large_patch16_224_in21k&#39;,&#39;vit_large_patch16_384&#39;,&#39;vit_large_patch32_224_in21k&#39;,&#39;vit_large_patch32_384&#39;,&#39;vit_small_patch16_224&#39;,&#39;wide_resnet50_2&#39;,&#39;wide_resnet101_2&#39;,&#39;xception&#39;,&#39;xception41&#39;,&#39;xception65&#39;,&#39;xception71&#39; 3.</description>
    </item>
    
    <item>
      <title>【现场分享】智源大会类脑视觉</title>
      <link>https://ioyy900205.github.io/post/2021-05-31-%E7%8E%B0%E5%9C%BA%E5%88%86%E4%BA%AB%E6%99%BA%E6%BA%90%E5%A4%A7%E4%BC%9A%E7%B1%BB%E8%84%91%E8%A7%86%E8%A7%89/</link>
      <pubDate>Mon, 31 May 2021 10:41:41 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-31-%E7%8E%B0%E5%9C%BA%E5%88%86%E4%BA%AB%E6%99%BA%E6%BA%90%E5%A4%A7%E4%BC%9A%E7%B1%BB%E8%84%91%E8%A7%86%E8%A7%89/</guid>
      <description>【现场分享】智源大会类脑视觉
 @TOC
标题 黄铁军
唐华锦
张兆翔
王威
类脑 拓展马尔视觉计算原理
研究内容 计算 成像 应用
 参考资料</description>
    </item>
    
    <item>
      <title>21——MSDNet论文的推理模块</title>
      <link>https://ioyy900205.github.io/post/2021-05-22-21msdnet%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Sat, 22 May 2021 10:21:24 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-22-21msdnet%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</guid>
      <description>本文的核心主旨在于：在计算资源限制下对不同的图像进行不同的处理，可以理解成对于简单样本采用简单的方式处理，对于复杂样本则尽可能给其分配资源，以避免不必要的资源浪费节省计算量，并且在这种推理的思想上要实现网络对数据的自动适应。所以本文设计了一个新颖的二维多尺度网络结构，根据不同的资源需求训练了多个分类器，为了最大程度地重用分类器之间的计算，我们将它们作为早期出口并入单个深度卷积神经网络，并通过密集连接将它们互连，该构架在整个网络中同时保持粗略和精细的scale，获得了良好的效果。
  1. model 2. 推理部分  2.1. 实时推理方法 2.2. 出口分配   3. 讨论  1. model 这里有两个基本模块，一个是第一层的横向传播模块，一个是下采样加横向传播模块。
2. 推理部分 2.1. 实时推理方法 其实就是将每个出口的结果打印出来。
只不过在推理过程中，考虑了载入数据的时间（大约0.45s），整个batch的推理时间大约是0.6-0.7s&amp;rsquo;s之间。
程序在最后的地方对每个出口进行了输出
 prec@1 56.632 prec@5 79.942 prec@1 65.136 prec@5 86.252 prec@1 68.420 prec@5 88.632 prec@1 69.770 prec@5 89.418 prec@1 71.336 prec@5 90.364   2.2. 出口分配 这种方式对5个出口进行了样本数量设定（这里设置了40组）。进而可以学习到每个出口的threshold，从而实现了不同出口的退出机制。
3. 讨论 每个出口样本数量设定 是一个超参数。MSDNet所得到的结果目前来看不一定是最好的。
以下有收获：
 多尺度架构的编程方法。 网络模型参数和计算量的计算。 退出机制的学习方法。   参考资料
手动debug</description>
    </item>
    
    <item>
      <title>20——torch_view操作指南</title>
      <link>https://ioyy900205.github.io/post/2021-05-21-20torch_view%E6%93%8D%E4%BD%9C%E6%8C%87%E5%8D%97/</link>
      <pubDate>Fri, 21 May 2021 18:13:22 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-21-20torch_view%E6%93%8D%E4%BD%9C%E6%8C%87%E5%8D%97/</guid>
      <description>还是发现对view的操作不够深入，这里做了一个小demo。自行体会一下
  1. 代码 2. 结果 3. 理解  1. 代码 &amp;#39;&amp;#39;&amp;#39; Date: 2021-05-21 15:43:43 LastEditors: Liuliang LastEditTime: 2021-05-21 15:50:36 Description: view &amp;#39;&amp;#39;&amp;#39; import torch a = torch.arange(0,12,1) print(a) b = a.view(2,-1) print(b) c = b.view(6,2) print(c) 2. 结果 tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) tensor([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) tensor([[ 0, 1], [ 2, 3], [ 4, 5], [ 6, 7], [ 8, 9], [10, 11]]) 3.</description>
    </item>
    
    <item>
      <title>19——avg_pool2d</title>
      <link>https://ioyy900205.github.io/post/2021-05-21-19avg_pool2d/</link>
      <pubDate>Fri, 21 May 2021 15:06:07 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-21-19avg_pool2d/</guid>
      <description>avg——pool2d理解
 @TOC
1. 直接上代码 import torch from torch.nn import functional as F # 1.初始化 input = torch.tensor( [ [1,1,1,1,1], [1,1,1,1,1], [0,0,0,1,1], [1,1,1,1,1] ]).unsqueeze(0).float() print(input.size())#torch.Size([1, 4, 5]) print(input) #2. avg_pool1d # m1 = F.avg_pool1d(input,kernel_size=2) # print(m1)#tensor([[[1.0000, 1.0000], # # [1.0000, 1.0000], # # [0.0000, 0.5000], # # [1.0000, 1.0000]]]) #3. avg_pool2d # m = F.avg_pool2d(input,kernel_size=2)#这里是在原矩阵中找出2*2的区域求平均，不够的舍弃，stride=(2,2) # print(m)#tensor([[[1.0000, 1.0000], # # [0.5000, 0.7500]]]) m3= F.avg_pool2d(input,kernel_size=2,stride=1)#这里是在原矩阵中找出2*2的区域求平均，不够的舍弃，stride=(1,1),[1,1,4,5]--&amp;gt;[1,1,3,4] print(m3)#tensor([[[1.0000, 1.0000, 1.0000, 1.0000], # [0.5000, 0.</description>
    </item>
    
    <item>
      <title>18——bottleneck结构解析和make_layers解析</title>
      <link>https://ioyy900205.github.io/post/2021-05-21-18bottleneck%E7%BB%93%E6%9E%84%E8%A7%A3%E6%9E%90%E5%92%8Cmake_layers%E8%A7%A3%E6%9E%90/</link>
      <pubDate>Fri, 21 May 2021 11:37:47 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-21-18bottleneck%E7%BB%93%E6%9E%84%E8%A7%A3%E6%9E%90%E5%92%8Cmake_layers%E8%A7%A3%E6%9E%90/</guid>
      <description>bottleneck
make_layers
  1. bottleneck 2. make_layers 3. DenseNet模型  1. bottleneck class Bottleneck(nn.Module): def __init__(self, in_planes, growth_rate): super(Bottleneck, self).__init__() self.bn1 = nn.BatchNorm2d(in_planes) self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False) self.bn2 = nn.BatchNorm2d(4*growth_rate) self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False) def forward(self, x): out = self.conv1(F.relu(self.bn1(x))) out = self.conv2(F.relu(self.bn2(out))) out = torch.cat([out,x], 1) return out 这个结构叫做瓶颈结构,如果in_planes=12，growth_rate=12，输出如下：
(0): Bottleneck( (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv1): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.</description>
    </item>
    
    <item>
      <title>17——tqdm载入数据集测试</title>
      <link>https://ioyy900205.github.io/post/2021-05-20-17tqdm%E8%BD%BD%E5%85%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E6%B5%8B%E8%AF%95/</link>
      <pubDate>Thu, 20 May 2021 16:50:49 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-20-17tqdm%E8%BD%BD%E5%85%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E6%B5%8B%E8%AF%95/</guid>
      <description>代码已放在mess_around文件夹下
  1. 基本使用方式  1.1. list方式 1.2. 放一个迭代器在里面   2. 实例  2.1. 使用tqdm带来的效率损失  2.1.1. MNIST 2.1.2. ImageNet   2.2. num_workers的使用    1. 基本使用方式 1.1. list方式 （1）
for i in tqdm(range(10000)): #do something sleep(0.01) pass （2）
for char in tqdm([&amp;#34;a&amp;#34;, &amp;#34;b&amp;#34;, &amp;#34;c&amp;#34;, &amp;#34;d&amp;#34;]): #do something pass （3）
pbar = tqdm([&amp;#34;a&amp;#34;, &amp;#34;b&amp;#34;, &amp;#34;c&amp;#34;, &amp;#34;d&amp;#34;]) for char in pbar: sleep(1) pbar.set_description(&amp;#34;Processing %s&amp;#34; % char) 1.2. 放一个迭代器在里面 count = 0 for batch_idx, item in tqdm(enumerate(train_loader)): count+=batch_idx print(count) 这种方式是我使用的方式。简单来说就是把迭代器train_loader放在tqdm里面，这里增加了一个enumerate去增加一个序列。</description>
    </item>
    
    <item>
      <title>16——计算机视觉大类分类</title>
      <link>https://ioyy900205.github.io/post/2021-05-19-16%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%A4%A7%E7%B1%BB%E5%88%86%E7%B1%BB/</link>
      <pubDate>Wed, 19 May 2021 16:22:00 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-19-16%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%A4%A7%E7%B1%BB%E5%88%86%E7%B1%BB/</guid>
      <description>待完善
框架重新梳理一下
在语义感知层面下
  1. 任务分类  1. 任务分类 分类、检测、识别、分割、检索、描述、生成
 参考资料</description>
    </item>
    
    <item>
      <title>15——torch.tensor叶子节点</title>
      <link>https://ioyy900205.github.io/post/2021-05-19-15torch.tensor-%E5%8F%B6%E5%AD%90%E8%8A%82%E7%82%B9/</link>
      <pubDate>Wed, 19 May 2021 09:53:52 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-19-15torch.tensor-%E5%8F%B6%E5%AD%90%E8%8A%82%E7%82%B9/</guid>
      <description>讨论叶子节点
  1. 理顺逻辑  1.1. 元素 1.2. 计算    1. 理顺逻辑 1.1. 元素 在pytorch的计算图中，只有两种元素：
数据（tensor）
  叶子节点(leaf node)
  叶子节点可以理解成不依赖其他tensor的tensor。
  在pytorch中，神经网络层中的权值w的tensor均为叶子节点。
  自己定义的tensor例如a=torch.tensor([1.0])定义的节点是叶子节点。
  All Tensors that have requires_grad which is False will be leaf Tensors by convention.
  For Tensors that have requires_grad which is True, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so grad_fn is None.</description>
    </item>
    
    <item>
      <title>14——NFNnet论文解读</title>
      <link>https://ioyy900205.github.io/post/2021-05-18-14nfnnet%E7%BF%BB%E8%AF%91/</link>
      <pubDate>Tue, 18 May 2021 15:33:30 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-18-14nfnnet%E7%BF%BB%E8%AF%91/</guid>
      <description>Title：High-Performance Large-Scale Image Recognition Without Normalization
Time: [v1] Thu, 11 Feb 2021 18:23:20 UTC (241 KB)
link：https://arxiv.org/abs/2102.06171
github: https://github.com/deepmind/deepmind-research/tree/master/nfnets
 NFNet（Normalizer-Free ResNets）是DeepMind提出了一种不需要Batch Normalization的基于ResNet的网络结构，其核心为一种AGC（adaptive gradient clipping technique，自适应梯度裁剪）技术。如下图所示，最小的NFNet版本达到了EfficientNet-B7的准确率，并且训练速度快了8.7倍，最大版本的模型实现了新的SOTA效果。
 1. 摘要 批量归一化是大多数图像分类模型的一个关键组成部分，但它有许多不理想的特性，源于它对批量大小和例子之间的相互作用的依赖。尽管最近的工作已经成功地训练了没有归一化层的深度ResNets，但这些模型并不符合最好的批量归一化网络的测试精度，而且对于大的学习率或强大的数据增量来说往往是不稳定的。在这项工作中，我们开发了一种自适应梯度剪裁技术，克服了这些不稳定性，并设计了一类明显改进的无归一化网络。我们较小的模型与EfficientNet-B7在ImageNet上的测试精度相匹配，同时训练速度快了8.7倍，而我们最大的模型达到了86.5%的最新顶级精度。此外，在对3亿张标记图像的数据集进行大规模预训练后，在ImageNet上进行微调时，无规范化模型的性能明显优于它们的批量规范化模型，我们最好的模型获得了89.2%的准确性。
2. 引言 最近计算机视觉领域的绝大多数模型都是深度残差网络的变种（He等人，2016b;a），通过批量规范化训练（Ioffe &amp;amp; Szegedy，2015）。这两个架构创新的结合使从业者能够训练出更深的网络，在训练集和测试集上都能达到更高的精度。批量归一化还可以平滑损失景观（Santurkar等人，2018），这使得在更大的学习率和更大的批次规模下进行稳定的训练（Bjorck等人，2018；De &amp;amp; Smith，2020），而且它可以产生正则化效应（Hoffer等人，2017；Luo等人，2018）。
然而，批量规范化有三个重要的实际缺点。首先，它是一个令人惊讶的昂贵的计算基元，会产生内存开销（Rota Bulo`等人，2018），并大大增加了在一些网络中评估梯度所需的时间（Gitman &amp;amp; Ginsburg, 2017）。第二，它引入了模型在训练期间和推理时间的行为之间的差异（Summers &amp;amp; Dinneen，2019；Singh &amp;amp; Shrivastava，2019），引入了必须调整的隐藏超参数。第三，也是最重要的一点，批量规范化打破了minibatch中训练实例之间的独立性。
这第三个属性有一系列的负面后果。例如，从业者发现，采用BN的网络往往难以在不同的硬件上精确复制，BN往往是导致微妙的实施错误的原因，特别是在分布式训练期间（Pham等人，2019）。此外，批处理归一化不能用于某些任务，因为批处理中的训练实例之间的互动使网络能够 &amp;ldquo;欺骗 &amp;ldquo;某些损失函数。例如，批量归一化需要特别注意防止一些对比学习算法中的信息泄露（Chen等人，2020；He等人，2020）。这也是序列建模任务的一个主要问题，这促使语言模型采用替代的规范化器（Ba等人，2016；Vaswani等人，2017）。如果在训练过程中，批量归一化的网络有很大的方差，那么批量归一化网络的性能也会下降（Shen等人，2020）。最后，批量归一化的性能对批量大小很敏感，当批量大小太小时，批量归一化网络的性能很差（Hoffer等人，2017；Ioffe，2017；Wu &amp;amp; He，2018），这限制了我们在有限硬件上可以训练的最大模型大小。我们在附录B中阐述了与批量归一化相关的挑战。
因此，尽管BN使深度学习社区近年来取得了实质性的进展，但我们预计从长远来看，它可能会阻碍进展。我们认为社区应该寻求确定一个简单的替代方案，以实现有竞争力的测试精度，并可用于广泛的任务。虽然已经提出了一些替代的规范化器（Ba等人，2016；Wu &amp;amp; He，2018；Huang等人，2020），但这些替代物往往实现了较差的测试精度，并引入了自己的缺点，如推理时的额外计算成本。幸运的是，近年来出现了两个有前途的研究主题。第一个是研究训练期间BN的好处的来源（Balduzzi等人，2017；Santurkar等人，2018；Bjorck等人，2018；Luo等人，2018；Yang等人，2019；Jacot等人。2019年；De &amp;amp; Smith，2020年），而第二种方法是在没有归一化层的情况下将深度ResNets训练到有竞争力的精度（Hanin &amp;amp; Rolnick，2018；Zhang等人，2019a；De &amp;amp; Smith，2020；Shao等人，2020；Brock等人，2021）。
图1
许多这些工作的一个关键主题是，通过抑制剩余分支上的隐性激活的规模，可以在没有规范化的情况下训练非常深的ResNets。实现这一目标的最简单方法是在每个残差分支的末端引入一个可学习的标量，初始化为零（Goyal等人，2017；Zhang等人，2019a；De &amp;amp; Smith，2020；Bachlechner等人，2020）。然而仅靠这一招还不足以在具有挑战性的基准上获得有竞争力的测试精度。另一项工作表明，ReLU激活引入了一个 &amp;ldquo;平均转移&amp;rdquo;，这导致不同训练实例的隐藏激活随着网络深度的增加而变得越来越相关（Huang等人，2017；Jacot等人，2019）。在最近的一项工作中，Brock等人（2021）引入了 &amp;ldquo;无正则化 &amp;ldquo;的ResNets，它在初始化时抑制了残余分支，并应用Scaled Weight Standardization（Qiao等人，2019）来消除平均移动。通过额外的正则化，这些未正则化的网络在ImageNet（Russakovsky等人，2015）上与批量正则化的ResNets（He等人，2016a）的性能相匹配，但它们在大批量时并不稳定，也不符合EfficientNets（Tan &amp;amp; Le，2019）的性能，即目前的技术状态（Gong等人，2020）。本文在这一工作思路的基础上，试图解决这些核心限制。我们的主要贡献如下:</description>
    </item>
    
    <item>
      <title>13————tensor内存占用计算实例</title>
      <link>https://ioyy900205.github.io/post/2021-05-17-13tensor%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8%E8%AE%A1%E7%AE%97%E5%AE%9E%E4%BE%8B/</link>
      <pubDate>Mon, 17 May 2021 19:01:17 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-17-13tensor%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8%E8%AE%A1%E7%AE%97%E5%AE%9E%E4%BE%8B/</guid>
      <description>网上查看了CSDN的代码，写的有误。所以这里写一个正确版本的
  ## 1. 各类型所占用的字节数 2. tensor内存计算 3. 清空内存  1. 各类型所占用的字节数  测试代码
import numpy as np import sys # 32位整型 ai32 = np.array([], dtype=np.int32) bi32 = np.arange(1, dtype=np.int32) ci32 = np.arange(5, dtype=np.int32) # 64位整型 ai64 = np.array([], dtype=np.int64) bi64 = np.arange(1, dtype=np.int64) ci64 = np.arange(5, dtype=np.int64) # 32位浮点数 af32 = np.array([], dtype=np.float32) bf32 = np.arange(1, dtype=np.float32) cf32 = np.arange(5, dtype=np.float32) # 64位浮点数 af64 = np.array([], dtype=np.float64) bf64 = np.</description>
    </item>
    
    <item>
      <title>12————阅读论文记录</title>
      <link>https://ioyy900205.github.io/post/2021-05-14-12%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87%E9%93%BE%E6%8E%A5/</link>
      <pubDate>Fri, 14 May 2021 15:20:41 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-14-12%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87%E9%93%BE%E6%8E%A5/</guid>
      <description>阅读论文记录
  1. 图像分类(Classification) 2. 目标检测(Object Detection) 3. 目标分割(Segmentation) 4. Others 5. 动态神经网络   参考资料
 1. 图像分类(Classification)  AlexNet http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf ZFNet(Visualizing and Understanding Convolutional Networks) https://arxiv.org/abs/1311.2901 VGG https://arxiv.org/abs/1409.1556 GoogLeNet, Inceptionv1(Going deeper with convolutions) https://arxiv.org/abs/1409.4842 Batch Normalization https://arxiv.org/abs/1502.03167 Inceptionv3(Rethinking the Inception Architecture for Computer Vision) https://arxiv.org/abs/1512.00567 Inceptionv4, Inception-ResNet https://arxiv.org/abs/1602.07261 Xception(Deep Learning with Depthwise Separable Convolutions) https://arxiv.org/abs/1610.02357 ResNet https://arxiv.org/abs/1512.03385 ResNeXt https://arxiv.org/abs/1611.05431 DenseNet https://arxiv.org/abs/1608.06993 NASNet-A(Learning Transferable Architectures for Scalable Image Recognition) https://arxiv.</description>
    </item>
    
    <item>
      <title>11——SSD:Single Shot MultiBox Detector</title>
      <link>https://ioyy900205.github.io/post/2021-05-13-11ssd_single-shot-multibox-detector-copy/</link>
      <pubDate>Thu, 13 May 2021 10:40:00 +0800</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-13-11ssd_single-shot-multibox-detector-copy/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. background 2. 目标损失函数 3. 多尺度多比例默认框 4. 负样本均衡 5. 数据增强 6. 性能比较  6.1. 在VOC2007测试集上的检测性能 6.2. 在VOC2012测试集上的检测性能 6.3. 在COCO测试集上的检测性能   7. 总结  1. background ECCV2016 author:wei liu 实现真正的实时
2016年，Wei Liu等人提出了SSD算法，它是继YOLOv1算法提出后的又一单阶段目标检测算法。在当时，单阶段算法相比于以R-CNN系列为主的双阶段算法更快，所以被学术界和工业界所青睐。为了提高检测的精度，SSD创造性地提出了多尺度预测的目标检测算法，使得网络更容易适应对不同大小物体的检测。
放上对比结构如下： 我的理解是 网络抽取不同的特征层进行预测，较低的特征层可以预测教细节物体，较高特征层可以预测教大物体.
整体结构上，SSD采用了VGG-16作为骨干网络提取特征。在骨干网络末尾添加了若干个特征层，特征层与特征层之间使用1x1和3x3的卷积核计算特征和降采样，并从中选择了6个不同大小的特征层来预测目标。
需要注意的是，用来预测目标的每一个特征层上预设的Anchors数量不一定是相同的。比如对于38x38的特征层，每个格子预设4个不同比例的Anchors；对于19x19的特征层，每个格子预设了6个不同比例的Anchors。
2. 目标损失函数 每一个目标检测网络都具有其独特的目标损失函数，它决定着网络训练迭代的方向。
根据目标检测网络的一般定义，我们知道总损失函数一般由两部分组成：定位损失和置信度损失。
其中，定位损失计算公式为：
定位损失函数借鉴了Faster R-CNN中的Smooth L1函数，用来计算预测边界框 l 和真实边界框 g 之间的损失误差。其中，m 是代表边界框位置信息的集合{cx，cy，w，h}。
容易看出，SSD没有直接将目标的中心位置、长宽作为真实标签。而是采取真实框与预选框Anchors的偏移作为真实标签。那么在学习的过程中，也会去主动预测偏移值，这样更有利于网络的训练，避免在训练初期产生较大的振荡。
置信度损失函数是在多个类别置信度c上的softmax损失：
3. 多尺度多比例默认框 前面已经提到，SSD算法中采用了类似于Faster R-CNN中的Anchors机制用于目标框的回归。值得一提的是，SSD采用的Anchors方法更为灵活，不仅在每一个特征层设置了不同大小和比例的预选框，而且针对于不同的特征层，也设计了相应的大小的预选框。
经过计算可知，对于越深的特征层（尺寸越小），设置的预选框尺寸越大。这是因为，尺寸越小的特征层，感受野越大。SSD的目的就是：要让感受野小的特征层检测小目标，使用感受野大的特征层检测更大的目标。
4. 负样本均衡 在训练过程中，大部分的预选框Anchors并不能和真实框匹配上，因此负样本很多，而正样本却很少，正负样本数量严重失衡，不利于训练。
考虑到这个问题，SSD在训练过程中，没有选取所有的负样本。而是先将负样本的置信度损失进行排序，仅选取置信度高的一些负样本，使得负样本数与正样本数之间的比率最大不超过3：1。作者认为，这样使得训练更稳定更快。
5. 数据增强 数据增强又叫数据增广，是扩大数据集的一种方式，经常用于深度学习中来防止过拟合。在SSD算法中，也使用了一些数据增强的技巧。采用数据增强后，目标检测性能也得到了明显的提升，具体实验结果如下:
6. 性能比较 6.</description>
    </item>
    
    <item>
      <title>09——Faster R-CNN</title>
      <link>https://ioyy900205.github.io/post/2021-05-12-09-faster-r-cnn/</link>
      <pubDate>Wed, 12 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-12-09-faster-r-cnn/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. 背景  1.1. 时间线 1.2. 结构  1.2.1. CNN 1.2.2. RPN 1.2.3. ROI Pooling     2. 一些问题 3. 损失函数 4. 后处理  4.1. NMS 4.2. Proposal selection 4.3. Standalone application   5. 训练 6. 推理 7. 后记  1. 背景 Faster R-CNN最初发表于NIPS 2015。发表后，它经历了几次修改。
Faster R-CNN是R-CNN论文的第三次迭代&amp;ndash;Ross Girshick是作者和合作者。
1.1. 时间线   R-CNN
Published in 2014
“Rich feature hierarchies for accurate object detection and semantic segmentation”,</description>
    </item>
    
    <item>
      <title>10——目标检测参数</title>
      <link>https://ioyy900205.github.io/post/2021-05-12-10%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%8F%82%E6%95%B0/</link>
      <pubDate>Wed, 12 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-12-10%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%8F%82%E6%95%B0/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. background Pascal VOC 和 COCO 2. precision 和 recall  2.1. 基本定义 2.2. precision-recall curve   3. COCO Evaluation Result 4. 参考  1. background Pascal VOC 和 COCO 最近看目标检测，少不了了解其中参数的意义。一顿搜所哎。
最精简的回答：
目标检测中衡量识别精度的指标是mAP（mean average precision）。多个类别物体检测中，每一个类别都可以根据recall和precision绘制一条曲线，AP就是该曲线下的面积，mAP是多个类别AP的平均值
番外篇 在coco数据集出来之前，基本都用pascal voc 现在都用coco了。
2. precision 和 recall 2.1. 基本定义  precision查准率 preicision是在你认为的正样本中， 有多大比例真的是正样本 recall查全率 recall则是在真正的正样本中， 有多少被你找到了  2.2. precision-recall curve 如果threshold太高， prediction非常严格， 所以我们认为是鸭子的基本都是鸭子，precision就高了；但也因为筛选太严格， 我们也放过了一些score比较低的鸭子， 所以recall就低了
如果threshold太低， 什么都会被当成鸭子， precision就会很低， recall就会很高。</description>
    </item>
    
    <item>
      <title>08——数据集操作和图像处理——PyTorch</title>
      <link>https://ioyy900205.github.io/post/2021-05-11-voc2012%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%9B%BE%E7%89%87%E5%92%8Cbbox%E4%BF%A1%E6%81%AF%E6%98%BE%E7%A4%BA/</link>
      <pubDate>Tue, 11 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-11-voc2012%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%9B%BE%E7%89%87%E5%92%8Cbbox%E4%BF%A1%E6%81%AF%E6%98%BE%E7%A4%BA/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. 数据集读取 2. 转换函数 3. bbox画图  1. 数据集读取 from torch.utils.data import Dataset import os import torch import json from PIL import Image from lxml import etree class VOC2012DataSet(Dataset): &amp;#34;&amp;#34;&amp;#34;读取解析PASCAL VOC2012数据集&amp;#34;&amp;#34;&amp;#34; def __init__(self, voc_root, transforms, txt_name: str = &amp;#34;train.txt&amp;#34;): self.root = os.path.join(voc_root, &amp;#34;VOCdevkit&amp;#34;, &amp;#34;VOC2012&amp;#34;) self.img_root = os.path.join(self.root, &amp;#34;JPEGImages&amp;#34;) self.annotations_root = os.path.join(self.root, &amp;#34;Annotations&amp;#34;) # read train.txt or val.txt file txt_path = os.path.join(self.root, &amp;#34;ImageSets&amp;#34;, &amp;#34;Main&amp;#34;, txt_name) assert os.</description>
    </item>
    
    <item>
      <title>06——torch.cat维度操作——PyTorch</title>
      <link>https://ioyy900205.github.io/post/2021-05-10-torch.cat%E7%9A%84%E7%BB%B4%E5%BA%A6%E6%93%8D%E4%BD%9C/</link>
      <pubDate>Mon, 10 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-10-torch.cat%E7%9A%84%E7%BB%B4%E5%BA%A6%E6%93%8D%E4%BD%9C/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. torch.cat  1.1. 二维数组  1.1.1. dim=0 1.1.2. dim=1   1.2. 三维数组  1.2.1. dim=0 1.2.2. dim=1 1.2.3. dim=2     2. 小结  1. torch.cat 1.1. 二维数组 1.1.1. dim=0 运行
import torch A = torch.ones(2,3) #2x3的张量（矩阵）  print(&amp;#34;A:&amp;#34;,A) B=2*torch.ones(4,3) #4x3的张量（矩阵）  print(&amp;#34;B:&amp;#34;,B) C=torch.cat((A,B),0)#按维数0（行）拼接 print(&amp;#34;C:&amp;#34;,C) print(C.size()) 结果
A: tensor([[1., 1., 1.], [1., 1., 1.]]) B: tensor([[2., 2., 2.], [2., 2., 2.], [2.</description>
    </item>
    
    <item>
      <title>07——torch.stack维度操作——PyTorch</title>
      <link>https://ioyy900205.github.io/post/2021-05-10-torch.stack%E7%9A%84%E7%BB%B4%E5%BA%A6%E6%93%8D%E4%BD%9C/</link>
      <pubDate>Mon, 10 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-10-torch.stack%E7%9A%84%E7%BB%B4%E5%BA%A6%E6%93%8D%E4%BD%9C/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. 初始化 2. torch.stak  2.1. dim = 0 2.2. dim = 1 2.3. dim = 2    1. 初始化 input
import torch import numpy as np # 创建3*3的矩阵，a、b a=np.array([[1,2,3],[4,5,6],[7,8,9]]) b=np.array([[10,20,30],[40,50,60],[70,80,90]]) # 将矩阵转化为Tensor a = torch.from_numpy(a) b = torch.from_numpy(b) # 打印a、b、c print(a,a.size()) print(b,b.size()) output
tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) torch.Size([3, 3]) tensor([[10, 20, 30], [40, 50, 60], [70, 80, 90]]) torch.</description>
    </item>
    
    <item>
      <title>审稿学习系列01——图像质量评估</title>
      <link>https://ioyy900205.github.io/post/2021-05-10-%E5%AE%A1%E7%A8%BF%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%9701/</link>
      <pubDate>Mon, 10 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-10-%E5%AE%A1%E7%A8%BF%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%9701/</guid>
      <description>审稿学习系列01——图像质量评估 LIU Liang
  1. 背景 2. 图像质量评估（Image Quality Assessment, IQA）——方法分类  2.1. 主观方法 2.2. 客观方法   3. 图像质量评估（Image Quality Assessment, IQA）——图像提供信息分类  3.1. 全参考(Full Reference-IQA, FR-IQA) 3.2. 半参考(Reduced Reference-IQA, RR-IQA) 3.3. 无参考(No Reference-IQA, NR-IQA)   4. 数据集 5. 评估方法  5.1. 评估指标   6. 结果  1. 背景 质量评估(Quality Assessment，QA)在许多领域有其广泛的实用性。（比如图像压缩、视频编解码、视频监控等。）
并且对高效、可靠质量评估的需求日益增加，所以QA成为一个感兴趣的研究领域。
每年都涌现出大量的新的QA算法，有些是扩展已有的算法，也有一些是QA算法的应用。
质量评估可分为：
  图像质量评估（Image Quality Assessment, IQA）
  视频质量评估（Video Quality Assessment, VQA）</description>
    </item>
    
    <item>
      <title>05——ShuffleNet学习</title>
      <link>https://ioyy900205.github.io/post/2021-05-07-shufflenet/</link>
      <pubDate>Fri, 07 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-05-07-shufflenet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>03——nn.module学习——PyTorch</title>
      <link>https://ioyy900205.github.io/post/2021-04-27-nn.module/</link>
      <pubDate>Tue, 27 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-04-27-nn.module/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. nn.moduel初步  1.1. Linear类 1.2. Conv2d类 1.3. 自定义层的步骤   2. 自定义层  2.1. 定义一个自定义层MyLayer 2.2. 自定义模型并且训练    1. nn.moduel初步 keras更加注重的是层Layer、pytorch更加注重的是模型Module。
1.1. Linear类 import math import torch from torch.nn.parameter import Parameter from .. import functional as F from .. import init from .module import Module from ..._jit_internal import weak_module, weak_script_method class Linear(Module): __constants__ = [&amp;#39;bias&amp;#39;] def __init__(self, in_features, out_features, bias=True): super(Linear, self).__init__() self.</description>
    </item>
    
    <item>
      <title>04——轻量级神经网络总结</title>
      <link>https://ioyy900205.github.io/post/2021-04-27-%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/</link>
      <pubDate>Tue, 27 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-04-27-%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. 需解决的问题 2. 衡量指标  2.1. FLOPs 2.2. Params  2.2.1. 卷积层的参数量 2.2.2. 全连接层的参数量   2.3. MAC 2.4. MACC(也叫MADD)   3. 方法  3.1. 模型结构设计  3.1.1. 分组卷积 3.1.2. 分解卷积   3.2. 模型压缩  3.2.1. 权值量化 3.2.2. 网络剪枝 3.2.3. 低秩近似 3.2.4. 知识蒸馏      1. 需解决的问题 存储问题
数百层网络有着大量的权值参数，保存大量权值参数对设备的内存要求很高； 速度问题
在实际应用中，往往是毫秒级别，为了达到实际应用标准，要么提高处理器性能，要么就减少计算量。 而提高处理器性能在短时间内是无法完成的，因此减少计算量成为了主要的技术手段。 2. 衡量指标 目前，网络架构设计主要由计算复杂度的间接度量（如FLOPs）测量。然而，直接度量（例如，速度）还取决于诸如存储器访问成本和平台特性的其他因素。
2.1. FLOPs FLOPS(floating point operations per second)</description>
    </item>
    
    <item>
      <title>01——ResNeXt学习</title>
      <link>https://ioyy900205.github.io/post/2021-04-26-resnext/</link>
      <pubDate>Mon, 26 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-04-26-resnext/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. ResNeXt  1.1. 概述 1.2. 思路 1.3. Block 1.4. 组卷积 1.5. 模型复杂度 1.6. Shortcut 1.7. 结果    1. ResNeXt 1.1. 概述 论文：Aggregated Residual Transformations for Deep Neural Networks
论文链接：https://arxiv.org/abs/1611.05431
PyTorch代码：https://github.com/miraclewkf/ResNeXt-PyTorch 2016年,ISCLVCR 2016 no.2
1.2. 思路 Split-Transform-Merge （来源于inception）
堆叠（来源于VGG）
1.3. Block cardinality ，原文的解释是the size of the set of transformations，如上图右边是 cardinality=32 的样子，这里注意每个被聚合的拓扑结构都是一样的(这也是和 Inception 的差别，减轻设计负担)
1.4. 组卷积 最早可以追溯到AlexNet。
32x4d 中 32为组卷积数目，4d为每组卷积4个卷积核。
组卷积可以有不同的配置，但是不同的配置需要通过实验判断效果。
可以发现通过组卷积能够有效降低parameter的大小。
1.5. 模型复杂度 如果想增加模型复杂度，几个选择：</description>
    </item>
    
    <item>
      <title>02——SeNet学习</title>
      <link>https://ioyy900205.github.io/post/2021-04-26-senet/</link>
      <pubDate>Mon, 26 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-04-26-senet/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. SENET  1.1. 链接 1.2. 贡献  1.2.1. 提供了子结构 1.2.2. SOTA   1.3. 核心思想 1.4. Squeeze 1.5. Excitation   2. 思考  2.1. 浅层作用较大    1. SENET 1.1. 链接 论文：Squeeze-and-Excitation Networks论文链接：https://arxiv.org/abs/1709.01507代码地址：https://github.com/hujie-frank/SENetPyTorch代码地址：https://github.com/miraclewkf/SENet-PyTorch
A central theme of computer vision research is the search for more powerful representations that capture only those properties of an image that are most salient for a given task</description>
    </item>
    
    <item>
      <title>REF01——避坑指南——打印的RESNET18</title>
      <link>https://ioyy900205.github.io/post/2021-04-25-ref01%E9%81%BF%E5%9D%91%E6%8C%87%E5%8D%97%E6%89%93%E5%8D%B0%E7%9A%84resnet18/</link>
      <pubDate>Sun, 25 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ioyy900205.github.io/post/2021-04-25-ref01%E9%81%BF%E5%9D%91%E6%8C%87%E5%8D%97%E6%89%93%E5%8D%B0%E7%9A%84resnet18/</guid>
      <description>请不要假装很努力， 因为结果不会陪你演戏！
  1. BasicBlock 2. BottleNeck 3. ResNet18  1. BasicBlock 其中有个坑是下采样。如下图所示：
具体来说，如代码所展示的：
if stride != 1 or in_planes != self.expansion*planes: self.shortcut = nn.Sequential( nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(self.expansion*planes) ) 当 stride不等于1 or 输入通道数和输出通道数不相同 时： 进行下采样操作。
下采样：Conv2D+BN
目的是让跳层能够保持维数
2. BottleNeck 同样的BottleNeck也存在这样的问题。
3. ResNet18 ResNet( (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layer1): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.</description>
    </item>
    
  </channel>
</rss>
