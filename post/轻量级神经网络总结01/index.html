<!doctype html>
<html lang="en-us">
  <head>
    <title>04——轻量级神经网络总结 // 亮的笔记</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.82.1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Liu Liang" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://ioyy900205.github.io/css/main.min.88e7083eff65effb7485b6e6f38d10afbec25093a6fac42d734ce9024d3defbd.css" />

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="04——轻量级神经网络总结"/>
<meta name="twitter:description" content="1. 需解决的问题 2. 方法  2.1. 模型结构设计  2.1.1. 分组卷积   2.2. 模型压缩  2.2.1. 权值量化 2.2.2. 网络剪枝 2.2.3. 低秩近似 2.2.4. 知识蒸馏      1. 需解决的问题 存储问题
数百层网络有着大量的权值参数，保存大量权值参数对设备的内存要求很高； 速度问题
在实际应用中，往往是毫秒级别，为了达到实际应用标准，要么提高处理器性能，要么就减少计算量。 而提高处理器性能在短时间内是无法完成的，因此减少计算量成为了主要的技术手段。 2. 方法 2.1. 模型结构设计 主要思想在于设计更高效的「网络计算方式」（主要针对卷积方式），从而使网络参数减少的同时，不损失网络性能。
  GoogleNet 使用了Inception 模块而不再是简单的堆叠网络层从而减小了计算量。
  ResNet 通过引入瓶颈结构取得了极好的图像识别效果。
  ShuffleNet 结合了群组概念和深度可分离卷积，在 ResNet 上取得了很好的加速效果。
  MobileNet 采用了深度可分离卷积实现了目前的最好网络压缩效果。
  采用的方法
  减小卷积核大小
  减少通道数"/>

    <meta property="og:title" content="04——轻量级神经网络总结" />
<meta property="og:description" content="1. 需解决的问题 2. 方法  2.1. 模型结构设计  2.1.1. 分组卷积   2.2. 模型压缩  2.2.1. 权值量化 2.2.2. 网络剪枝 2.2.3. 低秩近似 2.2.4. 知识蒸馏      1. 需解决的问题 存储问题
数百层网络有着大量的权值参数，保存大量权值参数对设备的内存要求很高； 速度问题
在实际应用中，往往是毫秒级别，为了达到实际应用标准，要么提高处理器性能，要么就减少计算量。 而提高处理器性能在短时间内是无法完成的，因此减少计算量成为了主要的技术手段。 2. 方法 2.1. 模型结构设计 主要思想在于设计更高效的「网络计算方式」（主要针对卷积方式），从而使网络参数减少的同时，不损失网络性能。
  GoogleNet 使用了Inception 模块而不再是简单的堆叠网络层从而减小了计算量。
  ResNet 通过引入瓶颈结构取得了极好的图像识别效果。
  ShuffleNet 结合了群组概念和深度可分离卷积，在 ResNet 上取得了很好的加速效果。
  MobileNet 采用了深度可分离卷积实现了目前的最好网络压缩效果。
  采用的方法
  减小卷积核大小
  减少通道数" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ioyy900205.github.io/post/%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%9301/" /><meta property="article:section" content="post" />





  </head>
  <body>
    <header class="app-header">
      <a href="https://ioyy900205.github.io"><img class="app-header-avatar" src="https://ss1.bdstatic.com/70cFvXSh_Q1YnxGkpoWK1HF6hhy/it/u=3520060145,2875461924&amp;fm=26&amp;gp=0.jpg" alt="Liu Liang" /></a>
      <h1>亮的笔记</h1>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/">Home</a>
             - 
          
          <a class="app-header-menu-item" href="/tags/">Tags</a>
             - 
          
          <a class="app-header-menu-item" href="/about/">About</a>
      </nav>
      <p>深度学习笔记本</p>
      <div class="app-header-social">
        
          <a href="https://github.com/ioyy900205" target="_blank" rel="noreferrer noopener">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>Github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg>
          </a>
        
          <a href="https://twitter.com/ioyy900205" target="_blank" rel="noreferrer noopener">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-twitter">
  <title>Twitter</title>
  <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path>
</svg>
          </a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">04——轻量级神经网络总结</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Jan 1, 0001
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          1 min read
        </div>
      </div>
    </header>
    <div class="post-content">
      <ul>
<li><a href="#1-%E9%9C%80%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98">1. 需解决的问题</a></li>
<li><a href="#2-%E6%96%B9%E6%B3%95">2. 方法</a>
<ul>
<li><a href="#21-%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E8%AE%BE%E8%AE%A1">2.1. 模型结构设计</a>
<ul>
<li><a href="#211-%E5%88%86%E7%BB%84%E5%8D%B7%E7%A7%AF">2.1.1. 分组卷积</a></li>
</ul>
</li>
<li><a href="#22-%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9">2.2. 模型压缩</a>
<ul>
<li><a href="#221-%E6%9D%83%E5%80%BC%E9%87%8F%E5%8C%96">2.2.1. 权值量化</a></li>
<li><a href="#222-%E7%BD%91%E7%BB%9C%E5%89%AA%E6%9E%9D">2.2.2. 网络剪枝</a></li>
<li><a href="#223-%E4%BD%8E%E7%A7%A9%E8%BF%91%E4%BC%BC">2.2.3. 低秩近似</a></li>
<li><a href="#224-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F">2.2.4. 知识蒸馏</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="1-需解决的问题">1. 需解决的问题</h1>
<p>存储问题</p>
<pre><code>数百层网络有着大量的权值参数，保存大量权值参数对设备的内存要求很高；
</code></pre>
<p>速度问题</p>
<pre><code>在实际应用中，往往是毫秒级别，为了达到实际应用标准，要么提高处理器性能，要么就减少计算量。 而提高处理器性能在短时间内是无法完成的，因此减少计算量成为了主要的技术手段。
</code></pre>
<h1 id="2-方法">2. 方法</h1>
<h2 id="21-模型结构设计">2.1. 模型结构设计</h2>
<p>主要思想在于设计更高效的「网络计算方式」（主要针对卷积方式），从而使网络参数减少的同时，不损失网络性能。</p>
<ul>
<li>
<p>GoogleNet 使用了Inception 模块而不再是简单的堆叠网络层从而减小了计算量。</p>
</li>
<li>
<p>ResNet 通过引入瓶颈结构取得了极好的图像识别效果。</p>
</li>
<li>
<p>ShuffleNet 结合了群组概念和深度可分离卷积，在 ResNet 上取得了很好的加速效果。</p>
</li>
<li>
<p>MobileNet 采用了深度可分离卷积实现了目前的最好网络压缩效果。</p>
</li>
</ul>
<p>采用的方法</p>
<ul>
<li>
<p>减小卷积核大小</p>
</li>
<li>
<p>减少通道数</p>
</li>
<li>
<p>减少filter数目</p>
</li>
<li>
<p>池化操作</p>
</li>
</ul>
<h3 id="211-分组卷积">2.1.1. 分组卷积</h3>
<p>分组卷积即将输入的feature maps分成不同的组（沿channel维度进行分组），然后对不同的组分别进行卷积操作，即每一个卷积核至于输入的feature maps的其中一组进行连接，而普通的卷积操作是与所有的feature maps进行连接计算。分组数k越多，卷积操作的总参数量和总计算量就越少（减少k倍）。然而分组卷积有一个致命的缺点就是不同分组的通道间减少了信息流通，即输出的feature maps只考虑了输入特征的部分信息，因此在实际应用的时候会在分组卷积之后进行信息融合操作，接下来主要讲两个比较经典的结构，ShuffleNet和MobileNet结构。</p>
<ol>
<li>ShuffleNet结构</li>
<li>分解卷积</li>
</ol>
<h2 id="22-模型压缩">2.2. 模型压缩</h2>
<p>即在已经训练好的模型上进行压缩，使得网络携带更少的网络参数，从而解决内存问题，同时可以解决速度问题。</p>
<h3 id="221-权值量化">2.2.1. 权值量化</h3>
<p>网络量化通过减少表示每个权重的比特数的方法来压缩神经网络。量化的思想就是对权重数值进行聚类。模型的权值参数往往以 32 位浮点数的形式保存，神经网络的参数，会占据极大的存储空间，因此，如果在存储模型参数时将 32 位浮点数量化为 8 位的定点数，可以把参数大小缩小为原来的 1/4，整个模型的大小也可以缩小为原来的 1/4，不仅如此，随着参数量化后模型的减小，网络前向运算阶段所需要的计算资源也会大大减少。</p>
<p>量化有效原因：</p>
<ol>
<li>量化相当于引入噪声，但CNN对噪声不敏感。</li>
<li>位数减少后降低乘法操作，运算变快。</li>
<li>减少了访存开销（节能），同时所需的乘法器数目也减少（减少芯片面积）。</li>
</ol>
<h3 id="222-网络剪枝">2.2.2. 网络剪枝</h3>
<p>剪枝目的在于找出网络冗余连接并移除。由于全连接的连接冗余度远高于卷积层，传统的剪枝多在全连接层对冗余神经元及连接进行移除。虽然移出的神经元及连接对结果影响不大，但仍会对性能造成影响，常见方法为对剪枝后的模型进行微调，剪枝与微调交替进行，保证模型性能。</p>
<ul>
<li>后剪枝</li>
</ul>
<p>模型训练后进行剪枝</p>
<ul>
<li>训练时剪枝</li>
</ul>
<p>模型边训练边剪枝</p>
<p>剪枝后，权值矩阵由稠密变得稀疏</p>
<p><img src="https://upload-images.jianshu.io/upload_images/11659928-dd5e8980e3d45abb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/390/format/webp" alt="https://upload-images.jianshu.io/upload_images/11659928-dd5e8980e3d45abb.png?imageMogr2/auto-orient/strip|imageView2/2/w/390/format/webp"></p>
<h3 id="223-低秩近似">2.2.3. 低秩近似</h3>
<p>低秩分解的方法从分解矩阵运算的角度对模型计算过程进行了优化。
通过使用线性代数的方法将参数矩阵分解为一系列小矩阵的组合，使得小矩阵的组合在表达能力上与原始卷积层基本一致，这就是基于低秩分解方法的本质。</p>
<p>缺点：</p>
<ol>
<li>低秩分解实现并不容易，且计算成本高昂；</li>
<li>目前没有特别好的卷积层实现方式，而目前研究已知，卷积神经网络计算复杂度集中在卷积层；</li>
<li>低秩近似只能逐层进行，无法执行全局参数压缩。</li>
</ol>
<h3 id="224-知识蒸馏">2.2.4. 知识蒸馏</h3>
<p>参考论文：Distilling the Knowledge in a Neural Network</p>
<p>链接：<a href="https://arxiv.org/pdf/1503.02531.pdf">https://arxiv.org/pdf/1503.02531.pdf</a></p>
<p>使用一个大型预先训练的网络（即教师网络）来训练一个更小的网络(又名学生网络)。一旦对一个繁琐笨重的网络模型进行了训练，就可以使用另外一种训练（一种蒸馏的方式），将知识从繁琐的模型转移到更适合部署的小模型。</p>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
