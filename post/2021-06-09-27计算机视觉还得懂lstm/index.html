<!doctype html>
<html lang="en-us">
  <head>
    <title>27——做计算机视觉还得懂LSTM // 亮的笔记</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.90.1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Liu Liang" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://ioyy900205.github.io/css/main.min.88e7083eff65effb7485b6e6f38d10afbec25093a6fac42d734ce9024d3defbd.css" />

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="27——做计算机视觉还得懂LSTM"/>
<meta name="twitter:description" content="Long Short-Term Memory Networks
LSTMs are a special kind of Recurrent Neural Network — capable of learning long-term dependencies by remembering information for long periods is the default behavior.
All recurrent neural networks are in the form of a chain of repeating modules of a neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.
  1. Step 1: Decide how much past data it should remember 2."/>

    <meta property="og:title" content="27——做计算机视觉还得懂LSTM" />
<meta property="og:description" content="Long Short-Term Memory Networks
LSTMs are a special kind of Recurrent Neural Network — capable of learning long-term dependencies by remembering information for long periods is the default behavior.
All recurrent neural networks are in the form of a chain of repeating modules of a neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.
  1. Step 1: Decide how much past data it should remember 2." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ioyy900205.github.io/post/2021-06-09-27%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%98%E5%BE%97%E6%87%82lstm/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-06-09T12:14:22+08:00" />
<meta property="article:modified_time" content="2021-06-09T12:14:22+08:00" />



  </head>
  <body>
    <header class="app-header">
      <a href="https://ioyy900205.github.io"><img class="app-header-avatar" src="https://ss1.bdstatic.com/70cFvXSh_Q1YnxGkpoWK1HF6hhy/it/u=3520060145,2875461924&amp;fm=26&amp;gp=0.jpg" alt="Liu Liang" /></a>
      <h1>亮的笔记</h1>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/">Home</a>
             - 
          
          <a class="app-header-menu-item" href="/tags/">Tags</a>
             - 
          
          <a class="app-header-menu-item" href="/about/">About</a>
      </nav>
      <p>深度学习笔记本</p>
      <div class="app-header-social">
        
          <a href="https://github.com/ioyy900205" target="_blank" rel="noreferrer noopener">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>Github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg>
          </a>
        
          <a href="https://twitter.com/ioyy900205" target="_blank" rel="noreferrer noopener">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-twitter">
  <title>Twitter</title>
  <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path>
</svg>
          </a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">27——做计算机视觉还得懂LSTM</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Jun 9, 2021
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          3 min read
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7.01" y2="7"></line>
</svg>
              <a class="tag" href="https://ioyy900205.github.io/tags/pytorch/">PyTorch</a>
        </div>
      </div>
    </header>
    <div class="post-content">
      <p>Long Short-Term Memory Networks</p>
<p>LSTMs are a special kind of Recurrent Neural Network — capable of learning long-term dependencies by remembering information for long periods is the default behavior.</p>
<p>All recurrent neural networks are in the form of a chain of repeating modules of a neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.</p>
<p><img src="https://www.simplilearn.com/ice9/free_resources_article_thumb/Workings_of_LSTMs.png" alt=""></p>
<hr>
<ul>
<li><a href="#1-step-1-decide-how-much-past-data-it-should-remember">1. Step 1: Decide how much past data it should remember</a></li>
<li><a href="#2-step-2-decide-how-much-this-unit-adds-to-the-current-state">2. Step 2: Decide how much this unit adds to the current state</a></li>
<li><a href="#3-step-3-decide-what-part-of-the-current-cell-state-makes-it-to-the-output">3. Step 3: Decide what part of the current cell state makes it to the output</a></li>
</ul>
<h2 id="1-step-1-decide-how-much-past-data-it-should-remember">1. Step 1: Decide how much past data it should remember</h2>
<p>The first step in the LSTM is to decide which information should be omitted（省略） from the cell in that particular time step. The sigmoid function determines this. It looks at the previous state (ht-1) along with the current input xt and computes the function.</p>
<p><img src="https://www.simplilearn.com/ice9/free_resources_article_thumb/LSTMs_step1.png" alt=""></p>
<p>Consider the following two sentences:</p>
<p>Let the output of h(t-1) be “Alice is good in Physics. John, on the other hand, is good at Chemistry.”</p>
<p>Let the current input at x(t) be “John plays football well. He told me yesterday over the phone that he had served as the captain of his college football team.”</p>
<p>The forget gate realizes there might be a change in context after encountering the first full stop. It compares with the current input sentence at x(t). The next sentence talks about John, so the information on Alice is deleted. The position of the subject is vacated and assigned to John.</p>
<h2 id="2-step-2-decide-how-much-this-unit-adds-to-the-current-state">2. Step 2: Decide how much this unit adds to the current state</h2>
<p>In the second layer, there are two parts. One is the sigmoid function, and the other is the tanh function. In the sigmoid function, it decides which values to let through (0 or 1). tanh function gives weightage to the values which are passed, deciding their level of importance (-1 to 1).</p>
<p><img src="https://www.simplilearn.com/ice9/free_resources_article_thumb/LSTMs_step2.png" alt=""></p>
<p>With the current input at x(t), the input gate analyzes the important information — John plays football, and the fact that he was the captain of his college team is important.</p>
<p>“He told me yesterday over the phone” is less important; hence it&rsquo;s forgotten. This process of adding some new information can be done via the input gate.</p>
<h2 id="3-step-3-decide-what-part-of-the-current-cell-state-makes-it-to-the-output">3. Step 3: Decide what part of the current cell state makes it to the output</h2>
<p>The third step is to decide what the output will be. First, we run a sigmoid layer, which decides what parts of the cell state make it to the output. Then, we put the cell state through tanh to push the values to be between -1 and 1 and multiply it by the output of the sigmoid gate.</p>
<p><img src="https://s3.amazonaws.com/static2.simplilearn.com/ice9/free_resources_article_thumb/LSTMs_step3.png" alt=""></p>
<p>Let’s consider this example to predict the next word in the sentence: “John played tremendously well against the opponent and won for his team. For his contributions, brave ____ was awarded player of the match.”</p>
<p>There could be many choices for the empty space. The current input brave is an adjective, and adjectives describe a noun. So, “John” could be the best output after brave.</p>
<hr>
<p><strong>参考资料</strong></p>
<p><a href="https://www.simplilearn.com/tutorials/deep-learning-tutorial/rnn">https://www.simplilearn.com/tutorials/deep-learning-tutorial/rnn</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/34203833">https://zhuanlan.zhihu.com/p/34203833</a></p>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
