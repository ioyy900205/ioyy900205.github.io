<!doctype html>
<html lang="en-us">
  <head>
    <title>04——轻量级神经网络总结 // 亮的笔记</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.82.1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Liu Liang" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://ioyy900205.github.io/css/main.min.88e7083eff65effb7485b6e6f38d10afbec25093a6fac42d734ce9024d3defbd.css" />

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="04——轻量级神经网络总结"/>
<meta name="twitter:description" content="请不要假装很努力， 因为结果不会陪你演戏！
  1. 需解决的问题 2. 衡量指标  2.1. FLOPs 2.2. Params  2.2.1. 卷积层的参数量 2.2.2. 全连接层的参数量   2.3. MAC 2.4. MACC(也叫MADD)   3. 方法  3.1. 模型结构设计  3.1.1. 分组卷积 3.1.2. 分解卷积   3.2. 模型压缩  3.2.1. 权值量化 3.2.2. 网络剪枝 3.2.3. 低秩近似 3.2.4. 知识蒸馏      1. 需解决的问题 存储问题
数百层网络有着大量的权值参数，保存大量权值参数对设备的内存要求很高； 速度问题
在实际应用中，往往是毫秒级别，为了达到实际应用标准，要么提高处理器性能，要么就减少计算量。 而提高处理器性能在短时间内是无法完成的，因此减少计算量成为了主要的技术手段。 2. 衡量指标 目前，网络架构设计主要由计算复杂度的间接度量（如FLOPs）测量。然而，直接度量（例如，速度）还取决于诸如存储器访问成本和平台特性的其他因素。
2.1. FLOPs FLOPS(floating point operations per second)"/>

    <meta property="og:title" content="04——轻量级神经网络总结" />
<meta property="og:description" content="请不要假装很努力， 因为结果不会陪你演戏！
  1. 需解决的问题 2. 衡量指标  2.1. FLOPs 2.2. Params  2.2.1. 卷积层的参数量 2.2.2. 全连接层的参数量   2.3. MAC 2.4. MACC(也叫MADD)   3. 方法  3.1. 模型结构设计  3.1.1. 分组卷积 3.1.2. 分解卷积   3.2. 模型压缩  3.2.1. 权值量化 3.2.2. 网络剪枝 3.2.3. 低秩近似 3.2.4. 知识蒸馏      1. 需解决的问题 存储问题
数百层网络有着大量的权值参数，保存大量权值参数对设备的内存要求很高； 速度问题
在实际应用中，往往是毫秒级别，为了达到实际应用标准，要么提高处理器性能，要么就减少计算量。 而提高处理器性能在短时间内是无法完成的，因此减少计算量成为了主要的技术手段。 2. 衡量指标 目前，网络架构设计主要由计算复杂度的间接度量（如FLOPs）测量。然而，直接度量（例如，速度）还取决于诸如存储器访问成本和平台特性的其他因素。
2.1. FLOPs FLOPS(floating point operations per second)" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ioyy900205.github.io/post/%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-04-27T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2021-04-27T00:00:00&#43;00:00" />



  </head>
  <body>
    <header class="app-header">
      <a href="https://ioyy900205.github.io"><img class="app-header-avatar" src="https://ss1.bdstatic.com/70cFvXSh_Q1YnxGkpoWK1HF6hhy/it/u=3520060145,2875461924&amp;fm=26&amp;gp=0.jpg" alt="Liu Liang" /></a>
      <h1>亮的笔记</h1>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/">Home</a>
             - 
          
          <a class="app-header-menu-item" href="/tags/">Tags</a>
             - 
          
          <a class="app-header-menu-item" href="/about/">About</a>
      </nav>
      <p>深度学习笔记本</p>
      <div class="app-header-social">
        
          <a href="https://github.com/ioyy900205" target="_blank" rel="noreferrer noopener">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>Github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg>
          </a>
        
          <a href="https://twitter.com/ioyy900205" target="_blank" rel="noreferrer noopener">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-twitter">
  <title>Twitter</title>
  <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path>
</svg>
          </a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">04——轻量级神经网络总结</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Apr 27, 2021
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          1 min read
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7.01" y2="7"></line>
</svg>
              <a class="tag" href="https://ioyy900205.github.io/tags/overview/">Overview</a>
        </div>
      </div>
    </header>
    <div class="post-content">
      <hr>
<p>请不要假装很努力， 因为结果不会陪你演戏！</p>
<hr>
<ul>
<li><a href="#1-%E9%9C%80%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98">1. 需解决的问题</a></li>
<li><a href="#2-%E8%A1%A1%E9%87%8F%E6%8C%87%E6%A0%87">2. 衡量指标</a>
<ul>
<li><a href="#21-flops">2.1. FLOPs</a></li>
<li><a href="#22-params">2.2. Params</a>
<ul>
<li><a href="#221-%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%9A%84%E5%8F%82%E6%95%B0%E9%87%8F">2.2.1. 卷积层的参数量</a></li>
<li><a href="#222-%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E7%9A%84%E5%8F%82%E6%95%B0%E9%87%8F">2.2.2. 全连接层的参数量</a></li>
</ul>
</li>
<li><a href="#23-mac">2.3. MAC</a></li>
<li><a href="#24-macc%E4%B9%9F%E5%8F%ABmadd">2.4. MACC(也叫MADD)</a></li>
</ul>
</li>
<li><a href="#3-%E6%96%B9%E6%B3%95">3. 方法</a>
<ul>
<li><a href="#31-%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E8%AE%BE%E8%AE%A1">3.1. 模型结构设计</a>
<ul>
<li><a href="#311-%E5%88%86%E7%BB%84%E5%8D%B7%E7%A7%AF">3.1.1. 分组卷积</a></li>
<li><a href="#312-%E5%88%86%E8%A7%A3%E5%8D%B7%E7%A7%AF">3.1.2. 分解卷积</a></li>
</ul>
</li>
<li><a href="#32-%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9">3.2. 模型压缩</a>
<ul>
<li><a href="#321-%E6%9D%83%E5%80%BC%E9%87%8F%E5%8C%96">3.2.1. 权值量化</a></li>
<li><a href="#322-%E7%BD%91%E7%BB%9C%E5%89%AA%E6%9E%9D">3.2.2. 网络剪枝</a></li>
<li><a href="#323-%E4%BD%8E%E7%A7%A9%E8%BF%91%E4%BC%BC">3.2.3. 低秩近似</a></li>
<li><a href="#324-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F">3.2.4. 知识蒸馏</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="1-需解决的问题">1. 需解决的问题</h1>
<p>存储问题</p>
<pre><code>数百层网络有着大量的权值参数，保存大量权值参数对设备的内存要求很高；
</code></pre>
<p>速度问题</p>
<pre><code>在实际应用中，往往是毫秒级别，为了达到实际应用标准，要么提高处理器性能，要么就减少计算量。 而提高处理器性能在短时间内是无法完成的，因此减少计算量成为了主要的技术手段。
</code></pre>
<h1 id="2-衡量指标">2. 衡量指标</h1>
<p>目前，网络架构设计主要由计算复杂度的间接度量（如FLOPs）测量。然而，直接度量（例如，速度）还取决于诸如存储器访问成本和平台特性的其他因素。</p>
<h2 id="21-flops">2.1. FLOPs</h2>
<p>FLOPS(floating point operations per second)</p>
<p>全大写，指每秒浮点运算次数，可以理解为计算的速度。是衡量硬件性能的一个指标。（硬件）</p>
<p>FLOPs(floating point operations)</p>
<p>s小写，指浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度。（模型） 在论文中常用GFLOPs（1 GFLOPs = 10^9 FLOPs）</p>
<p><img src="https://pic1.zhimg.com/80/v2-cb4e69ccbeffb008e17b2a1d40f02804_720w.jpg" alt="https://pic1.zhimg.com/80/v2-cb4e69ccbeffb008e17b2a1d40f02804_720w.jpg"></p>
<p>这里有个坑，HW应该是out feature的大小。</p>
<p>常用的网络参数</p>
<p><img src="https://pic3.zhimg.com/v2-83f2bd13d9e36099138bf0b3dcdff722_r.jpg" alt="https://pic3.zhimg.com/v2-83f2bd13d9e36099138bf0b3dcdff722_r.jpg"></p>
<h2 id="22-params">2.2. Params</h2>
<p>模型的参数量。</p>
<h3 id="221-卷积层的参数量">2.2.1. 卷积层的参数量</h3>
<p>$parameter = (k_w \times k_h \times C_{in} + 1) \times C_{out}$</p>
<h3 id="222-全连接层的参数量">2.2.2. 全连接层的参数量</h3>
<p>$parameter = (N_{in}+1) \times N_{out}$</p>
<h2 id="23-mac">2.3. MAC</h2>
<p>Memory Access Cost。</p>
<h2 id="24-macc也叫madd">2.4. MACC(也叫MADD)</h2>
<p>multiply-accumulate operations：先乘起来再加起来的运算次数。</p>
<h1 id="3-方法">3. 方法</h1>
<h2 id="31-模型结构设计">3.1. 模型结构设计</h2>
<p>主要思想在于设计更高效的「网络计算方式」（主要针对卷积方式），从而使网络参数减少的同时，不损失网络性能。</p>
<ul>
<li>
<p>GoogleNet 使用了Inception 模块而不再是简单的堆叠网络层从而减小了计算量。</p>
</li>
<li>
<p>ResNet 通过引入瓶颈结构取得了极好的图像识别效果。</p>
</li>
<li>
<p>ShuffleNet 结合了群组概念和深度可分离卷积，在 ResNet 上取得了很好的加速效果。</p>
</li>
<li>
<p>MobileNet 采用了深度可分离卷积实现了目前的最好网络压缩效果。</p>
</li>
</ul>
<p>采用的方法</p>
<ul>
<li>
<p>减小卷积核大小</p>
</li>
<li>
<p>减少通道数</p>
</li>
<li>
<p>减少filter数目</p>
</li>
<li>
<p>池化操作</p>
</li>
</ul>
<h3 id="311-分组卷积">3.1.1. 分组卷积</h3>
<p>分组卷积即将输入的feature maps分成不同的组（沿channel维度进行分组），然后对不同的组分别进行卷积操作，即每一个卷积核至于输入的feature maps的其中一组进行连接，而普通的卷积操作是与所有的feature maps进行连接计算。分组数k越多，卷积操作的总参数量和总计算量就越少（减少k倍）。然而分组卷积有一个致命的缺点就是不同分组的通道间减少了信息流通，即输出的feature maps只考虑了输入特征的部分信息，因此在实际应用的时候会在分组卷积之后进行信息融合操作，接下来主要讲两个比较经典的结构，ShuffleNet和MobileNet结构。</p>
<ol>
<li>
<p>ShuffleNet结构</p>
<p><img src="https://pic2.zhimg.com/80/v2-f6bf0be7944872780495433690d06961_720w.jpg" alt="https://pic2.zhimg.com/80/v2-f6bf0be7944872780495433690d06961_720w.jpg"></p>
<p>如上图所示，图a是一般的group convolution的实现效果，其造成的问题是，输出通道只和输入的某些通道有关，导致全局信息 流通不畅，网络表达能力不足。图b就是shufflenet结构，即通过均匀排列，把group convolution后的feature map按通道进行均匀混合，这样就可以更好的获取全局信息了。 图c是操作后的等价效果图。在分组卷积的时候，每一个卷积核操作的通道数减少，所以可以大量减少计算量。</p>
</li>
<li>
<p>MobileNet结构</p>
</li>
</ol>
<p><img src="https://pic4.zhimg.com/80/v2-f8f698003d74a2ece29b736733ce8c7f_720w.jpg" alt="https://pic4.zhimg.com/80/v2-f8f698003d74a2ece29b736733ce8c7f_720w.jpg"></p>
<p>mobilenet采用了depthwise separable convolutions的思想，采用depthwise (或叫channelwise)和1x1 pointwise的方法进行分解卷积。其中depthwise separable convolutions即对每一个通道进行卷积操作，可以看成是每组只有一个通道的分组卷积，最后使用开销较小的1x1卷积进行通道融合，可以大大减少计算量。</p>
<p>从维度的角度看，卷积核可以看成是一个空间维(宽和高)和通道维的组合，而卷积操作则可以视为空间相关性和通道相关性的联合映射。从inception的1*1卷积来看，卷积中的空间相关性和通道相关性是可以解耦的，将它们分开进行映射，可能会达到更好的效果。</p>
<h3 id="312-分解卷积">3.1.2. 分解卷积</h3>
<p>分解卷积，即将普通的kxk卷积分解为kx1和1xk卷积，通过这种方式可以在感受野相同的时候大量减少计算量，同时也减少了参数量，在某种程度上可以看成是使用2k个参数模拟k*k个参数的卷积效果，从而造成网络的容量减小，但是可以在较少损失精度的前提下，达到网络加速的效果。</p>
<h2 id="32-模型压缩">3.2. 模型压缩</h2>
<p>即在已经训练好的模型上进行压缩，使得网络携带更少的网络参数，从而解决内存问题，同时可以解决速度问题。</p>
<h3 id="321-权值量化">3.2.1. 权值量化</h3>
<p>网络量化通过减少表示每个权重的比特数的方法来压缩神经网络。量化的思想就是对权重数值进行聚类。模型的权值参数往往以 32 位浮点数的形式保存，神经网络的参数，会占据极大的存储空间，因此，如果在存储模型参数时将 32 位浮点数量化为 8 位的定点数，可以把参数大小缩小为原来的 1/4，整个模型的大小也可以缩小为原来的 1/4，不仅如此，随着参数量化后模型的减小，网络前向运算阶段所需要的计算资源也会大大减少。</p>
<p>量化有效原因：</p>
<ol>
<li>量化相当于引入噪声，但CNN对噪声不敏感。</li>
<li>位数减少后降低乘法操作，运算变快。</li>
<li>减少了访存开销（节能），同时所需的乘法器数目也减少（减少芯片面积）。</li>
</ol>
<h3 id="322-网络剪枝">3.2.2. 网络剪枝</h3>
<p>剪枝目的在于找出网络冗余连接并移除。由于全连接的连接冗余度远高于卷积层，传统的剪枝多在全连接层对冗余神经元及连接进行移除。虽然移出的神经元及连接对结果影响不大，但仍会对性能造成影响，常见方法为对剪枝后的模型进行微调，剪枝与微调交替进行，保证模型性能。</p>
<ul>
<li>后剪枝</li>
</ul>
<p>模型训练后进行剪枝</p>
<ul>
<li>训练时剪枝</li>
</ul>
<p>模型边训练边剪枝</p>
<p>剪枝后，权值矩阵由稠密变得稀疏</p>
<p><img src="https://upload-images.jianshu.io/upload_images/11659928-dd5e8980e3d45abb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/390/format/webp" alt="https://upload-images.jianshu.io/upload_images/11659928-dd5e8980e3d45abb.png?imageMogr2/auto-orient/strip|imageView2/2/w/390/format/webp"></p>
<h3 id="323-低秩近似">3.2.3. 低秩近似</h3>
<p>低秩分解的方法从分解矩阵运算的角度对模型计算过程进行了优化。
通过使用线性代数的方法将参数矩阵分解为一系列小矩阵的组合，使得小矩阵的组合在表达能力上与原始卷积层基本一致，这就是基于低秩分解方法的本质。</p>
<p>缺点：</p>
<ol>
<li>低秩分解实现并不容易，且计算成本高昂；</li>
<li>目前没有特别好的卷积层实现方式，而目前研究已知，卷积神经网络计算复杂度集中在卷积层；</li>
<li>低秩近似只能逐层进行，无法执行全局参数压缩。</li>
</ol>
<h3 id="324-知识蒸馏">3.2.4. 知识蒸馏</h3>
<p>参考论文：Distilling the Knowledge in a Neural Network</p>
<p>链接：<a href="https://arxiv.org/pdf/1503.02531.pdf">https://arxiv.org/pdf/1503.02531.pdf</a></p>
<p>使用一个大型预先训练的网络（即教师网络）来训练一个更小的网络(又名学生网络)。一旦对一个繁琐笨重的网络模型进行了训练，就可以使用另外一种训练（一种蒸馏的方式），将知识从繁琐的模型转移到更适合部署的小模型。</p>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
