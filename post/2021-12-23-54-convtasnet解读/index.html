<!doctype html>
<html lang="en-us">
  <head>
    <title>54-ConvTasnet解读 // 亮的笔记</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.101.0" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Liu Liang" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://ioyy900205.github.io/css/main.min.4a7ec8660f9a44b08c4da97c5f2e31b1192df1d4d0322e65c0dbbc6ecb1b863f.css" />

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="54-ConvTasnet解读"/>
<meta name="twitter:description" content="挖坑挖坑
@TOC
1. 原来的缺点 信号相位和幅度的解耦 语音分离时频表示的次优性 以及计算频谱的时间延迟等 2. STFT的问题 第一，STFT是一种通用的信号变换方法，它对语音分离来说不一定是最佳的。 第二，对干净声源的相位进行精确重建是一个重要的问题，并且相位的错误估计- 会拉低重建音频的上限。尽管一些方法通过重建相位来缓解此问题，但性能仍然欠佳。 第三，成功地从时频表示中分离声源，要求对混合信号进行高分辨-率的频率分解，这需要较长的时间窗来计算STFT。该要求增加了系统的最小延迟，这限制了其在实时、低延迟应用中的适用性。例如，在大多数语音分离系统中，STFT的窗口长度至少为32 ms，在音乐分离应用中甚至更大，而音乐分离应用则需要更高分辨率的频谱（高于90 ms ）。 3. LSTM的问题 原始TasNet将深度长短时记忆（LSTM）网络作为分离模块会大大限制其适用性。
第一，在编码器中选择较小的核大小（即波形段的长度）会增加编码器输出的长度，这使得LSTM的训练变得难以管理。 第二，深度LSTM网络中的大量参数显著增加了其计算成本，并限制了其在低资源、低功耗平台（例如可穿戴式听力设备）中的适用性。 第三个问题是LSTM网络在时间上的长期依赖性，这经常导致分离准确性不一致， 4. 代码给你的启发 我这里参考的是官方pytorch example里面实现的代码
def forward(self, input: torch.Tensor) -&gt; torch.Tensor: &#34;&#34;&#34;Perform source separation. Generate audio source waveforms. Args: input (torch.Tensor): 3D Tensor with shape [batch, channel==1, frames] Returns: Tensor: 3D Tensor with shape [batch, channel==num_sources, frames] &#34;&#34;&#34; if input.ndim != 3 or input.shape[1] != 1: raise ValueError( f&#34;Expected 3D tensor (batch, channel==1, frames)."/>

    <meta property="og:title" content="54-ConvTasnet解读" />
<meta property="og:description" content="挖坑挖坑
@TOC
1. 原来的缺点 信号相位和幅度的解耦 语音分离时频表示的次优性 以及计算频谱的时间延迟等 2. STFT的问题 第一，STFT是一种通用的信号变换方法，它对语音分离来说不一定是最佳的。 第二，对干净声源的相位进行精确重建是一个重要的问题，并且相位的错误估计- 会拉低重建音频的上限。尽管一些方法通过重建相位来缓解此问题，但性能仍然欠佳。 第三，成功地从时频表示中分离声源，要求对混合信号进行高分辨-率的频率分解，这需要较长的时间窗来计算STFT。该要求增加了系统的最小延迟，这限制了其在实时、低延迟应用中的适用性。例如，在大多数语音分离系统中，STFT的窗口长度至少为32 ms，在音乐分离应用中甚至更大，而音乐分离应用则需要更高分辨率的频谱（高于90 ms ）。 3. LSTM的问题 原始TasNet将深度长短时记忆（LSTM）网络作为分离模块会大大限制其适用性。
第一，在编码器中选择较小的核大小（即波形段的长度）会增加编码器输出的长度，这使得LSTM的训练变得难以管理。 第二，深度LSTM网络中的大量参数显著增加了其计算成本，并限制了其在低资源、低功耗平台（例如可穿戴式听力设备）中的适用性。 第三个问题是LSTM网络在时间上的长期依赖性，这经常导致分离准确性不一致， 4. 代码给你的启发 我这里参考的是官方pytorch example里面实现的代码
def forward(self, input: torch.Tensor) -&gt; torch.Tensor: &#34;&#34;&#34;Perform source separation. Generate audio source waveforms. Args: input (torch.Tensor): 3D Tensor with shape [batch, channel==1, frames] Returns: Tensor: 3D Tensor with shape [batch, channel==num_sources, frames] &#34;&#34;&#34; if input.ndim != 3 or input.shape[1] != 1: raise ValueError( f&#34;Expected 3D tensor (batch, channel==1, frames)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ioyy900205.github.io/post/2021-12-23-54-convtasnet%E8%A7%A3%E8%AF%BB/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-12-23T09:53:44+08:00" />
<meta property="article:modified_time" content="2021-12-23T09:53:44+08:00" />



  </head>
  <body>
    <header class="app-header">
      <a href="https://ioyy900205.github.io"><img class="app-header-avatar" src="https://ss1.bdstatic.com/70cFvXSh_Q1YnxGkpoWK1HF6hhy/it/u=3520060145,2875461924&amp;fm=26&amp;gp=0.jpg" alt="Liu Liang" /></a>
      <h1>亮的笔记</h1>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/">Home</a>
             - 
          
          <a class="app-header-menu-item" href="/tags/">Tags</a>
             - 
          
          <a class="app-header-menu-item" href="/about/">About</a>
      </nav>
      <p>深度学习笔记本</p>
      <div class="app-header-social">
        
          <a href="https://github.com/ioyy900205" target="_blank" rel="noreferrer noopener">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>Github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg>
          </a>
        
          <a href="https://twitter.com/ioyy900205" target="_blank" rel="noreferrer noopener">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-twitter">
  <title>Twitter</title>
  <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path>
</svg>
          </a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">54-ConvTasnet解读</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Dec 23, 2021
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          2 min read
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7.01" y2="7"></line>
</svg>
              <a class="tag" href="https://ioyy900205.github.io/tags/pytorch/">PyTorch</a>
        </div>
      </div>
    </header>
    <div class="post-content">
      <p>挖坑挖坑</p>
<hr>
<p>@<a href="%E6%9C%AC%E6%96%87%E5%86%85%E5%AE%B9">TOC</a></p>
<h2 id="1-原来的缺点">1. 原来的缺点</h2>
<ul>
<li>信号相位和幅度的解耦</li>
<li>语音分离时频表示的次优性</li>
<li>以及计算频谱的时间延迟等</li>
</ul>
<h2 id="2-stft的问题">2. STFT的问题</h2>
<ul>
<li>第一，STFT是一种通用的信号变换方法，它对语音分离来说不一定是最佳的。</li>
<li>第二，对干净声源的相位进行精确重建是一个重要的问题，并且相位的错误估计- 会拉低重建音频的上限。尽管一些方法通过重建相位来缓解此问题，但性能仍然欠佳。</li>
<li>第三，成功地从时频表示中分离声源，要求对混合信号进行高分辨-率的频率分解，这需要较长的时间窗来计算STFT。该要求增加了系统的最小延迟，这限制了其在实时、低延迟应用中的适用性。例如，在大多数语音分离系统中，STFT的窗口长度至少为32 ms，在音乐分离应用中甚至更大，而音乐分离应用则需要更高分辨率的频谱（高于90 ms ）。</li>
</ul>
<h2 id="3-lstm的问题">3. LSTM的问题</h2>
<p>原始TasNet将深度长短时记忆（LSTM）网络作为分离模块会大大限制其适用性。</p>
<ul>
<li>第一，在编码器中选择较小的核大小（即波形段的长度）会增加编码器输出的长度，这使得LSTM的训练变得难以管理。</li>
<li>第二，深度LSTM网络中的大量参数显著增加了其计算成本，并限制了其在低资源、低功耗平台（例如可穿戴式听力设备）中的适用性。</li>
<li>第三个问题是LSTM网络在时间上的长期依赖性，这经常导致分离准确性不一致，</li>
</ul>
<h2 id="4-代码给你的启发">4. 代码给你的启发</h2>
<p>我这里参考的是官方pytorch example里面实现的代码</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input: torch<span style="color:#f92672">.</span>Tensor) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>     <span style="color:#e6db74">&#34;&#34;&#34;Perform source separation. Generate audio source waveforms.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">     Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">         input (torch.Tensor): 3D Tensor with shape [batch, channel==1, frames]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">     Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">         Tensor: 3D Tensor with shape [batch, channel==num_sources, frames]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">     &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>     <span style="color:#66d9ef">if</span> input<span style="color:#f92672">.</span>ndim <span style="color:#f92672">!=</span> <span style="color:#ae81ff">3</span> <span style="color:#f92672">or</span> input<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">!=</span> <span style="color:#ae81ff">1</span>:
</span></span><span style="display:flex;"><span>         <span style="color:#66d9ef">raise</span> <span style="color:#a6e22e">ValueError</span>(
</span></span><span style="display:flex;"><span>             <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Expected 3D tensor (batch, channel==1, frames). Found: </span><span style="color:#e6db74">{</span>input<span style="color:#f92672">.</span>shape<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>         )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>     <span style="color:#75715e"># B: batch size</span>
</span></span><span style="display:flex;"><span>     <span style="color:#75715e"># L: input frame length</span>
</span></span><span style="display:flex;"><span>     <span style="color:#75715e"># L&#39;: padded input frame length</span>
</span></span><span style="display:flex;"><span>     <span style="color:#75715e"># F: feature dimension</span>
</span></span><span style="display:flex;"><span>     <span style="color:#75715e"># M: feature frame length</span>
</span></span><span style="display:flex;"><span>     <span style="color:#75715e"># S: number of sources</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>     padded, num_pads <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_align_num_frames_with_strides(input)  <span style="color:#75715e"># B, 1, L&#39;</span>
</span></span><span style="display:flex;"><span>     batch_size, num_padded_frames <span style="color:#f92672">=</span> padded<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], padded<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">2</span>]
</span></span><span style="display:flex;"><span>     feats <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>encoder(padded)  <span style="color:#75715e"># B, F, M</span>
</span></span><span style="display:flex;"><span>     masked <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>mask_generator(feats) <span style="color:#f92672">*</span> feats<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># B, S, F, M</span>
</span></span><span style="display:flex;"><span>     masked <span style="color:#f92672">=</span> masked<span style="color:#f92672">.</span>view(
</span></span><span style="display:flex;"><span>         batch_size <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>num_sources, self<span style="color:#f92672">.</span>enc_num_feats, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>     )  <span style="color:#75715e"># B*S, F, M</span>
</span></span><span style="display:flex;"><span>     decoded <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>decoder(masked)  <span style="color:#75715e"># B*S, 1, L&#39;</span>
</span></span><span style="display:flex;"><span>     output <span style="color:#f92672">=</span> decoded<span style="color:#f92672">.</span>view(
</span></span><span style="display:flex;"><span>         batch_size, self<span style="color:#f92672">.</span>num_sources, num_padded_frames
</span></span><span style="display:flex;"><span>     )  <span style="color:#75715e"># B, S, L&#39;</span>
</span></span><span style="display:flex;"><span>     <span style="color:#66d9ef">if</span> num_pads <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>         output <span style="color:#f92672">=</span> output[<span style="color:#f92672">...</span>, :<span style="color:#f92672">-</span>num_pads]  <span style="color:#75715e"># B, S, L</span>
</span></span><span style="display:flex;"><span>     <span style="color:#66d9ef">return</span> output
</span></span></code></pre></div><p>首先将input规整，得到padded的wav和pad的数量。 这个最后需要将语音进行裁剪的。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">if</span> num_pads <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>            output <span style="color:#f92672">=</span> output[<span style="color:#f92672">...</span>, :<span style="color:#f92672">-</span>num_pads] 
</span></span></code></pre></div><p>self.encoder 可类比 512的STFT</p>
<p>我这里直接给结果</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>padded<span style="color:#f92672">.</span>shape <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Size([<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">320000</span>])
</span></span><span style="display:flex;"><span>feats<span style="color:#f92672">.</span>shape <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Size([<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">512</span>, <span style="color:#ae81ff">40001</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>feats <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>encoder(padded)
</span></span></code></pre></div><p>之后feats 进 self.mask_generator 并点乘 feats 得到 masked</p>
<p>比较重要的地方是mask_generator</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input: torch<span style="color:#f92672">.</span>Tensor) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>     <span style="color:#e6db74">&#34;&#34;&#34;Generate separation mask.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">     Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">         input (torch.Tensor): 3D Tensor with shape [batch, features, frames]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">     Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">         Tensor: shape [batch, num_sources, features, frames]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">     &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>     batch_size <span style="color:#f92672">=</span> input<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>     feats <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>input_norm(input)
</span></span><span style="display:flex;"><span>     feats <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>input_conv(feats)
</span></span><span style="display:flex;"><span>     output <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>     <span style="color:#66d9ef">for</span> layer <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>conv_layers:
</span></span><span style="display:flex;"><span>         residual, skip <span style="color:#f92672">=</span> layer(feats)
</span></span><span style="display:flex;"><span>         <span style="color:#66d9ef">if</span> residual <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:  <span style="color:#75715e"># the last conv layer does not produce residual</span>
</span></span><span style="display:flex;"><span>             feats <span style="color:#f92672">=</span> feats <span style="color:#f92672">+</span> residual
</span></span><span style="display:flex;"><span>         output <span style="color:#f92672">=</span> output <span style="color:#f92672">+</span> skip
</span></span><span style="display:flex;"><span>     output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>output_prelu(output)
</span></span><span style="display:flex;"><span>     output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>output_conv(output)
</span></span><span style="display:flex;"><span>     output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>mask_activate(output)
</span></span><span style="display:flex;"><span>     <span style="color:#66d9ef">return</span> output<span style="color:#f92672">.</span>view(batch_size, self<span style="color:#f92672">.</span>num_sources, self<span style="color:#f92672">.</span>input_dim, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><p>这里做数据规整操纵的是GN</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>self<span style="color:#f92672">.</span>input_norm <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>GroupNorm(
</span></span><span style="display:flex;"><span>         num_groups<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, num_channels<span style="color:#f92672">=</span>input_dim, eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-8</span>
</span></span><span style="display:flex;"><span>     )
</span></span></code></pre></div><p>再接一个数据压缩</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>self<span style="color:#f92672">.</span>input_conv <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Conv1d(
</span></span><span style="display:flex;"><span>   in_channels<span style="color:#f92672">=</span>input_dim, out_channels<span style="color:#f92672">=</span>num_feats, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>将512维压缩到128维</p>
<p>**之后进入Conv_Layers: **</p>
<ul>
<li>是conv_layers 使用append方式加入到nn.ModuleList</li>
<li>添加的就是ConvBLock</li>
<li>ConvBLock中
<ul>
<li>堆叠Conv1d PReLU GN Conv1d PReLU GN</li>
<li>打包一个残差连接</li>
<li>打包一个skip连接</li>
</ul>
</li>
</ul>
<p>有意思的是 残差和skip用同样的Conv1d和同样的配置实现</p>
<p><img src="https://img-blog.csdnimg.cn/20200629161816304.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dqcmVueGlubGVp,size_16,color_FFFFFF,t_70#pic_center" alt=""></p>
<hr>
<p><strong>参考资料</strong></p>
<p><a href="https://blog.csdn.net/wjrenxinlei/article/details/107018571">https://blog.csdn.net/wjrenxinlei/article/details/107018571</a></p>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
